{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b11904b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76f5838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0a0+f70bd71a48.nv24.6)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0a0)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.24.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (184.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp310-cp310-manylinux_2_28_x86_64.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.4/493.4 kB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12.1\n",
      "    Uninstalling sympy-1.12.1:\n",
      "      Successfully uninstalled sympy-1.12.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0a0+f70bd71a48.nv24.6\n",
      "    Uninstalling torch-2.4.0a0+f70bd71a48.nv24.6:\n",
      "      Successfully uninstalled torch-2.4.0a0+f70bd71a48.nv24.6\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.0a0\n",
      "    Uninstalling torchvision-0.19.0a0:\n",
      "      Successfully uninstalled torchvision-0.19.0a0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch-tensorrt 2.4.0a0 requires torch<2.5.0,>=2.4.0.dev, but you have torch 2.9.1+cpu which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sympy-1.14.0 torch-2.9.1+cpu torchaudio-2.9.1+cpu torchvision-0.24.1+cpu\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1296d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fda78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 26 12:11:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:17:00.0 Off |                    0 |\n",
      "| N/A   59C    P0             73W /  300W |    4219MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:31:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             75W /  300W |    7067MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off |   00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   60C    P0             97W /  300W |   16040MiB /  81920MiB |     49%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off |   00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   52C    P0            161W /  300W |   16080MiB /  81920MiB |     56%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55ebbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Prompting/responses/ready_for_metrics\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d06baef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(197285, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labse_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "labse_model = AutoModel.from_pretrained(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "muril_tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n",
    "muril_model = AutoModel.from_pretrained(\"google/muril-base-cased\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "labse_model.to(device)\n",
    "muril_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98950ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ref_based_scores(mt_texts, ref_texts, device=None):\n",
    "    \"\"\"\n",
    "    Compute reference-based similarity scores (LaBSE and MuRIL) between MT outputs and references.\n",
    "    \n",
    "    Args:\n",
    "        mt_texts (list of str): Machine-translated sentences.\n",
    "        ref_texts (list of str): Reference sentences.\n",
    "        device (torch.device): Torch device (CPU/GPU). If None, auto-select.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Average scores {'labse_ref_mt': ..., 'muril_ref_mt': ...}\n",
    "    \"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    labse_model.eval()\n",
    "    muril_model.eval()\n",
    "    \n",
    "    labse_scores = []\n",
    "    muril_scores = []\n",
    "\n",
    "    for mt, ref in zip(mt_texts, ref_texts):\n",
    "        # Skip if any text is empty or NaN\n",
    "        if not mt or not ref or pd.isna(mt) or pd.isna(ref):\n",
    "            continue\n",
    "        \n",
    "        # --- LaBSE ---\n",
    "        try:\n",
    "            inputs = labse_tokenizer(ref, mt, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                embeddings = labse_model(**inputs).pooler_output\n",
    "                # Ensure we have 2 embeddings\n",
    "                if embeddings.shape[0] >= 2:\n",
    "                    cos_sim = F.cosine_similarity(embeddings[0:1], embeddings[1:2]).item()\n",
    "                    labse_scores.append(cos_sim)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping LaBSE pair due to error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # --- MuRIL ---\n",
    "        try:\n",
    "            inputs = muril_tokenizer(ref, mt, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                embeddings = muril_model(**inputs).pooler_output\n",
    "                if embeddings.shape[0] >= 2:\n",
    "                    cos_sim = F.cosine_similarity(embeddings[0:1], embeddings[1:2]).item()\n",
    "                    muril_scores.append(cos_sim)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping MuRIL pair due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'labse_ref_mt': sum(labse_scores)/len(labse_scores) if labse_scores else 0.0,\n",
    "        'muril_ref_mt': sum(muril_scores)/len(muril_scores) if muril_scores else 0.0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f7bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labse = {}\n",
    "muril = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e7e05e",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b74708",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"llama_original.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbe58da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Response</th>\n",
       "      <th>sent_meant</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chanting, the choir raised the volume as the c...</td>\n",
       "      <td>श्रद्धांजली घेतल्यानंतर, गायिका घोषवाण्याने प्...</td>\n",
       "      <td>Chanting, the choir raised the volume as the c...</td>\n",
       "      <td>धर्मगुरू प्रार्थना म्हणत असताना, घोष करणाऱ्या ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A six-month-old calf was submitted for examina...</td>\n",
       "      <td>एकूण सहा महिन्यांचा वाढतोळा ज्याला परीक्षेसाठी...</td>\n",
       "      <td>A six-month-old calf was submitted for examina...</td>\n",
       "      <td>तपासणीसाठी आणलेल्या सहा महिन्यांच्या एका वासरा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planning authorities should provide alternativ...</td>\n",
       "      <td>व्यवस्थापन अधिकारी लहान व्यवसायासाठी घरगुती क्...</td>\n",
       "      <td>Planning authorities should provide alternativ...</td>\n",
       "      <td>नियोजन प्राधिकरणांनी लहान व्यवसायांसाठी पर्याय...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As the machine develops, the forms we use to r...</td>\n",
       "      <td>येत्या वेळी मशीन विकसित होत असताना, आम्ही पूर्...</td>\n",
       "      <td>As the machine develops, the forms we use to r...</td>\n",
       "      <td>जसजशी यंत्रणा विकसित होईल, तसतसे मागील प्रकल्प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As mentioned first, impressions can be mislead...</td>\n",
       "      <td>पहिल्या वेळी सांगितल्याप्रमाणे भावना वाईट वाटू...</td>\n",
       "      <td>As mentioned first, impressions can be mislead...</td>\n",
       "      <td>आधी सांगितल्याप्रमाणे, पहिली छाप फसवी असू शकते.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  \\\n",
       "0  Chanting, the choir raised the volume as the c...   \n",
       "1  A six-month-old calf was submitted for examina...   \n",
       "2  Planning authorities should provide alternativ...   \n",
       "3  As the machine develops, the forms we use to r...   \n",
       "4  As mentioned first, impressions can be mislead...   \n",
       "\n",
       "                                            Response  \\\n",
       "0  श्रद्धांजली घेतल्यानंतर, गायिका घोषवाण्याने प्...   \n",
       "1  एकूण सहा महिन्यांचा वाढतोळा ज्याला परीक्षेसाठी...   \n",
       "2  व्यवस्थापन अधिकारी लहान व्यवसायासाठी घरगुती क्...   \n",
       "3  येत्या वेळी मशीन विकसित होत असताना, आम्ही पूर्...   \n",
       "4  पहिल्या वेळी सांगितल्याप्रमाणे भावना वाईट वाटू...   \n",
       "\n",
       "                                          sent_meant  \\\n",
       "0  Chanting, the choir raised the volume as the c...   \n",
       "1  A six-month-old calf was submitted for examina...   \n",
       "2  Planning authorities should provide alternativ...   \n",
       "3  As the machine develops, the forms we use to r...   \n",
       "4  As mentioned first, impressions can be mislead...   \n",
       "\n",
       "                                                  gt  \n",
       "0  धर्मगुरू प्रार्थना म्हणत असताना, घोष करणाऱ्या ...  \n",
       "1  तपासणीसाठी आणलेल्या सहा महिन्यांच्या एका वासरा...  \n",
       "2  नियोजन प्राधिकरणांनी लहान व्यवसायांसाठी पर्याय...  \n",
       "3  जसजशी यंत्रणा विकसित होईल, तसतसे मागील प्रकल्प...  \n",
       "4    आधी सांगितल्याप्रमाणे, पहिली छाप फसवी असू शकते.  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d0fa363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for llama_original.csv:\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "predictions = df['Response'].tolist()\n",
    "references = df['gt'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse = scores['labse_ref_mt']\n",
    "muril= scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {file_name}:\")\n",
    "print(labse, muril)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b654ea",
   "metadata": {},
   "source": [
    "## Only Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3597778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"with_outputs.csv\"\n",
    "mode = \"with\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414b4bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for with:\n",
      "{'labse_ref_mt': 0.9197827294812637, 'muril_ref_mt': 0.9981659788903645}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "predictions = df['prediction_mar'].tolist()\n",
    "references = df['gt_mar'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse[mode] = scores['labse_ref_mt']\n",
    "muril[mode] = scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {mode}:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf47b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868417e8",
   "metadata": {},
   "source": [
    "## Without Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32428f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"without_outputs.csv\"\n",
    "mode = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7bfb75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for without:\n",
      "{'labse_ref_mt': 0.9201303059861079, 'muril_ref_mt': 0.9981740449472081}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "predictions = df['prediction_mar'].tolist()\n",
    "references = df['gt_mar'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse[mode] = scores['labse_ref_mt']\n",
    "muril[mode] = scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {mode}:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5913607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75971cf2",
   "metadata": {},
   "source": [
    "## Combined - LR & Epochs Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e439df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"combined_2x_outputs.csv\"\n",
    "mode = \"combined_2x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89daab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for combined_2x:\n",
      "{'labse_ref_mt': 0.9204223514679323, 'muril_ref_mt': 0.9981520064735604}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "predictions = df['prediction_mar'].tolist()\n",
    "references = df['gt_mar'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse[mode] = scores['labse_ref_mt']\n",
    "muril[mode] = scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {mode}:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1596892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff94aac8",
   "metadata": {},
   "source": [
    "## Combined - LR & Epochs & Dataset Changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717a9874",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"combined_x_outputs.csv\"\n",
    "mode = \"combined_x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd12e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for combined_x:\n",
      "{'labse_ref_mt': 0.9217669360018302, 'muril_ref_mt': 0.9981642475458181}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "predictions = df['prediction_mar'].tolist()\n",
    "references = df['gt_mar'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse[mode] = scores['labse_ref_mt']\n",
    "muril[mode] = scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {mode}:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5107dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9dddfab",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203efd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"t5_outputs.csv\"\n",
    "mode = \"t5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0a13815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaBSE and MuRIL Scores for t5:\n",
      "{'labse_ref_mt': 0.9212691990159818, 'muril_ref_mt': 0.9981615691031951}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)\n",
    "\n",
    "predictions = df['prediction_mar'].tolist()\n",
    "references = df['gt_mar'].tolist()\n",
    "\n",
    "scores = compute_ref_based_scores(predictions, references)\n",
    "labse[mode] = scores['labse_ref_mt']\n",
    "muril[mode] = scores['muril_ref_mt']\n",
    "\n",
    "print(f\"LaBSE and MuRIL Scores for {mode}:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c32f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc4b51d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : 0.9226\n",
      "with : 0.9198\n",
      "without : 0.9201\n",
      "combined_2x : 0.9204\n",
      "combined_x : 0.9218\n",
      "t5 : 0.9213\n"
     ]
    }
   ],
   "source": [
    "for key, val in labse.items():\n",
    "    print(f\"{key} : {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a526669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : 0.998117\n",
      "with : 0.998166\n",
      "without : 0.998174\n",
      "combined_2x : 0.998152\n",
      "combined_x : 0.998164\n",
      "t5 : 0.998162\n"
     ]
    }
   ],
   "source": [
    "for key, val in muril.items():\n",
    "    print(f\"{key} : {val:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
