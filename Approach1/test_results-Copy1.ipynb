{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f623bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 10:54:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              68W / 300W |  81044MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              71W / 300W |  80288MiB / 81920MiB |     23%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              74W / 300W |  80858MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   63C    P0              92W / 300W |  74223MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     18015      C   /usr/bin/python                             518MiB |\n",
      "|    0   N/A  N/A     21505      C   /usr/bin/python                            2024MiB |\n",
      "|    1   N/A  N/A     23286      C   /usr/bin/python                             996MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eeefc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30fbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"thenlpresearcher/bert_punct_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b11252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label list: ['O', 'COMMA', 'PERIOD', 'QUESTION', 'EXCLAMATION', 'SEMICOLON', 'COLON', 'HYPHEN', 'EN_DASH', 'EM_DASH', 'LEFT_PAREN', 'RIGHT_PAREN', 'LEFT_BRACKET', 'RIGHT_BRACKET', 'LEFT_BRACE', 'RIGHT_BRACE', 'DOUBLE_QUOTE', 'SINGLE_QUOTE', 'ELLIPSIS', 'SLASH', 'BACKSLASH', 'AT_SYMBOL', 'HASH', 'DOLLAR', 'PERCENT', 'AMPERSAND', 'ASTERISK', 'PLUS', 'EQUALS', 'LESS_THAN', 'GREATER_THAN', 'PIPE', 'CARET', 'BACKTICK', 'TILDE']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. define punctuation map\n",
    "punctuation_map = {\n",
    "    ',': 'COMMA',\n",
    "    '.': 'PERIOD',\n",
    "    '?': 'QUESTION',\n",
    "    '!': 'EXCLAMATION',\n",
    "    ';': 'SEMICOLON',\n",
    "    ':': 'COLON',\n",
    "    '-': 'HYPHEN',\n",
    "    '–': 'EN_DASH',\n",
    "    '—': 'EM_DASH',\n",
    "    '(': 'LEFT_PAREN',\n",
    "    ')': 'RIGHT_PAREN',\n",
    "    '[': 'LEFT_BRACKET',\n",
    "    ']': 'RIGHT_BRACKET',\n",
    "    '{': 'LEFT_BRACE',\n",
    "    '}': 'RIGHT_BRACE',\n",
    "    '\"': 'DOUBLE_QUOTE',\n",
    "    \"'\": 'SINGLE_QUOTE',\n",
    "    '…': 'ELLIPSIS',\n",
    "    '/': 'SLASH',\n",
    "    '\\\\': 'BACKSLASH',\n",
    "    '@': 'AT_SYMBOL',\n",
    "    '#': 'HASH',\n",
    "    '$': 'DOLLAR',\n",
    "    '%': 'PERCENT',\n",
    "    '&': 'AMPERSAND',\n",
    "    '*': 'ASTERISK',\n",
    "    '+': 'PLUS',\n",
    "    '=': 'EQUALS',\n",
    "    '<': 'LESS_THAN',\n",
    "    '>': 'GREATER_THAN',\n",
    "    '|': 'PIPE',\n",
    "    '^': 'CARET',\n",
    "    '`': 'BACKTICK',\n",
    "    '~': 'TILDE'\n",
    "}\n",
    "\n",
    "# Automatically create label_list from punctuation_map\n",
    "label_list = [\"O\"] + list(punctuation_map.values())\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "print(\"Label list:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d09f63e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████| 8079/8079 [00:01<00:00, 6713.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 2. Token-label creation function\n",
    "# -------------------------------\n",
    "def create_token_labels(sentence):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    parts = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r\"\\w+\", part):  # token\n",
    "            tokens.append(part)\n",
    "            if i+1 < len(parts) and parts[i+1] in punctuation_map:\n",
    "                labels.append(punctuation_map[parts[i+1]])\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "    return tokens, labels\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load CSV and create token-label dataset\n",
    "# -------------------------------\n",
    "def load_and_process(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sent in df['text']:\n",
    "        tokens, labels = create_token_labels(str(sent))\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append([label_to_id[l] for l in labels])\n",
    "\n",
    "    return Dataset.from_dict({\"tokens\": all_tokens, \"labels\": all_labels})\n",
    "\n",
    "\n",
    "test_dataset  = load_and_process(\"iwslt2017_en_test.csv\")\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    new_labels = []\n",
    "    for i, label in enumerate(batch[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                aligned_labels.append(label[word_id])\n",
    "        new_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "test_dataset  = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "test_dataset  = test_dataset.remove_columns([\"tokens\"])\n",
    "\n",
    "test_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb7efa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9979f07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24131/1153594170.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Make predictions on test dataset\n",
    "# -------------------------------\n",
    "predictions, labels, _ = trainer.predict(test_dataset)  # test_dataset must be already prepared\n",
    "pred_ids = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Map predictions and labels back to strings\n",
    "# -------------------------------\n",
    "true_labels_list = []\n",
    "pred_labels_list = []\n",
    "\n",
    "for label_row, pred_row in zip(labels, pred_ids):\n",
    "    true_row = []\n",
    "    pred_row_labels = []\n",
    "    for l, p in zip(label_row, pred_row):\n",
    "        if l != -100:  # ignore padding\n",
    "            true_row.append(label_list[l])\n",
    "            pred_row_labels.append(label_list[p])\n",
    "    true_labels_list.append(true_row)\n",
    "    pred_labels_list.append(pred_row_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40a4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed classification report (per label) using sklearn:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "    AMPERSAND     1.0000    1.0000    1.0000         8\n",
      "        COLON     0.5710    0.6336    0.6006       292\n",
      "        COMMA     0.7985    0.8220    0.8101      9916\n",
      "       DOLLAR     0.6000    0.3750    0.4615         8\n",
      " DOUBLE_QUOTE     0.5837    0.3898    0.4674       313\n",
      "      EM_DASH     0.0000    0.0000    0.0000        27\n",
      "       EQUALS     0.0000    0.0000    0.0000         2\n",
      "  EXCLAMATION     0.7500    0.1552    0.2571        58\n",
      "         HASH     0.0000    0.0000    0.0000         2\n",
      "       HYPHEN     0.6538    0.5157    0.5766      1117\n",
      " LEFT_BRACKET     0.0000    0.0000    0.0000        15\n",
      "            O     0.9844    0.9858    0.9851    117294\n",
      "      PERCENT     0.0000    0.0000    0.0000         0\n",
      "       PERIOD     0.9662    0.9714    0.9688      8729\n",
      "         PLUS     0.0000    0.0000    0.0000         2\n",
      "     QUESTION     0.9108    0.8994    0.9051       795\n",
      "RIGHT_BRACKET     0.0000    0.0000    0.0000        34\n",
      "    SEMICOLON     0.4000    0.2273    0.2899       132\n",
      " SINGLE_QUOTE     0.9957    0.9968    0.9962      3720\n",
      "        SLASH     1.0000    0.3333    0.5000         9\n",
      "\n",
      "     accuracy                         0.9659    142473\n",
      "    macro avg     0.5107    0.4153    0.4409    142473\n",
      " weighted avg     0.9646    0.9659    0.9650    142473\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Flatten lists for sklearn\n",
    "y_true_flat = [label for seq in true_labels_list for label in seq]\n",
    "y_pred_flat = [label for seq in pred_labels_list for label in seq]\n",
    "\n",
    "print(\"\\nDetailed classification report (per label) using sklearn:\")\n",
    "print(classification_report(y_true_flat, y_pred_flat, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8230efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
