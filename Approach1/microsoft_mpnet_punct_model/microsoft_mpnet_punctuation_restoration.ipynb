{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2311685f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label list: ['O', 'COMMA', 'PERIOD', 'QUESTION', 'EXCLAMATION', 'SEMICOLON', 'COLON', 'HYPHEN', 'EN_DASH', 'EM_DASH', 'LEFT_PAREN', 'RIGHT_PAREN', 'LEFT_BRACKET', 'RIGHT_BRACKET', 'LEFT_BRACE', 'RIGHT_BRACE', 'DOUBLE_QUOTE', 'SINGLE_QUOTE', 'ELLIPSIS', 'SLASH', 'BACKSLASH', 'AT_SYMBOL', 'HASH', 'DOLLAR', 'PERCENT', 'AMPERSAND', 'ASTERISK', 'PLUS', 'EQUALS', 'LESS_THAN', 'GREATER_THAN', 'PIPE', 'CARET', 'BACKTICK', 'TILDE']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. define punctuation map\n",
    "punctuation_map = {\n",
    "    ',': 'COMMA',\n",
    "    '.': 'PERIOD',\n",
    "    '?': 'QUESTION',\n",
    "    '!': 'EXCLAMATION',\n",
    "    ';': 'SEMICOLON',\n",
    "    ':': 'COLON',\n",
    "    '-': 'HYPHEN',\n",
    "    '–': 'EN_DASH',\n",
    "    '—': 'EM_DASH',\n",
    "    '(': 'LEFT_PAREN',\n",
    "    ')': 'RIGHT_PAREN',\n",
    "    '[': 'LEFT_BRACKET',\n",
    "    ']': 'RIGHT_BRACKET',\n",
    "    '{': 'LEFT_BRACE',\n",
    "    '}': 'RIGHT_BRACE',\n",
    "    '\"': 'DOUBLE_QUOTE',\n",
    "    \"'\": 'SINGLE_QUOTE',\n",
    "    '…': 'ELLIPSIS',\n",
    "    '/': 'SLASH',\n",
    "    '\\\\': 'BACKSLASH',\n",
    "    '@': 'AT_SYMBOL',\n",
    "    '#': 'HASH',\n",
    "    '$': 'DOLLAR',\n",
    "    '%': 'PERCENT',\n",
    "    '&': 'AMPERSAND',\n",
    "    '*': 'ASTERISK',\n",
    "    '+': 'PLUS',\n",
    "    '=': 'EQUALS',\n",
    "    '<': 'LESS_THAN',\n",
    "    '>': 'GREATER_THAN',\n",
    "    '|': 'PIPE',\n",
    "    '^': 'CARET',\n",
    "    '`': 'BACKTICK',\n",
    "    '~': 'TILDE'\n",
    "}\n",
    "\n",
    "# Automatically create label_list from punctuation_map\n",
    "label_list = [\"O\"] + list(punctuation_map.values())\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "print(\"Label list:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29147768",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_reverse_map = {v: k for k, v in punctuation_map.items()}\n",
    "punctuation_reverse_map[\"O\"] = \" \"   # no punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481fadb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 11:23:58.392136: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-25 11:23:58.456113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-25 11:24:00.572300: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"thenlpresearcher/bert_punct_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291c8d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_and_prediction_ids(text: str, model, tokenizer, device) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Takes an unpunctuated text string and returns the word IDs and prediction IDs \n",
    "    for all tokens in the sequence.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if not words:\n",
    "        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)\n",
    "    \n",
    "    encoded_input = tokenizer(\n",
    "        words, \n",
    "        is_split_into_words=True, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    print(encoded_input)\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0]))\n",
    "    \n",
    "    encoded_input = encoded_input.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Squeeze to handle single-item batch\n",
    "    pred_ids = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Convert list of word IDs (including None) to a numpy array, \n",
    "    # replacing None with a placeholder -1 for special tokens\n",
    "    word_ids_list = encoded_input.word_ids()\n",
    "    word_ids_array = np.array([w if w is not None else -1 for w in word_ids_list], dtype=np.int64)\n",
    "\n",
    "    return word_ids_array, pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a9e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Punctuation Restoration Results ---\n",
      "{'input_ids': tensor([[  101, 10166,  2008,  2003,  6429,  3475,  2102,  2009,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['[CLS]', 'wow', 'that', 'is', 'amazing', 'isn', '##t', 'it', '[SEP]']\n",
      "Word ids: [0 1 2 3 4 4 5]\n",
      "Prediction ids [ 1  0  0  1 17  0  3]\n",
      "\n",
      "Original: wow that is amazing isnt it\n",
      "Punctuated: Wow, that is amazing, isnt' it?\n"
     ]
    }
   ],
   "source": [
    "def restore_punctuation(text: str, model, tokenizer, label_list, device) -> str:\n",
    "    \"\"\"\n",
    "    Restores punctuation to an unpunctuated text string using the BERT Punctuation model.\n",
    "    \"\"\"\n",
    "    words = text.strip().split()\n",
    "    w_ids, p_ids = get_word_and_prediction_ids(text, model, tokenizer, device)\n",
    "    w_ids = w_ids[1:-1]\n",
    "    print(f\"Word ids: {w_ids}\")\n",
    "    \n",
    "    p_ids = p_ids[1:-1]\n",
    "    print(f\"Prediction ids {p_ids}\")\n",
    "    \n",
    "    final_output = []\n",
    "    seen = [False]*(len(words))\n",
    "    for w_id, p_id in zip(w_ids, p_ids):\n",
    "        if not seen[w_id]:\n",
    "            punct = punctuation_reverse_map[label_list[p_id]]\n",
    "            if punct != \" \":\n",
    "                punct = punct + \" \"\n",
    "            seen[w_id] = True\n",
    "            final_output.extend([words[w_id], punct])\n",
    "                 \n",
    "    # Join the words back into a sentence string, capitalizing the first letter.\n",
    "    result = \"\".join(final_output).strip()\n",
    "    \n",
    "    if result:\n",
    "        # Capitalize the first letter\n",
    "        return result[0].upper() + result[1:]\n",
    "    return \"\"\n",
    "\n",
    "print(\"\\n--- Punctuation Restoration Results ---\")\n",
    "\n",
    "# # Example 1\n",
    "# input_text_1 = \"how old are you i am a language model\"\n",
    "# punctuated_text_1 = restore_punctuation(input_text_1, model, tokenizer, label_list, device)\n",
    "# print(f\"Original: {input_text_1}\")\n",
    "# print(f\"Punctuated: {punctuated_text_1}\")\n",
    "\n",
    "# # Example 2\n",
    "# input_text_2 = \"what is the capital of france it is paris\"\n",
    "# punctuated_text_2 = restore_punctuation(input_text_2, model, tokenizer, label_list, device)\n",
    "# print(f\"\\nOriginal: {input_text_2}\")\n",
    "# print(f\"Punctuated: {punctuated_text_2}\")\n",
    "\n",
    "# # Example 3 (for demonstration of comma prediction)\n",
    "# input_text_3 = \"if you want to know more ask me anything\"\n",
    "# punctuated_text_3 = restore_punctuation(input_text_3, model, tokenizer, label_list, device)\n",
    "# print(f\"\\nOriginal: {input_text_3}\")\n",
    "# print(f\"Punctuated: {punctuated_text_3}\")\n",
    "\n",
    "# Example 4 (for demonstration of exclamation)\n",
    "input_text_4 = \"wow that is amazing isnt it\"\n",
    "punctuated_text_4 = restore_punctuation(input_text_4, model, tokenizer, label_list, device)\n",
    "print(f\"\\nOriginal: {input_text_4}\")\n",
    "print(f\"Punctuated: {punctuated_text_4}\")\n",
    "\n",
    "# # Example 4 (for demonstration of exclamation)\n",
    "# input_text_5 = \"i am kaustubh who are you\"\n",
    "# punctuated_text_5 = restore_punctuation(input_text_5, model, tokenizer, label_list, device)\n",
    "# print(f\"\\nOriginal: {input_text_5}\")\n",
    "# print(f\"Punctuated: {punctuated_text_5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c067e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_punctuation(text: str, model, tokenizer, label_list, device, punctuation_reverse_map) -> str:\n",
    "    \"\"\"\n",
    "    Restores punctuation to an unpunctuated text string, utilizing tokenizer subwords\n",
    "    and word_ids() for accurate mapping.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input text\n",
    "    words = re.findall(r\"\\w+|[^\\w\\s]\", text.strip())\n",
    "    print(words)\n",
    "    encoded_input = tokenizer(\n",
    "        words, \n",
    "        is_split_into_words=True, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # 2. Extract word IDs and perform inference\n",
    "    word_ids_list = encoded_input.word_ids() # Maps subword token index to original word index (or None for special tokens)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "    tokens = tokens[1:-1]\n",
    "    # Run model inference to get logits/predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get the predicted label index (p_id) for each token\n",
    "    # We take the first element [0] because the input is a batch of size 1\n",
    "    p_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    p_ids = p_ids[1:-1]\n",
    "    word_ids_list = word_ids_list[1:-1]\n",
    "    print(word_ids_list)\n",
    "    print(p_ids)\n",
    "    print(tokens)\n",
    "    \n",
    "    final_output = []\n",
    "    \n",
    "    i = 0\n",
    "    for t, p in zip(tokens, p_ids):\n",
    "        punct = punctuation_reverse_map[label_list[p]]\n",
    "        t = t.strip('#')\n",
    "        if t == punct:\n",
    "            continue\n",
    "        if punct != \" \" and i < len(word_ids_list) - 1 and word_ids_list[i] != word_ids_list[i+1]:\n",
    "            punct = punct + \" \"\n",
    "        if punct == \" \" and i < len(word_ids_list) - 1 and word_ids_list[i] == word_ids_list[i+1]:\n",
    "            punct = \"\"\n",
    "        \n",
    "        if i < len(word_ids_list) - 1 and word_ids_list[i] == word_ids_list[i+1] and p_ids[i] == p_ids[i+1]:\n",
    "            final_output.append(t)\n",
    "        else:\n",
    "            final_output.extend([t, punct])\n",
    "        i = i + 1\n",
    "        \n",
    "    # 6. Final cleanup\n",
    "    result = \"\".join(final_output).strip()\n",
    "    \n",
    "    if result:\n",
    "        # Capitalize the first letter\n",
    "        return result[0].upper() + result[1:]\n",
    "    return \"\"\n",
    "\n",
    "# Assume model, tokenizer, label_list, device, and punctuation_reverse_map are defined\n",
    "# NOTE: You will need to define or pass the 'punctuation_reverse_map' dictionary for this to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e02680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'six', '-', 'month', '-', 'old', 'calf', 'was', 'submitted', 'for', 'examination', ',', 'showing', 'lameness', 'in', 'all', 'four', 'legs', 'which', 'had', 'been', 'present', 'since', 'soon', 'after', 'birth']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[0, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "['a', 'six', '-', 'month', '-', 'old', 'calf', 'was', 'submitted', 'for', 'examination', ',', 'showing', 'lame', '##ness', 'in', 'all', 'four', 'legs', 'which', 'had', 'been', 'present', 'since', 'soon', 'after', 'birth']\n",
      "\n",
      "Original: A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth\n",
      "Punctuated: A six- month- old calf was submitted for examination , showing lame ness inall four legs, which had been present since soon after birth.\n"
     ]
    }
   ],
   "source": [
    "input_text_4 = \"A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth.\"\n",
    "import string\n",
    "input_text_4 = input_text_4.strip('.')\n",
    "punctuated_text_4 = restore_punctuation(input_text_4, model, tokenizer, label_list, device, punctuation_reverse_map)\n",
    "print(f\"\\nOriginal: {input_text_4}\")\n",
    "print(f\"Punctuated: {punctuated_text_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cf58ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'company', 'is', 'in', 'tiruchirapalli', 'i', 'am', 'good', 'how', 'are', 'you']\n",
      "[0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 5, 6, 7, 8, 9, 10]\n",
      "[0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 3]\n",
      "['good', 'company', 'is', 'in', 'ti', '##ru', '##chi', '##ra', '##pal', '##li', 'i', 'am', 'good', 'how', 'are', 'you']\n",
      "\n",
      "Original: good company is in tiruchirapalli i am good how are you\n",
      "Punctuated: Good company is in tiruchirapalli. i am good. how are you?\n"
     ]
    }
   ],
   "source": [
    "input_text_4 = \"good company is in tiruchirapalli i am good how are you\"\n",
    "punctuated_text_4 = restore_punctuation(input_text_4, model, tokenizer, label_list, device, punctuation_reverse_map)\n",
    "print(f\"\\nOriginal: {input_text_4}\")\n",
    "print(f\"Punctuated: {punctuated_text_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b46cfcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_labels(sentence):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    parts = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r\"\\w+\", part):  # token\n",
    "            tokens.append(part)\n",
    "            if i+1 < len(parts) and parts[i+1] in punctuation_map:\n",
    "                labels.append(punctuation_map[parts[i+1]])\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e855d516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
