{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3795637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 12:09:21 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              68W / 300W |  58395MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              90W / 300W |  68866MiB / 81920MiB |     31%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              73W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   68C    P0             305W / 300W |  49852MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2ae6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab205f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MPNetForTokenClassification(\n",
       "  (mpnet): MPNetModel(\n",
       "    (embeddings): MPNetEmbeddings(\n",
       "      (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): MPNetEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x MPNetLayer(\n",
       "          (attention): MPNetAttention(\n",
       "            (attn): MPNetSelfAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (intermediate): MPNetIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): MPNetOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (relative_attention_bias): Embedding(32, 12)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=35, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load tokenizer and model\n",
    "# model_name = \"thenlpresearcher/bert_punct_model\"\n",
    "model_name = \"thenlpresearcher/mpnet_token_cls_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a6e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label list: ['O', 'COMMA', 'PERIOD', 'QUESTION', 'EXCLAMATION', 'SEMICOLON', 'COLON', 'HYPHEN', 'EN_DASH', 'EM_DASH', 'LEFT_PAREN', 'RIGHT_PAREN', 'LEFT_BRACKET', 'RIGHT_BRACKET', 'LEFT_BRACE', 'RIGHT_BRACE', 'DOUBLE_QUOTE', 'SINGLE_QUOTE', 'ELLIPSIS', 'SLASH', 'BACKSLASH', 'AT_SYMBOL', 'HASH', 'DOLLAR', 'PERCENT', 'AMPERSAND', 'ASTERISK', 'PLUS', 'EQUALS', 'LESS_THAN', 'GREATER_THAN', 'PIPE', 'CARET', 'BACKTICK', 'TILDE']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. define punctuation map\n",
    "punctuation_map = {\n",
    "    ',': 'COMMA',\n",
    "    '.': 'PERIOD',\n",
    "    '?': 'QUESTION',\n",
    "    '!': 'EXCLAMATION',\n",
    "    ';': 'SEMICOLON',\n",
    "    ':': 'COLON',\n",
    "    '-': 'HYPHEN',\n",
    "    '–': 'EN_DASH',\n",
    "    '—': 'EM_DASH',\n",
    "    '(': 'LEFT_PAREN',\n",
    "    ')': 'RIGHT_PAREN',\n",
    "    '[': 'LEFT_BRACKET',\n",
    "    ']': 'RIGHT_BRACKET',\n",
    "    '{': 'LEFT_BRACE',\n",
    "    '}': 'RIGHT_BRACE',\n",
    "    '\"': 'DOUBLE_QUOTE',\n",
    "    \"'\": 'SINGLE_QUOTE',\n",
    "    '…': 'ELLIPSIS',\n",
    "    '/': 'SLASH',\n",
    "    '\\\\': 'BACKSLASH',\n",
    "    '@': 'AT_SYMBOL',\n",
    "    '#': 'HASH',\n",
    "    '$': 'DOLLAR',\n",
    "    '%': 'PERCENT',\n",
    "    '&': 'AMPERSAND',\n",
    "    '*': 'ASTERISK',\n",
    "    '+': 'PLUS',\n",
    "    '=': 'EQUALS',\n",
    "    '<': 'LESS_THAN',\n",
    "    '>': 'GREATER_THAN',\n",
    "    '|': 'PIPE',\n",
    "    '^': 'CARET',\n",
    "    '`': 'BACKTICK',\n",
    "    '~': 'TILDE'\n",
    "}\n",
    "\n",
    "# Automatically create label_list from punctuation_map\n",
    "label_list = [\"O\"] + list(punctuation_map.values())\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "print(\"Label list:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c76c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def create_token_labels(sentence):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    parts = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r\"\\w+\", part):  # token\n",
    "            tokens.append(part)\n",
    "            if i+1 < len(parts) and parts[i+1] in punctuation_map:\n",
    "                labels.append(punctuation_map[parts[i+1]])\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, labels = create_token_labels(\"Wow, this is amazing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "745a6df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wow', 'COMMA'), ('this', 'O'), ('is', 'O'), ('amazing', 'EXCLAMATION')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e1b87c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████| 54/54 [00:00<00:00, 1963.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Token-label creation function\n",
    "# -------------------------------\n",
    "def create_token_labels(sentence):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    parts = re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r\"\\w+\", part):  # token\n",
    "            tokens.append(part)\n",
    "            if i+1 < len(parts) and parts[i+1] in punctuation_map:\n",
    "                labels.append(punctuation_map[parts[i+1]])\n",
    "            else:\n",
    "                labels.append(\"O\")\n",
    "    return tokens, labels\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load CSV and create token-label dataset\n",
    "# -------------------------------\n",
    "def load_and_process(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sent in df['sent_meant']:\n",
    "        tokens, labels = create_token_labels(str(sent))\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append([label_to_id[l] for l in labels])\n",
    "\n",
    "    return Dataset.from_dict({\"tokens\": all_tokens, \"labels\": all_labels})\n",
    "\n",
    "\n",
    "# test_dataset  = load_and_process(\"iwslt2017_en_test.csv\")\n",
    "test_dataset = load_dataset(\"thenlpresearcher/test_data_marathi\")[\"test\"]\n",
    "\n",
    "def load_and_process_hf(ds, text_column=\"sent_meant\"):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for sent in ds[text_column]:\n",
    "        tokens, labels = create_token_labels(str(sent))\n",
    "        all_tokens.append(tokens)\n",
    "        all_labels.append([label_to_id[l] for l in labels])\n",
    "\n",
    "    return Dataset.from_dict({\"tokens\": all_tokens, \"labels\": all_labels})\n",
    "\n",
    "test_dataset = load_and_process_hf(test_dataset)\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"], is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    new_labels = []\n",
    "    for i, label in enumerate(batch[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                aligned_labels.append(label[word_id])\n",
    "        new_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "test_dataset  = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "test_dataset  = test_dataset.remove_columns([\"tokens\"])\n",
    "\n",
    "test_dataset.set_format(type=\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8246f56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc44675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38087/4103322971.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Make predictions on test dataset\n",
    "# -------------------------------\n",
    "predictions, labels, _ = trainer.predict(test_dataset)  # test_dataset must be already prepared\n",
    "pred_ids = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Map predictions and labels back to strings\n",
    "# -------------------------------\n",
    "true_labels_list = []\n",
    "pred_labels_list = []\n",
    "\n",
    "for label_row, pred_row in zip(labels, pred_ids):\n",
    "    true_row = []\n",
    "    pred_row_labels = []\n",
    "    for l, p in zip(label_row, pred_row):\n",
    "        if l != -100:  # ignore padding\n",
    "            true_row.append(label_list[l])\n",
    "            pred_row_labels.append(label_list[p])\n",
    "    true_labels_list.append(true_row)\n",
    "    pred_labels_list.append(pred_row_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34acc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       COLON     1.0000    0.6000    0.7500         5\n",
      "       COMMA     0.7606    0.6835    0.7200        79\n",
      "      DOLLAR     0.0000    0.0000    0.0000         0\n",
      "DOUBLE_QUOTE     0.0000    0.0000    0.0000         0\n",
      "     EM_DASH     0.0000    0.0000    0.0000         1\n",
      " EXCLAMATION     0.0000    0.0000    0.0000         1\n",
      "      HYPHEN     0.9000    0.5294    0.6667        17\n",
      "           O     0.9574    0.9728    0.9650       808\n",
      "     PERCENT     0.0000    0.0000    0.0000         2\n",
      "      PERIOD     0.8750    0.9825    0.9256        57\n",
      "    QUESTION     1.0000    1.0000    1.0000         1\n",
      " RIGHT_PAREN     0.0000    0.0000    0.0000         2\n",
      "   SEMICOLON     0.0000    0.0000    0.0000         2\n",
      "SINGLE_QUOTE     0.0000    0.0000    0.0000         2\n",
      "       SLASH     1.0000    0.3333    0.5000         3\n",
      "\n",
      "    accuracy                         0.9286       980\n",
      "   macro avg     0.4329    0.3401    0.3685       980\n",
      "weighted avg     0.9263    0.9286    0.9255       980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Flatten lists for sklearn\n",
    "y_true_flat = [label for seq in true_labels_list for label in seq]\n",
    "y_pred_flat = [label for seq in pred_labels_list for label in seq]\n",
    "\n",
    "# print(\"\\nDetailed classification report (per label) using sklearn:\")\n",
    "print(classification_report(y_true_flat, y_pred_flat, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf161b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
