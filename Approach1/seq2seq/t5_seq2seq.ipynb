{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dc24b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: apex 0.1\n",
      "Uninstalling apex-0.1:\n",
      "  Successfully uninstalled apex-0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall apex -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26ce746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 24 14:16:15 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   56C    P0              71W / 300W |  31780MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              71W / 300W |  58562MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              74W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   64C    P0             213W / 300W |   5176MiB / 81920MiB |     63%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993b1fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach1/seq2seq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bccef43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîΩ Loading IWSLT2017 English‚ÄìGerman dataset...\n",
      "üìÑ Extracting English sentences from each split...\n",
      "üíæ Saving splits...\n",
      "\n",
      "‚úÖ Done!\n",
      "Train sentences: 206,112\n",
      "Validation sentences: 888\n",
      "Test sentences: 8,079\n",
      "\n",
      "Saved files:\n",
      " - iwslt2017_en_train.csv\n",
      " - iwslt2017_en_valid.csv\n",
      " - iwslt2017_en_test.csv\n",
      "\n",
      "üîç Example:\n",
      "                                       train_example\n",
      "0                          Thank you so much, Chris.\n",
      "1  And it's truly a great honor to have the oppor...\n",
      "2  I have been blown away by this conference, and...\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load dataset\n",
    "print(\"üîΩ Loading IWSLT2017 English‚ÄìGerman dataset...\")\n",
    "dataset = load_dataset(\"IWSLT/iwslt2017\", \"iwslt2017-en-de\")\n",
    "\n",
    "# Step 2: Extract English sentences from each split\n",
    "def extract_english(ds):\n",
    "    \"\"\"Extract English sentences from a dataset split.\"\"\"\n",
    "    return [ex[\"translation\"][\"en\"] for ex in ds]\n",
    "\n",
    "print(\"üìÑ Extracting English sentences from each split...\")\n",
    "train_en = extract_english(dataset[\"train\"])\n",
    "valid_en = extract_english(dataset[\"validation\"])\n",
    "test_en  = extract_english(dataset[\"test\"])\n",
    "\n",
    "# Step 3: Save each split separately\n",
    "print(\"üíæ Saving splits...\")\n",
    "\n",
    "pd.DataFrame({\"text\": train_en}).to_csv(\"iwslt2017_en_train.csv\", index=False)\n",
    "pd.DataFrame({\"text\": valid_en}).to_csv(\"iwslt2017_en_valid.csv\", index=False)\n",
    "pd.DataFrame({\"text\": test_en}).to_csv(\"iwslt2017_en_test.csv\", index=False)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Done!\n",
    "Train sentences: {len(train_en):,}\n",
    "Validation sentences: {len(valid_en):,}\n",
    "Test sentences: {len(test_en):,}\n",
    "\n",
    "Saved files:\n",
    " - iwslt2017_en_train.csv\n",
    " - iwslt2017_en_valid.csv\n",
    " - iwslt2017_en_test.csv\n",
    "\"\"\")\n",
    "\n",
    "# Optional preview\n",
    "print(\"üîç Example:\")\n",
    "print(pd.DataFrame({'train_example': train_en[:3]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c30ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 src  \\\n",
      "0                            Thank you so much Chris   \n",
      "1  And its truly a great honor to have the opport...   \n",
      "2  I have been blown away by this conference and ...   \n",
      "3  And I say that sincerely partly because  I nee...   \n",
      "4                      Put yourselves in my position   \n",
      "\n",
      "                                                 tgt  \n",
      "0                          Thank you so much, Chris.  \n",
      "1  And it's truly a great honor to have the oppor...  \n",
      "2  I have been blown away by this conference, and...  \n",
      "3  And I say that sincerely, partly because  I ne...  \n",
      "4                     Put yourselves in my position.  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Function to remove punctuation\n",
    "# -------------------------------\n",
    "def remove_puncts(text_series):\n",
    "    \"\"\"\n",
    "    Remove all punctuation from a Pandas Series of text.\n",
    "    \"\"\"\n",
    "    return text_series.str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to prepare dataset\n",
    "# -------------------------------\n",
    "def prepare_punct_dataset(df, text_col=\"text\"):\n",
    "    \"\"\"\n",
    "    Given a DataFrame, create 'src' (punctuation removed) and 'tgt' (original) columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['src'] = remove_puncts(df[text_col])\n",
    "    df['tgt'] = df[text_col]\n",
    "    return df[['src', 'tgt']]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Example usage\n",
    "# -------------------------------\n",
    "# Load CSVs\n",
    "train_df = pd.read_csv(\"iwslt2017_en_train.csv\")\n",
    "val_df = pd.read_csv(\"iwslt2017_en_valid.csv\")\n",
    "test_df = pd.read_csv(\"iwslt2017_en_test.csv\")\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = prepare_punct_dataset(train_df)\n",
    "val_dataset = prepare_punct_dataset(val_df)\n",
    "test_dataset = prepare_punct_dataset(test_df)\n",
    "\n",
    "# Quick check\n",
    "print(train_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5925d682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b443c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306bfdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/207 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  34%|‚ñé| 71/207 [00:00<00:00, 703.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|‚ñà| 207/207 [00:00<00:00, 679.82ba/s\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.75s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 178.28ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 940.87ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/english_punctuation_restoration/commit/9e20f337daf0f00dfe98d454f96908cb53609d17', commit_message='Upload dataset', commit_description='', oid='9e20f337daf0f00dfe98d454f96908cb53609d17', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/english_punctuation_restoration', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/english_punctuation_restoration'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = Features({\n",
    "    \"src\": Value(\"string\"),\n",
    "    \"tgt\": Value(\"string\")\n",
    "})\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_dataset, features=features)\n",
    "hf_val = Dataset.from_pandas(val_dataset, features=features)\n",
    "hf_test = Dataset.from_pandas(test_dataset, features=features)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": hf_train,\n",
    "    \"validation\": hf_val,\n",
    "    \"test\": hf_test\n",
    "})\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Push dataset to Hugging Face Hub\n",
    "# -------------------------------\n",
    "dataset_name = \"english_punctuation_restoration\"\n",
    "dataset.push_to_hub(dataset_name, private=False)  # set private=True if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43b3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thenlpresearcher/english_punctuation_restoration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f96f1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# T5 base checkpoint\n",
    "model_checkpoint = \"google-t5/t5-base\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1005ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And its truly a great honor to have the opportunity to come to this stage twice Im extremely grateful\n",
      "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [275, 165, 1892, 3, 9, 248, 3610, 12, 43, 8, 1004, 12, 369, 12, 48, 1726, 4394, 1318, 2033, 7335, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [275, 34, 31, 7, 1892, 3, 9, 248, 3610, 12, 43, 8, 1004, 12, 369, 12, 48, 1726, 4394, 117, 27, 31, 51, 2033, 7335, 5, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_punct_sentence = dataset[\"train\"][1][\"src\"]\n",
    "punct_sentence = dataset[\"train\"][1][\"tgt\"]\n",
    "\n",
    "inputs = tokenizer(without_punct_sentence, text_target=punct_sentence)\n",
    "print(dataset[\"train\"][1][\"src\"])\n",
    "print(dataset[\"train\"][1][\"tgt\"])\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678f73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅAnd', '‚ñÅit', \"'\", 's', '‚ñÅtruly', '‚ñÅ', 'a', '‚ñÅgreat', '‚ñÅhonor', '‚ñÅto', '‚ñÅhave', '‚ñÅthe', '‚ñÅopportunity', '‚ñÅto', '‚ñÅcome', '‚ñÅto', '‚ñÅthis', '‚ñÅstage', '‚ñÅtwice', ';', '‚ñÅI', \"'\", 'm', '‚ñÅextremely', '‚ñÅgrateful', '.', '</s>']\n",
      "['‚ñÅAnd', '‚ñÅit', \"'\", 's', '‚ñÅtruly', '‚ñÅ', 'a', '‚ñÅgreat', '‚ñÅhonor', '‚ñÅto', '‚ñÅhave', '‚ñÅthe', '‚ñÅopportunity', '‚ñÅto', '‚ñÅcome', '‚ñÅto', '‚ñÅthis', '‚ñÅstage', '‚ñÅtwice', ';', '‚ñÅI', \"'\", 'm', '‚ñÅextremely', '‚ñÅgrateful', '.', '</s>']\n",
      "['‚ñÅAnd', '‚ñÅits', '‚ñÅtruly', '‚ñÅ', 'a', '‚ñÅgreat', '‚ñÅhonor', '‚ñÅto', '‚ñÅhave', '‚ñÅthe', '‚ñÅopportunity', '‚ñÅto', '‚ñÅcome', '‚ñÅto', '‚ñÅthis', '‚ñÅstage', '‚ñÅtwice', '‚ñÅIm', '‚ñÅextremely', '‚ñÅgrateful', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(punct_sentence)\n",
    "targets1 = tokenizer(without_punct_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(targets1[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abcd061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"src\"]]\n",
    "    targets = [ex for ex in examples[\"tgt\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b56a638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 206112/206112 [00:12<00:00, 16585.09 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 888/888 [00:00<00:00, 13270.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8079/8079 [00:00<00:00, 20575.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Suppose you already have `dataset` with splits train/val/test\n",
    "# and a preprocess_function defined\n",
    "\n",
    "tokenized_datasets = {}\n",
    "for split in dataset.keys():  # e.g., \"train\", \"validation\", \"test\"\n",
    "    tokenized_datasets[split] = dataset[split].map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[split].column_names\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c10795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 06:48:57.763633: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-25 06:48:57.891760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-25 06:49:00.112919: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a4a9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186153f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff3858af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  275,    34,    31,     7,  1892,     3,     9,   248,  3610,    12,\n",
       "            43,     8,  1004,    12,   369,    12,    48,  1726,  4394,   117,\n",
       "            27,    31,    51,  2033,  7335,     5,     1,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100],\n",
       "        [   27,    43,   118,     3, 17378,   550,    57,    48,  2542,     6,\n",
       "            11,    27,   241,    12,  2763,    66,    13,    25,    21,     8,\n",
       "           186,  1245,  2622,    81,   125,    27,   141,    12,   497,     8,\n",
       "           119,   706,     5,     1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17f8018f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   275,    34,    31,     7,  1892,     3,     9,   248,  3610,\n",
       "            12,    43,     8,  1004,    12,   369,    12,    48,  1726,  4394,\n",
       "           117,    27,    31,    51,  2033,  7335,     5,     1,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [    0,    27,    43,   118,     3, 17378,   550,    57,    48,  2542,\n",
       "             6,    11,    27,   241,    12,  2763,    66,    13,    25,    21,\n",
       "             8,   186,  1245,  2622,    81,   125,    27,   141,    12,   497,\n",
       "             8,   119,   706,     5]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc3a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[275, 34, 31, 7, 1892, 3, 9, 248, 3610, 12, 43, 8, 1004, 12, 369, 12, 48, 1726, 4394, 117, 27, 31, 51, 2033, 7335, 5, 1]\n",
      "[27, 43, 118, 3, 17378, 550, 57, 48, 2542, 6, 11, 27, 241, 12, 2763, 66, 13, 25, 21, 8, 186, 1245, 2622, 81, 125, 27, 141, 12, 497, 8, 119, 706, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e531618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3f3abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676281a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615862b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"iitb-t5-finetuned-punctuation\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9b0234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17797/543751272.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bad5f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.501761794090271,\n",
       " 'eval_model_preparation_time': 0.0163,\n",
       " 'eval_bleu': 18.28442992448596,\n",
       " 'eval_runtime': 26.1347,\n",
       " 'eval_samples_per_second': 33.978,\n",
       " 'eval_steps_per_second': 0.536}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e954d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19323' max='19323' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19323/19323 34:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.094745</td>\n",
       "      <td>52.882251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.090962</td>\n",
       "      <td>52.969104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083200</td>\n",
       "      <td>0.089705</td>\n",
       "      <td>53.029294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19323, training_loss=0.0932592289211994, metrics={'train_runtime': 2094.0836, 'train_samples_per_second': 295.278, 'train_steps_per_second': 9.227, 'total_flos': 4.520747334057984e+16, 'train_loss': 0.0932592289211994, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2512f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (5 / 5): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|  892MB /  892MB,  179MB/s  \u001b[A\n",
      "New Data Upload: |                             |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thenlpresearcher/iitb-t5-finetuned-punctuation/commit/f924c3a798ad838f8f05be932d4bbd0e0d113555', commit_message='Training complete', commit_description='', oid='f924c3a798ad838f8f05be932d4bbd0e0d113555', pr_url=None, repo_url=RepoUrl('https://huggingface.co/thenlpresearcher/iitb-t5-finetuned-punctuation', endpoint='https://huggingface.co', repo_type='model', repo_id='thenlpresearcher/iitb-t5-finetuned-punctuation'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(tags=\"text2text-generation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd039ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53100f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62995274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# This might accidentally default to a translation task\n",
    "punctuator_pipeline = pipeline(\"text2text-generation\", model=\"thenlpresearcher/iitb-t5-finetuned-punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c978b881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets.'}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets\"\n",
    "punctuator_pipeline(text,\n",
    "                   max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8ac8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets.'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# This might accidentally default to a translation task\n",
    "punctuator_pipeline = pipeline(\"text2text-generation\", model=\"thenlpresearcher/iitb-t5-finetuned-punctuation\")\n",
    "\n",
    "text = \"the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets\"\n",
    "punctuator_pipeline(text,\n",
    "                   max_length=128)\n",
    "\n",
    "#output\n",
    "# [{'generated_text': 'the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47326916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8079"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd8fc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': 'Several years ago here at TED Peter Skillman  introduced a design challenge  called the marshmallow challenge',\n",
       " 'tgt': 'Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e36171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 25 12:20:59 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   52C    P0              67W / 300W |  57384MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              68W / 300W |  68866MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   54C    P0              73W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   64C    P0              77W / 300W |  52152MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cbed063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach1/seq2seq\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61debad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f948960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8079\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use batching + GPU (if available)\n",
    "punctuator_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"thenlpresearcher/iitb-t5-finetuned-punctuation\",\n",
    "    device=device,            # GPU; use device=-1 for CPU\n",
    "    batch_size=64        # adjust based on GPU RAM\n",
    ")\n",
    "\n",
    "def restore_punctuation_t5_batch(text_list):\n",
    "    # The pipeline automatically batches under the hood\n",
    "    outputs = punctuator_pipeline(\n",
    "        text_list,\n",
    "        max_length=128\n",
    "    )\n",
    "    # Pipeline returns list of dicts\n",
    "    return [o[\"generated_text\"] for o in outputs]\n",
    "\n",
    "# Collect all input sentences\n",
    "src_texts = list(dataset[\"test\"][\"src\"])\n",
    "\n",
    "# Run the whole batch in parallel\n",
    "predicted_sentences = restore_punctuation_t5_batch(src_texts)\n",
    "\n",
    "# # Optional: print paired outputs\n",
    "# for src, pred in zip(src_texts, predicted_sentences):\n",
    "#     print(src)\n",
    "#     print(pred)\n",
    "#     print(\"---\")\n",
    "print(len(predicted_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6711038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: approach1_eng_to_eng_t5_outputs_punct_restor_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a brand-new empty dataframe\n",
    "df = pd.DataFrame()\n",
    "# Add model predictions\n",
    "df[\"prediction\"] = predicted_sentences\n",
    "\n",
    "# Add source fields from HF dataset\n",
    "df[\"src\"] = dataset[\"test\"][\"src\"]\n",
    "df[\"gt\"]   = dataset[\"test\"][\"tgt\"]\n",
    "\n",
    "# Save the file\n",
    "output_file = \"approach1_eng_to_eng_t5_outputs_punct_restor_data.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Saved:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2db3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
