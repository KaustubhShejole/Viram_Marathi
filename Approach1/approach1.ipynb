{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cdfa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93691056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bed42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da175cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. define punctuation map\n",
    "punctuation_map = {\n",
    "    ',': 'COMMA',\n",
    "    '.': 'PERIOD',\n",
    "    '?': 'QUESTION',\n",
    "    '!': 'EXCLAMATION',\n",
    "    ';': 'SEMICOLON',\n",
    "    ':': 'COLON',\n",
    "    '-': 'HYPHEN',\n",
    "    'â€“': 'EN_DASH',\n",
    "    'â€”': 'EM_DASH',\n",
    "    '(': 'LEFT_PAREN',\n",
    "    ')': 'RIGHT_PAREN',\n",
    "    '[': 'LEFT_BRACKET',\n",
    "    ']': 'RIGHT_BRACKET',\n",
    "    '{': 'LEFT_BRACE',\n",
    "    '}': 'RIGHT_BRACE',\n",
    "    '\"': 'DOUBLE_QUOTE',\n",
    "    \"'\": 'SINGLE_QUOTE',\n",
    "    'â€¦': 'ELLIPSIS',\n",
    "    '/': 'SLASH',\n",
    "    '\\\\': 'BACKSLASH',\n",
    "    '@': 'AT_SYMBOL',\n",
    "    '#': 'HASH',\n",
    "    '$': 'DOLLAR',\n",
    "    '%': 'PERCENT',\n",
    "    '&': 'AMPERSAND',\n",
    "    '*': 'ASTERISK',\n",
    "    '+': 'PLUS',\n",
    "    '=': 'EQUALS',\n",
    "    '<': 'LESS_THAN',\n",
    "    '>': 'GREATER_THAN',\n",
    "    '|': 'PIPE',\n",
    "    '^': 'CARET',\n",
    "    '`': 'BACKTICK',\n",
    "    '~': 'TILDE'\n",
    "}\n",
    "\n",
    "# Automatically create label_list from punctuation_map\n",
    "label_list = [\"O\"] + list(punctuation_map.values())\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "\n",
    "print(\"Label list:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee995e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_reverse_map = {v: k for k, v in punctuation_map.items()}\n",
    "punctuation_reverse_map[\"O\"] = \"\"   # no punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "raw_datasets = load_dataset(\"thenlpresearcher/test_data_marathi\")\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def restore_punctuation(text: str, model, tokenizer, label_list, device, punctuation_reverse_map) -> str:\n",
    "    \"\"\"\n",
    "    Restores punctuation to an unpunctuated text string, utilizing tokenizer subwords\n",
    "    and word_ids() for accurate mapping.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input text\n",
    "    words = re.findall(r\"\\w+|[^\\w\\s]\", text.strip())\n",
    "#     print(words)\n",
    "    encoded_input = tokenizer(\n",
    "        words, \n",
    "        is_split_into_words=True, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # 2. Extract word IDs and perform inference\n",
    "    word_ids_list = encoded_input.word_ids() # Maps subword token index to original word index (or None for special tokens)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])\n",
    "    tokens = tokens[1:-1]\n",
    "    # Run model inference to get logits/predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_input)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get the predicted label index (p_id) for each token\n",
    "    # We take the first element [0] because the input is a batch of size 1\n",
    "    p_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "    \n",
    "    p_ids = p_ids[1:-1]\n",
    "    word_ids_list = word_ids_list[1:-1]\n",
    "#     print(word_ids_list)\n",
    "#     print(p_ids)\n",
    "#     print(tokens)\n",
    "    \n",
    "    final_output = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(len(tokens)):\n",
    "        t = tokens[i]\n",
    "        p = p_ids[i]\n",
    "        punct = punctuation_reverse_map[label_list[p]]\n",
    "        \n",
    "        t = t.strip('#')\n",
    "        if t == punct:\n",
    "            continue\n",
    "        \n",
    "        if i < len(tokens) -1 and punct == tokens[i+1]:\n",
    "            final_output.extend([t, punct])\n",
    "            i = i + 2\n",
    "            continue\n",
    "            \n",
    "        if punct != \" \" and i < len(word_ids_list) - 1 and word_ids_list[i] != word_ids_list[i+1]:\n",
    "            punct = punct + \" \"\n",
    "        if punct == \" \" and i < len(word_ids_list) - 1 and word_ids_list[i] == word_ids_list[i+1]:\n",
    "            punct = \"\"\n",
    "        \n",
    "        \n",
    "        if i < len(word_ids_list) - 1 and word_ids_list[i] == word_ids_list[i+1] and p_ids[i] == p_ids[i+1]:\n",
    "            final_output.append(t)\n",
    "        else:\n",
    "            final_output.extend([t, punct])\n",
    "        i = i + 1\n",
    "        \n",
    "    # 6. Final cleanup\n",
    "    result = \"\".join(final_output).strip()\n",
    "    \n",
    "    if result:\n",
    "        # Capitalize the first letter\n",
    "        return result[0].upper() + result[1:]\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"thenlpresearcher/mpnet_token_cls_model\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"thenlpresearcher/mpnet_token_cls_model\").to(device)\n",
    "\n",
    "# sentence = \"i am going to school but i forgot my bag\"\n",
    "# print(restore_punctuation(sentence, model, tokenizer, label_list, device, punctuation_reverse_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3333e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# This might accidentally default to a translation task\n",
    "punctuator_pipeline = pipeline(\"text2text-generation\", model=\"thenlpresearcher/iitb-t5-finetuned-punctuation\")\n",
    "\n",
    "text = \"the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets\"\n",
    "punctuator_pipeline(text,\n",
    "                   max_length=128)[0]['generated_text']\n",
    "\n",
    "#output\n",
    "# [{'generated_text': 'the morning sky stretched over the city like a quiet sheet of pale blue while people hurried through the streets.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_punctuation_t5(text):\n",
    "    return punctuator_pipeline(text, max_length=128)[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_sentences = []\n",
    "\n",
    "# for text in raw_datasets[\"test\"]['sent_written']:\n",
    "#     pred = restore_punctuation(text.strip('.').strip('?'), model, tokenizer, label_list, device, punctuation_reverse_map)\n",
    "#     predicted_sentences.append(pred)\n",
    "#     print(text)\n",
    "#     print(pred)\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a3e9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentences = []\n",
    "\n",
    "for text in raw_datasets[\"test\"]['sent_written']:\n",
    "    pred = restore_punctuation_t5(text)\n",
    "    predicted_sentences.append(pred)\n",
    "    print(text)\n",
    "    print(pred)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8773d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a brand-new empty dataframe\n",
    "df = pd.DataFrame()\n",
    "# Add model predictions\n",
    "df[\"prediction\"] = predicted_sentences\n",
    "\n",
    "# Add source fields from HF dataset\n",
    "df[\"src\"] = raw_datasets[\"test\"][\"sent_written\"]\n",
    "df[\"gt\"]   = raw_datasets[\"test\"][\"sent_meant\"]\n",
    "\n",
    "# Save the file\n",
    "output_file = \"approach1_eng_to_eng_t5_outputs_mar_data.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"Saved:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e80dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=5,  # ensure some minimum length\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Postprocess translations (remove language prefix, entity replacement, etc.)\n",
    "        translations += ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802d36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "db4d60783a2b4cc9ac647f53524e9201",
      "23e6dd2b833d44aabc5b78a83d7ef136",
      "8aff67fa78c640bf83598dd7fccc5b97",
      "97ed312717014144af97d250fcd74d9d",
      "7b69f1226cf5457aa164146a890a2c5e",
      "d5ce787580584a40883fdac45df7917a",
      "1962a04347894250993d2eb3d2946e5c",
      "d5e1f379a1084c6e978a633698b4b02a",
      "b689909adc244204af714b9a19e158c0",
      "8c32a5f5cb724610a4ea8a5b42be5b28",
      "44db6a1a111148ceaa804f7302b0629b",
      "7e8253e0de7f426c9f9af4b15775c660",
      "adfbc16f0b6144b6ab06544ed060f8fe",
      "1425686f748e45b9921c501d0e5ce904",
      "632fb54ffc9b44b98d082d097c17d7de"
     ]
    },
    "id": "agDlgrOix5Uj",
    "outputId": "d52e9acb-10d0-464f-c9f1-1947b7a7b0e6"
   },
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "\n",
    "    # Quantization setup\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    # Move to device and optionally convert to half precision\n",
    "    if qconfig is None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    # Make sure model is in training mode for fine-tuning\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=5,  # ensure some minimum length\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Postprocess translations (remove language prefix, entity replacement, etc.)\n",
    "        translations += ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4eea6f",
   "metadata": {
    "id": "E_XK7px-IDkC"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the Seq2SeqTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7b3a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = en_indic_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa0420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = en_indic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a7a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07d5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = IndicProcessor(inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9359f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------- LOAD DATA --------------------\n",
    "src_sentences = predicted_sentences\n",
    "ref_gt     = raw_datasets['test'][\"gt_marathi\"]\n",
    "ref_gem    = raw_datasets['test'][\"gemini_out\"]\n",
    "ref_cfilt  = raw_datasets['test'][\"cfilt_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b82c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(src_sentences))\n",
    "print(len(ref_gt))\n",
    "print(len(ref_gem))\n",
    "print(len(ref_cfilt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39cb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad541fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans with safety checks.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    # Safe access for decoder_start_token_id\n",
    "    decoder_start_token_id = getattr(model.config, \"decoder_start_token_id\", None)\n",
    "    pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "    eos_token_id = getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    if decoder_start_token_id is None:\n",
    "        print(\"[Warning] decoder_start_token_id is None. Using default generation behavior.\")\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "        print('here')\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch_preprocessed = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        if not isinstance(batch_preprocessed, list) or len(batch_preprocessed) == 0:\n",
    "            print(f\"[Warning] Preprocessed batch is empty at index {i}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "#         Debug: print first 2 sentences after preprocessing\n",
    "        print(f\"[Debug] Preprocessed batch sample: {batch_preprocessed[:2]}\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_preprocessed,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations with safety parameters\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_tokens = model.generate(\n",
    "                    **inputs,\n",
    "                    use_cache=True,\n",
    "                    min_length=5,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=5,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=decoder_start_token_id,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Generation failed for batch starting at index {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "#         Debug: print first 2 decoded outputs\n",
    "        print(f\"[Debug] Decoded sample: {decoded_texts[:2]}\")\n",
    "\n",
    "        # Postprocess translations\n",
    "        try:\n",
    "            postprocessed = ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "            translations += postprocessed\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Postprocessing failed for batch starting at index {i}: {e}\")\n",
    "            translations += decoded_texts  # fallback\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "prefix = f\"{tgt_lang} {src_lang}\"\n",
    "\n",
    "def remove_prefix(text):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- TRANSLATION --------------------\n",
    "valid_src = []\n",
    "valid_pred = []\n",
    "valid_gt = []\n",
    "valid_gem = []\n",
    "valid_cfilt = []\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "#     print(batch)\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        en_indic_model,\n",
    "        en_indic_tokenizer,\n",
    "        ip,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "    if translations is None:\n",
    "        print(f\"[SKIPPED] Batch {i}: Returned None\")\n",
    "        continue\n",
    "\n",
    "    cleaned = [remove_prefix(t) for t in translations]\n",
    "\n",
    "    valid_src.extend(batch)\n",
    "    valid_pred.extend(cleaned)\n",
    "    valid_gt.extend(ref_gt[i:i + len(batch)])\n",
    "    valid_gem.extend(ref_gem[i:i + len(batch)])\n",
    "    valid_cfilt.extend(ref_cfilt[i:i + len(batch)])\n",
    "\n",
    "print(f\"\\nSuccessful translations: {len(valid_pred)} / {len(src_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3be120",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"approach1_t5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e06b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": valid_src,\n",
    "    \"prediction\": valid_pred,\n",
    "    \"gt\": valid_gt,\n",
    "    \"gemini\": valid_gem,\n",
    "    \"cfilt\": valid_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"âœ” Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(valid_pred, valid_gt, valid_gem, valid_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=valid_pred, references=[[r] for r in valid_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined â†’ BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi â†’ BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    â†’ BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     â†’ BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined â†’ BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cc2b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd21202",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences = raw_datasets['test'][\"sent_meant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb80ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- TRANSLATION --------------------\n",
    "valid_src = []\n",
    "valid_pred = []\n",
    "valid_gt = []\n",
    "valid_gem = []\n",
    "valid_cfilt = []\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "#     print(batch)\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        en_indic_model,\n",
    "        en_indic_tokenizer,\n",
    "        ip,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "    if translations is None:\n",
    "        print(f\"[SKIPPED] Batch {i}: Returned None\")\n",
    "        continue\n",
    "\n",
    "    cleaned = [remove_prefix(t) for t in translations]\n",
    "\n",
    "    valid_src.extend(batch)\n",
    "    valid_pred.extend(cleaned)\n",
    "    valid_gt.extend(ref_gt[i:i + len(batch)])\n",
    "    valid_gem.extend(ref_gem[i:i + len(batch)])\n",
    "    valid_cfilt.extend(ref_cfilt[i:i + len(batch)])\n",
    "\n",
    "print(f\"\\nSuccessful translations: {len(valid_pred)} / {len(src_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"sent_meant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": valid_src,\n",
    "    \"prediction\": valid_pred,\n",
    "    \"gt\": valid_gt,\n",
    "    \"gemini\": valid_gem,\n",
    "    \"cfilt\": valid_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"âœ” Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(valid_pred, valid_gt, valid_gem, valid_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=valid_pred, references=[[r] for r in valid_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined â†’ BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi â†’ BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    â†’ BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     â†’ BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nðŸŽ¯ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined â†’ BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentences[7:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sentences[7:13]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
