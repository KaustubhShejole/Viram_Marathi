{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55630700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as eval_load\n",
    "from huggingface_hub import login as hf_login\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5902bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a9efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_indictrans_text(preds, labels, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    prefix = f\"{target_lang} {source_lang}\"\n",
    "\n",
    "    def remove_prefix(text: str) -> str:\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    preds = [remove_prefix(p) for p in preds]\n",
    "    # return labels as list-of-lists for BLEU (sacrebleu expects that)\n",
    "    labels = [[remove_prefix(l)] for l in labels]\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7e8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_extract_comet_score(comet_out: dict) -> Optional[float]:\n",
    "    # Try common keys used by various COMET wrappers\n",
    "    for k in (\"system_score\", \"mean_score\", \"score\", \"value\"):\n",
    "        if k in comet_out:\n",
    "            return float(comet_out[k])\n",
    "    # Some wrappers return {\"scores\": [...]} or similar\n",
    "    if \"scores\" in comet_out:\n",
    "        return float(np.mean(comet_out[\"scores\"]))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea26942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_indictrans(\n",
    "    eval_preds,\n",
    "    tokenizer,\n",
    "    metric_bleu,\n",
    "    metric_chrf=None,\n",
    "    metric_comet=None,\n",
    "    metric_bleurt=None,\n",
    "    source_lang=\"eng_Latn\",\n",
    "    target_lang=\"mar_Deva\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust compute_metrics that accepts either:\n",
    "      - eval_preds as (preds, labels) tuple (older usage), or\n",
    "      - eval_preds as transformers.EvalPrediction object with attributes\n",
    "        .predictions, .label_ids, and (optionally) .inputs when\n",
    "        include_inputs_for_metrics=True is set in training args.\n",
    "    \"\"\"\n",
    "    # Unpack in a robust way\n",
    "    if hasattr(eval_preds, \"predictions\"):\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "        inputs = getattr(eval_preds, \"inputs\", None)\n",
    "    else:\n",
    "        # fallback if user passed a tuple (preds, labels)\n",
    "        preds, labels = eval_preds\n",
    "        inputs = None\n",
    "\n",
    "    # If generation returned tuple (generated_ids, something) take first\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Convert tensors -> numpy arrays if necessary\n",
    "    def to_numpy(x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return x.cpu().numpy()\n",
    "        return x\n",
    "\n",
    "    preds = to_numpy(preds)\n",
    "    labels = to_numpy(labels)\n",
    "    inputs = to_numpy(inputs) if inputs is not None else None\n",
    "\n",
    "    # Decode predictions (list[str])\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Decode labels: replace -100 -> pad_token_id then decode\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # If inputs are present (requires include_inputs_for_metrics=True), decode sources\n",
    "    decoded_sources = None\n",
    "    if inputs is not None:\n",
    "        decoded_sources = tokenizer.batch_decode(inputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Postprocess (remove prefixes). Note: postprocess returns references as list-of-lists.\n",
    "    decoded_preds, decoded_labels = postprocess_indictrans_text(decoded_preds, decoded_labels, source_lang, target_lang)\n",
    "\n",
    "    # ----- BLEU (sacrebleu expects references as list-of-lists) -----\n",
    "    try:\n",
    "        bleu_out = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        bleu_score = float(bleu_out.get(\"score\", 0.0))\n",
    "    except Exception as e:\n",
    "        logger.warning(\"BLEU computation failed: %s\", e)\n",
    "        bleu_score = 0.0\n",
    "\n",
    "    # ----- ChrF++ -----\n",
    "    chrfpp_score = None\n",
    "    if metric_chrf is not None:\n",
    "        try:\n",
    "            chrf_out = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)\n",
    "            chrfpp_score = float(chrf_out.get(\"score\", np.nan))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"ChrF computation failed: %s\", e)\n",
    "\n",
    "    # ----- COMET -----\n",
    "    comet_score = None\n",
    "    if metric_comet is not None:\n",
    "        if decoded_sources is None:\n",
    "            logger.warning(\"COMET requested but decoded_sources is None â€” set include_inputs_for_metrics=True in TrainingArguments\")\n",
    "        else:\n",
    "            try:\n",
    "                # COMET wants flat references (not list-of-lists)\n",
    "                flat_refs = [r[0] if isinstance(r, (list, tuple)) and len(r) > 0 else r for r in decoded_labels]\n",
    "                comet_out = metric_comet.compute(sources=decoded_sources, predictions=decoded_preds, references=flat_refs)\n",
    "                comet_score = _safe_extract_comet_score(comet_out)\n",
    "            except Exception as e:\n",
    "                logger.warning(\"COMET computation failed: %s\", e)\n",
    "\n",
    "    # ----- BLEURT -----\n",
    "    bleurt_score = None\n",
    "    if metric_bleurt is not None:\n",
    "        try:\n",
    "            flat_refs = [r[0] if isinstance(r, (list, tuple)) and len(r) > 0 else r for r in decoded_labels]\n",
    "            bleurt_out = metric_bleurt.compute(predictions=decoded_preds, references=flat_refs)\n",
    "            # BLEURT returns \"scores\" usually\n",
    "            if \"scores\" in bleurt_out:\n",
    "                bleurt_score = float(np.mean(bleurt_out[\"scores\"]))\n",
    "            elif \"score\" in bleurt_out:\n",
    "                bleurt_score = float(bleurt_out[\"score\"])\n",
    "        except Exception as e:\n",
    "            logger.warning(\"BLEURT computation failed: %s\", e)\n",
    "\n",
    "    # ----- Generated length -----\n",
    "    try:\n",
    "        prediction_lens = [int(np.count_nonzero(p != tokenizer.pad_token_id)) for p in preds]\n",
    "        gen_len = float(np.mean(prediction_lens)) if len(prediction_lens) > 0 else 0.0\n",
    "    except Exception:\n",
    "        gen_len = 0.0\n",
    "\n",
    "    result = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrfpp\": round(chrfpp_score, 4) if chrfpp_score is not None else None,\n",
    "        \"comet\": round(comet_score, 4) if comet_score is not None else None,\n",
    "        \"bleurt\": round(bleurt_score, 4) if bleurt_score is not None else None,\n",
    "        \"gen_len\": round(gen_len, 4),\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95631b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_token(path=\"hf_token.txt\"):\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        raise RuntimeError(f\"HF token file not found at {path}. Create it with your token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f871941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 24 07:46:46 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   67C    P0             190W / 300W |  35873MiB / 81920MiB |     68%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              69W / 300W |   1237MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              74W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              60W / 300W |    889MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff85f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = \"1\"\n",
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "dataset = \"thenlpresearcher/shalaka_iitb_en_indic_tokenized\"\n",
    "batch_size = 16\n",
    "eval_steps = 6000\n",
    "save_steps = 6000\n",
    "num_train_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "save_total_limit = 3\n",
    "metric_for_best_model = \"chrfpp\"\n",
    "greater_is_better = True\n",
    "source_lang = \"eng_Latn\"\n",
    "target_lang = \"mar_Deva\"\n",
    "push_to_hub = True\n",
    "hub_model_id=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36829ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d55679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 07:47:07,254 - INFO - Loading model/tokenizer from ai4bharat/indictrans2-en-indic-dist-200M\n",
      "2025-11-24 07:47:09,935 - INFO - Loading dataset thenlpresearcher/shalaka_iitb_en_indic_tokenized\n",
      "2025-11-24 07:47:12,711 - INFO - Loading metrics\n",
      "Fetching 5 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 11478.66it/s]\n",
      "2025-11-24 07:47:19,149 - INFO - Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "2025-11-24 07:47:29,148 - INFO - Encoder model frozen.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "2025-11-24 07:47:31,078 - WARNING - Could not load BLEURT metric: No module named 'tensorflow'\n"
     ]
    }
   ],
   "source": [
    "hf_token = load_hf_token()\n",
    "hf_login(hf_token)\n",
    "\n",
    "logger.info(\"Loading model/tokenizer from %s\", model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "\n",
    "logger.info(\"Loading dataset %s\", dataset)\n",
    "tokenized_datasets = load_dataset(dataset)\n",
    "\n",
    "logger.info(\"Loading metrics\")\n",
    "metric_bleu = eval_load(\"sacrebleu\")\n",
    "metric_chrf = eval_load(\"chrf\")\n",
    "metric_comet = None\n",
    "metric_bleurt = None\n",
    "try:\n",
    "    metric_comet = eval_load(\"comet\")\n",
    "except Exception as e:\n",
    "    logger.warning(\"Could not load COMET metric: %s\", e)\n",
    "try:\n",
    "    metric_bleurt = eval_load(\"bleurt\")\n",
    "except Exception as e:\n",
    "    logger.warning(\"Could not load BLEURT metric: %s\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bd09ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.rstrip(\"/\").split(\"/\")[-1]\n",
    "hub_model_id = f\"thenlpresearcher/shalaka_fd_{model_name}_finetuned_{source_lang}_to_{target_lang}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc955646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indictrans2-en-indic-dist-200M\n",
      "thenlpresearcher/shalaka_fd_indictrans2-en-indic-dist-200M_finetuned_eng_Latn_to_mar_Deva\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print(hub_model_id)\n",
    "print(push_to_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "751a5c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Please use `include_for_metrics` list argument instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"shalaka_fd_{model_name}-en-indic-iitb-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_steps=save_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=save_total_limit,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    push_to_hub=push_to_hub,\n",
    "    hub_model_id=hub_model_id,\n",
    "    hub_private_repo=False,\n",
    "    hub_strategy=\"end\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    greater_is_better=greater_is_better,\n",
    "    include_inputs_for_metrics=True,   # IMPORTANT for COMET\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=\"longest\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5bbbcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88538/1878573675.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-11-24 07:47:31,281 - WARNING - Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                           # use local model variable\n",
    "    args=training_args,                    # pass Seq2SeqTrainingArguments instance\n",
    "    train_dataset=tokenized_datasets.get(\"train\"),\n",
    "    eval_dataset=tokenized_datasets.get(\"validation\"),\n",
    "    tokenizer=tokenizer,                   # important so EvalPrediction.inputs is set\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_preds: compute_metrics_indictrans(\n",
    "        eval_preds,\n",
    "        tokenizer=tokenizer,\n",
    "        metric_bleu=metric_bleu,\n",
    "        metric_chrf=metric_chrf,\n",
    "        metric_comet=metric_comet,\n",
    "        metric_bleurt=metric_bleurt,\n",
    "        source_lang=source_lang,\n",
    "        target_lang=target_lang,\n",
    "    ),\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 07:47:33,181 - INFO - Starting fine-tuning ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6001' max='59295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6001/59295 15:42 < 2:19:32, 6.37 it/s, Epoch 0.51/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1483' max='1483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1483/1483 07:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 08:12:11,073 - INFO - ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "2025-11-24 08:12:11,089 - INFO - GPU available: True (cuda), used: True\n",
      "2025-11-24 08:12:11,090 - INFO - TPU available: False, using: 0 TPU cores\n",
      "2025-11-24 08:12:11,094 - INFO - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Starting fine-tuning ...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb84207",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Pushing final checkpoint to hub: %s\", hub_model_id)\n",
    "trainer.push_to_hub()\n",
    "\n",
    "logger.info(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2ac71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09027d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
