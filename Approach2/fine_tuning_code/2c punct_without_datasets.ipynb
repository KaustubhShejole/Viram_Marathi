{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991e2140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 237175\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Make sure you have logged in using:\n",
    "# huggingface-cli login\n",
    "# to access this dataset if it requires authentication.\n",
    "\n",
    "# Load the BPCC dataset with the specific config\n",
    "ds = load_dataset(\"thenlpresearcher/iitb_eng_mar_dataset\")\n",
    "\n",
    "# Inspect available splits\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d173af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src_lang', 'tgt_lang', 'src', 'tgt']\n",
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example, let’s rename Sheet 2 as “Dump”.', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n"
     ]
    }
   ],
   "source": [
    "# Check the first example from the training split\n",
    "mar_ds = ds['train']\n",
    "\n",
    "# Optional: quick check\n",
    "print(mar_ds.column_names)\n",
    "print(mar_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4846029f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example, let’s rename Sheet 2 as “Dump”.', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n",
      "['src_lang', 'tgt_lang', 'src', 'tgt']\n"
     ]
    }
   ],
   "source": [
    "print(mar_ds[0])\n",
    "\n",
    "# Optional: see column names\n",
    "print(mar_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39b4961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 237175/237175 [00:08<00:00, 29177.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example let’s rename Sheet 2 as “Dump”', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n",
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'The Options dialogbox appears', 'tgt': 'Options चा डायलॉग बॉक्स उघडेल.'}\n",
      "Total rows: 237175\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for ex in tqdm(mar_ds):\n",
    "    # original\n",
    "#     all_rows.append({\n",
    "#         \"src_lang\": ex[\"src_lang\"],\n",
    "#         \"tgt_lang\": ex[\"tgt_lang\"],\n",
    "#         \"src\": ex[\"src\"],\n",
    "#         \"tgt\": ex[\"tgt\"]\n",
    "#     })\n",
    "    # punctuation removed\n",
    "    all_rows.append({\n",
    "        \"src_lang\": ex[\"src_lang\"],\n",
    "        \"tgt_lang\": ex[\"tgt_lang\"],\n",
    "        \"src\": ex[\"src\"].translate(punct_table),\n",
    "        \"tgt\": ex[\"tgt\"]\n",
    "    })\n",
    "\n",
    "# Create a new Dataset\n",
    "mar_ds_expanded = Dataset.from_list(all_rows)\n",
    "\n",
    "# Inspect\n",
    "print(mar_ds_expanded[0])\n",
    "print(mar_ds_expanded[1])\n",
    "print(\"Total rows:\", len(mar_ds_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90600335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5611efe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/238 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|▎| 79/238 [00:00<00:00, 786.13ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  66%|▋| 158/238 [00:00<00:00, 706.45ba/s\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 238/238 [00:00<00:00, 665.17ba/s\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:06<00:00,  6.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/iitb_marathi_without_punct/commit/578ca0285552f06699692843ca0efa4d8f8286ac', commit_message='Upload dataset', commit_description='', oid='578ca0285552f06699692843ca0efa4d8f8286ac', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/iitb_marathi_without_punct', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/iitb_marathi_without_punct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, Repository\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# If you have multiple splits, wrap in DatasetDict\n",
    "ds_dict = DatasetDict({\n",
    "    \"marathi_punct\": mar_ds_expanded\n",
    "})\n",
    "\n",
    "# Push to HF\n",
    "ds_dict.push_to_hub(\"thenlpresearcher/iitb_marathi_without_punct\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727df23",
   "metadata": {},
   "source": [
    "### Made validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d60641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b57304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 408/408 [00:00<00:00, 1.93kB/s]\n",
      "Downloading data: 100%|████████████████| 38.0M/38.0M [00:01<00:00, 19.0MB/s]\n",
      "Generating marathi_punct split: 100%|█| 237175/237175 [00:00<00:00, 681080.9\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "mar_ds = datasets.load_dataset(\"thenlpresearcher/iitb_marathi_without_punct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd6e16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    marathi_punct: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 237175\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04b7b40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 189740\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 23717\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 23718\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming your DatasetDict is called `dataset_dict`\n",
    "dataset = mar_ds['marathi_punct']\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Define split ratios\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "# Compute sizes\n",
    "total = len(dataset)\n",
    "train_size = int(train_frac * total)\n",
    "val_size = int(val_frac * total)\n",
    "test_size = total - train_size - val_size  # ensure all samples are used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.select(range(0, train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, total))\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4b409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/190 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   2%|   | 3/190 [00:00<00:06, 27.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   4%|   | 7/190 [00:00<00:05, 31.79ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   6%|  | 11/190 [00:00<00:05, 34.74ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   8%|▏ | 15/190 [00:00<00:04, 36.26ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  10%|▏ | 19/190 [00:00<00:04, 37.16ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  12%|▏ | 23/190 [00:00<00:04, 37.71ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  14%|▎ | 27/190 [00:00<00:04, 38.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  16%|▎ | 31/190 [00:00<00:04, 38.48ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|▎ | 35/190 [00:00<00:04, 38.69ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  21%|▍ | 39/190 [00:01<00:03, 38.81ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  23%|▍ | 43/190 [00:01<00:03, 38.91ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  25%|▍ | 47/190 [00:01<00:03, 39.00ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  27%|▌ | 51/190 [00:01<00:03, 38.36ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  29%|▌ | 55/190 [00:01<00:03, 38.48ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  31%|▌ | 59/190 [00:01<00:03, 38.15ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|▋ | 63/190 [00:01<00:03, 38.35ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  35%|▋ | 67/190 [00:01<00:03, 38.65ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  37%|▋ | 71/190 [00:01<00:03, 38.94ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  39%|▊ | 75/190 [00:01<00:02, 39.11ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  42%|▊ | 79/190 [00:02<00:02, 39.19ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  44%|▊ | 83/190 [00:02<00:02, 39.13ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  46%|▉ | 87/190 [00:02<00:02, 38.99ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  48%|▉ | 91/190 [00:02<00:02, 38.34ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  50%|█ | 95/190 [00:02<00:02, 38.66ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  52%|█ | 99/190 [00:02<00:02, 38.66ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  54%|▌| 103/190 [00:02<00:02, 37.68ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  56%|▌| 107/190 [00:02<00:02, 37.81ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  59%|▌| 112/190 [00:02<00:02, 38.59ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  61%|▌| 116/190 [00:03<00:01, 38.83ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  63%|▋| 120/190 [00:03<00:01, 39.04ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  65%|▋| 124/190 [00:03<00:01, 39.20ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  67%|▋| 128/190 [00:03<00:01, 39.32ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  69%|▋| 132/190 [00:03<00:01, 39.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  72%|▋| 136/190 [00:03<00:01, 39.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  74%|▋| 140/190 [00:03<00:01, 39.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  76%|▊| 144/190 [00:03<00:01, 39.47ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  78%|▊| 148/190 [00:03<00:01, 39.48ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  80%|▊| 152/190 [00:03<00:00, 39.48ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  82%|▊| 156/190 [00:04<00:00, 39.45ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  84%|▊| 160/190 [00:04<00:00, 39.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  86%|▊| 164/190 [00:04<00:00, 37.70ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  88%|▉| 168/190 [00:04<00:00, 38.16ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  91%|▉| 172/190 [00:04<00:00, 38.52ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  93%|▉| 176/190 [00:04<00:00, 38.76ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  95%|▉| 180/190 [00:04<00:00, 38.96ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  97%|▉| 184/190 [00:04<00:00, 39.15ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 190/190 [00:04<00:00, 38.40ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:09<00:00,  9.09s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  17%|▋   | 4/24 [00:00<00:00, 38.36ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|█▎  | 8/24 [00:00<00:00, 38.96ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  50%|█▌ | 12/24 [00:00<00:00, 39.14ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  67%|██ | 16/24 [00:00<00:00, 31.39ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|███| 24/24 [00:00<00:00, 35.76ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:03<00:00,  3.57s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  17%|▋   | 4/24 [00:00<00:00, 39.45ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  38%|█▌  | 9/24 [00:00<00:00, 40.04ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  58%|█▊ | 14/24 [00:00<00:00, 40.15ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  79%|██▍| 19/24 [00:00<00:00, 40.26ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|███| 24/24 [00:00<00:00, 40.50ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:01<00:00,  1.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/iitb_marathi_without_punct/commit/05a1cdc591b6e4789f9cc196856541d8c95b4587', commit_message='Upload dataset', commit_description='', oid='05a1cdc591b6e4789f9cc196856541d8c95b4587', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/iitb_marathi_without_punct', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/iitb_marathi_without_punct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset.push_to_hub(\n",
    "    \"thenlpresearcher/iitb_marathi_without_punct\", \n",
    "    private=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415992b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
