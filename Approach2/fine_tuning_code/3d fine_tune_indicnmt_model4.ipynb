{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV-pf35571ap"
   },
   "source": [
    "### If you dont want to use Wandb, disable Wandb otherwise optional\n",
    "\n",
    "references for WANDB\n",
    "https://analyticsindiamag.com/hands-on-guide-to-weights-and-biases-wandb-with-python-implementation/\n",
    "\n",
    "https://docs.wandb.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 16:33:25 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   52C    P0              68W / 300W |   2192MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              73W / 300W |  11588MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   57C    P0              75W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              61W / 300W |  11240MiB / 81920MiB |      3%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/bert_punct_model_final/data/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tmYJBBbwyiX3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1hOFHi-x5UZ"
   },
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqADZ2Sgx5Ua",
    "outputId": "60c95827-a686-45eb-c201-e23bc8b03b2b"
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN5RtJdM8xi5"
   },
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xKFwtNEx5Uc",
    "outputId": "1206f370-b7a8-4a34-bc6d-36f1a731be6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKkJGYY19I11"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MX7DsEmOx5Ud"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu45aGHIAQqM"
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "We will use the [datasets](https://github.com/huggingface/datasets/tree/master/datasets/wmt16) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric. We use the English/Romanian part of the WMT dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "34fd15427c7d4f4189c9ce38924107b5",
      "04672fbdc8754babbb7bb3bfb7bcfd2e",
      "53ec358cc4784c25a50e17bd8764fd56",
      "c916612f156b457f828410fdc85c8def",
      "de1e4b0f94f74b4380de2cb4447352f1",
      "c04bdf05ef684c8fb12ea45833394e77",
      "ed5983a37cba4faea852cf2e13b355b4",
      "f8f12dcd6d164513bae3d4d7835cc45e",
      "adbad0b85f78492583b2c5b291341030",
      "a9aa1538562840f2b969340f1cbf8bbb",
      "5600aa1b802c46b183417910f0e18cc4",
      "fd705f2d192c41f8bbbf5c6270e6ffb0",
      "d8499a1a46144034b56489025aaee86f",
      "b982f93475414c0c8a92601717138dc4",
      "98b513fb020744bfbbe0678daa33f2aa",
      "4295f3dc0a494198b776f682b327924d",
      "17ecb55f59dd411084d8692b80cc8cad",
      "a434f279213545da8a92da73bc320504",
      "e1d75f47a76d4bf8b2941fe543e46242",
      "0c2955e8cff44b5ba998ca584e485e54",
      "eb10554a79a04ecf92517ba96ca956d9",
      "ad65c758c1114adab13e6dbf29f660aa",
      "e3ada02e2bf34b5699475b1eb7801282",
      "9c9af690e08d490dbba5524e35fa6fcb",
      "c5d0a0ee946745609e561ffece8cde76",
      "9d13fcfc7b4e4ef182ef48df08253af4",
      "88c725d411d34463b632ec85fa67ed4a",
      "c4298a4942db4790a357a342c7cc13e7",
      "4d9080a460f34c8abdb3af1fdf0eddce",
      "a8744f13ff43450db14031275a7ba769",
      "edf107491f434e7486765ebda2ad938e",
      "922bfc020eb345c584ab31721a10922c",
      "f9c351511a33475d993fee0d6c1cddba",
      "4b5219e4a1a1400a982acc3065b46e2c"
     ]
    },
    "id": "biPo8vFTx5Ue",
    "outputId": "402f80b1-801c-4452-f502-dfcb9d9e9326"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 189740/189740 [00:00<00:00, 597279.57 exampl\n",
      "Generating validation split: 100%|█| 23717/23717 [00:00<00:00, 644819.52 exa\n",
      "Generating test split: 100%|█| 23718/23718 [00:00<00:00, 1001333.72 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "raw_datasets = load_dataset(\"thenlpresearcher/shalaka_iitb_marathi_punct\")\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"sacrebleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 Results:\n",
      "Keys: ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "BLEU Score: 100.0\n",
      "\n",
      "Example 2 Results:\n",
      "Keys: ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "BLEU Score: 39.8\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Example 1: perfect match\n",
    "predictions_1 = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references_1 = [\n",
    "    [\"hello there general kenobi\", \"hello there !\"],\n",
    "    [\"foo bar foobar\", \"foo bar foobar\"]\n",
    "]\n",
    "\n",
    "metric = load(\"sacrebleu\")\n",
    "results_1 = metric.compute(predictions=predictions_1, references=references_1)\n",
    "\n",
    "print(\"Example 1 Results:\")\n",
    "print(\"Keys:\", list(results_1.keys()))\n",
    "print(\"BLEU Score:\", round(results_1[\"score\"], 1))\n",
    "print()\n",
    "\n",
    "# Example 2: partial match\n",
    "predictions_2 = [\n",
    "    \"hello there general kenobi\",\n",
    "    \"on our way to ankh morpork\"\n",
    "]\n",
    "references_2 = [\n",
    "    [\"hello there general kenobi\", \"hello there !\"],\n",
    "    [\"goodbye ankh morpork\", \"ankh morpork\"]\n",
    "]\n",
    "\n",
    "results_2 = metric.compute(predictions=predictions_2, references=references_2)\n",
    "\n",
    "print(\"Example 2 Results:\")\n",
    "print(\"Keys:\", list(results_2.keys()))\n",
    "print(\"BLEU Score:\", round(results_2[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"pyarrow<17\" \"datasets>=2.14,<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow: 16.1.0\n",
      "datasets: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa, datasets\n",
    "print(\"pyarrow:\", pa.__version__)\n",
    "print(\"datasets:\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LqXNw9_BFQe"
   },
   "source": [
    "The dataset object itself is [datasetdict](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "379sQa4Ix5Uf",
    "outputId": "8d3baded-0cec-4bbf-fdc4-cdcc7944ee9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 189740\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 23717\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 23718\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkiu4ITXx5Ug",
    "outputId": "1eb6d334-c138-4ddb-c625-9a7f817fdb2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_lang': 'eng_Latn',\n",
       " 'tgt_lang': 'mar_Deva',\n",
       " 'src': 'If higher modalities of treatment like surgery is required, the patient should be referred to the tertiary care level.',\n",
       " 'tgt': 'शस्त्रक्रियेसारख्या उपचारांच्या उच्च पद्धती आवश्यक असल्यास, रुग्णाला तृतीयक काळजी स्तरावर पाठविले पाहिजे.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6hsuFL1Bwa6"
   },
   "source": [
    "To get a sense of how the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "1OE_CXoxx5Uh",
    "outputId": "c25cfc10-df85-49b7-bd06-524ce871ba9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>It is also a big tourist attraction during the monsoon months.</td>\n",
       "      <td>पावसाळ्यातही हे पर्यटकांसाठी मोठे आकर्षण असते.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>Remember that we have permitted the authenticated users to delete their own content only.</td>\n",
       "      <td>लक्षात ठेवा की आपण authenticated users फक्त त्यांची स्वतःची कन्टेन्ट डिलिट करण्यासाठी परवानगी दिली आहे.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>He was one of the key batsmen in the Indian team that won the 2008 U19 Cricket World Cup in Malaysia</td>\n",
       "      <td>२००८ साली मलेशिया येथे झालेल्या १९ वर्षाखालील विश्वचषक क्रिकेट स्पर्धेचे विजेतेपद पटकावणाऱ्या भारतीय संघातील प्रमुख फलंदाजांपैकी तो एक होता.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>It means that we have to have well organized tickets and we have to commit changesets in some meaningful fashion.</td>\n",
       "      <td>याचा अर्थ असा की आपल्याकडे तिकिटे व्यवस्थित असली पाहिजेत आणि आपल्याला काही अर्थपूर्ण पद्धतीने चेंजसेट्स कमिट करावे लागतील.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>The Dagars are Muslims but sing Hindu texts of Gods and Goddesses.</td>\n",
       "      <td>डागर हे मुसलमान असतात पण देव-देवतांची हिंदू स्तवने गातात.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6OLK8GQB8lK"
   },
   "source": [
    "You can call its compute method with your predictions and labels, which need to be list of decoded strings (list of list for the labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G24hShAwx5Uj",
    "outputId": "163c8294-85e6-4fc5-ff66-3f6cc62affe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 100.00000000000004\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load metric\n",
    "metric = load(\"sacrebleu\")\n",
    "\n",
    "preds = [\"hello there general kenobi\", \"you are a bold one\"]\n",
    "refs  = [[\"hello there general kenobi\"], [\"you are a bold one\"]]\n",
    "result = metric.compute(predictions=preds, references=refs)\n",
    "print(\"BLEU =\", result[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6HmkA7uCWRa"
   },
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "we download the vocabulary used when pretraining this specific checkpoint.\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell.\n",
    "\n",
    "If you downloaded the model manually, you can provide model present directory instead of model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "db4d60783a2b4cc9ac647f53524e9201",
      "23e6dd2b833d44aabc5b78a83d7ef136",
      "8aff67fa78c640bf83598dd7fccc5b97",
      "97ed312717014144af97d250fcd74d9d",
      "7b69f1226cf5457aa164146a890a2c5e",
      "d5ce787580584a40883fdac45df7917a",
      "1962a04347894250993d2eb3d2946e5c",
      "d5e1f379a1084c6e978a633698b4b02a",
      "b689909adc244204af714b9a19e158c0",
      "8c32a5f5cb724610a4ea8a5b42be5b28",
      "44db6a1a111148ceaa804f7302b0629b",
      "7e8253e0de7f426c9f9af4b15775c660",
      "adfbc16f0b6144b6ab06544ed060f8fe",
      "1425686f748e45b9921c501d0e5ce904",
      "632fb54ffc9b44b98d082d097c17d7de"
     ]
    },
    "id": "agDlgrOix5Uj",
    "outputId": "d52e9acb-10d0-464f-c9f1-1947b7a7b0e6"
   },
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "\n",
    "    # Quantization setup\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    # Move to device and optionally convert to half precision\n",
    "    if qconfig is None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    # Make sure model is in training mode for fine-tuning\n",
    "    model.train()\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbih6yhKG9JB"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9XTBM27x5Uk",
    "outputId": "2bd42d79-8437-41b1-e1b4-80723a18f614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentences: ['eng_Latn mar_Deva Hello , this is one sentence !', 'eng_Latn mar_Deva This is another sentence .']\n"
     ]
    }
   ],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "sentences = [\"Hello, this is one sentence!\", \"This is another sentence.\"]\n",
    "\n",
    "# Preprocess sentences\n",
    "preprocessed = ip.preprocess_batch(sentences, src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\")\n",
    "print(\"Preprocessed sentences:\", preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   4,   29, 7951,    7,   36,   13,   75, 4534,   74,    2],\n",
      "        [   1,    1,    4,   29,   67,   13,  309, 4534,    5,    2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the batch and generate input encodings\n",
    "inputs = en_indic_tokenizer(\n",
    "    preprocessed,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf2HfvKdHCeS"
   },
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the as_target_tokenizer context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvYg6BXJx5Ul",
    "outputId": "06d8f836-7a16-4fdc-a23e-9d0973135a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentences: ['eng_Latn mar_Deva Hello , this is one sentence !', 'eng_Latn mar_Deva This is another sentence .']\n",
      "{'input_ids': tensor([[67156, 59836,  3233, 11007,  7130, 60726, 59836,  2134,  5172, 43144,\n",
      "         36485, 64643,     5, 45158, 14444, 46728, 78577,    40,     2],\n",
      "        [67156, 59836,  3233, 11007,  7130, 60726, 59836,  2134,  5172, 43144,\n",
      "         56649, 14444, 79991, 78577,     4,     2,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Hello, this is one sentence!\", \"This is another sentence.\"]\n",
    "\n",
    "# Preprocess sentences\n",
    "preprocessed = ip.preprocess_batch(sentences, src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\")\n",
    "print(\"Preprocessed sentences:\", preprocessed)\n",
    "\n",
    "with en_indic_tokenizer.as_target_tokenizer():\n",
    "    print(en_indic_tokenizer(preprocessed,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "def decode_tokenized_batch(tokenizer, batch: Dict[str, torch.Tensor], ip=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Decode a batch of token IDs (input_ids or labels) into readable text.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        batch: dictionary containing 'input_ids' or 'labels' tensors\n",
    "        ip: IndicProcessor instance (optional, for postprocessing)\n",
    "        target_lang: target language code, required if ip is used\n",
    "    \n",
    "    Returns:\n",
    "        List of decoded strings\n",
    "    \"\"\"\n",
    "    # Choose which field to decode\n",
    "    if \"labels\" in batch:\n",
    "        tokens_to_decode = batch[\"labels\"]\n",
    "    elif \"input_ids\" in batch:\n",
    "        tokens_to_decode = batch[\"input_ids\"]\n",
    "    else:\n",
    "        raise ValueError(\"Batch must contain 'labels' or 'input_ids'.\")\n",
    "\n",
    "    # Convert to list of lists (batch of sequences)\n",
    "    if isinstance(tokens_to_decode, torch.Tensor):\n",
    "        tokens_to_decode = tokens_to_decode.tolist()\n",
    "\n",
    "    # Decode using tokenizer\n",
    "    decoded_texts = tokenizer.batch_decode(\n",
    "        tokens_to_decode,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    print(decoded_texts)\n",
    "    \n",
    "    # Postprocess with IndicProcessor if provided\n",
    "    if ip is not None and target_lang is not None:\n",
    "        decoded_texts = ip.postprocess_batch(decoded_texts, lang=target_lang)\n",
    "\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_lang': ['eng_Latn', 'eng_Latn'],\n",
       " 'tgt_lang': ['mar_Deva', 'mar_Deva'],\n",
       " 'src': ['If higher modalities of treatment like surgery is required, the patient should be referred to the tertiary care level.',\n",
       "  'She then moved to New York City to perform on the stage.'],\n",
       " 'tgt': ['शस्त्रक्रियेसारख्या उपचारांच्या उच्च पद्धती आवश्यक असल्यास, रुग्णाला तृतीयक काळजी स्तरावर पाठविले पाहिजे.',\n",
       "  'त्यानंतर ती मंचावर सादरीकरण करण्यासाठी न्यूयॉर्क शहरात रहायला गेली.']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define preprocessing ----\n",
    "def preprocess_function(examples, tokenizer, ip):\n",
    "    # Step 1: Preprocess both source and target\n",
    "    src_texts = ip.preprocess_batch(\n",
    "        examples[\"src\"], src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\"\n",
    "    )\n",
    "    tgt_texts = ip.preprocess_batch(\n",
    "        examples[\"tgt\"], src_lang=\"mar_Deva\", tgt_lang=\"eng_Latn\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Tokenize (new Transformers ≥v4.30 style)\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        text_target=tgt_texts,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=None,        # return lists for Dataset.map\n",
    "    )\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mar_Deva eng_Latn शस्त्रक्रियेसारख्या उपचारांच्या उच्च पद्धती आवश्यक असल्यास, रुग्णाला तृतीयक काळजी स्तरावर पाठविले पाहिजे.', 'mar_Deva eng_Latn त्यानंतर ती मंचावर सादरीकरण करण्यासाठी न्यूयॉर्क शहरात रहायला गेली.']\n",
      "['mar_Deva eng_Latn शस्त्रक्रियेसारख्या उपचारांच्या उच्च पद्धती आवश्यक असल्यास, रुग्णाला तृतीयक काळजी स्तरावर पाठविले पाहिजे.', 'mar_Deva eng_Latn त्यानंतर ती मंचावर सादरीकरण करण्यासाठी न्यूयॉर्क शहरात रहायला गेली.']\n"
     ]
    }
   ],
   "source": [
    "example_batch = preprocess_function(raw_datasets['train'][:2], en_indic_tokenizer, ip)\n",
    "print(decode_tokenized_batch(en_indic_tokenizer, example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ZDByENHTtO"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "f56b5b8abcfc415fbf46ca0f2fe619d0",
      "ef09f1189d75431591bebf33d288dacc",
      "12e50a27f9f24575bd86305f3b609095",
      "24cc9a4f2e4a411296dd6822329e5815",
      "f83d5ec459fe4dfd9ebf46fcca9895ee",
      "382f2bc5fc944482861293a496fb03ed",
      "330c9a9e97c9436f9d29bb12c586253a",
      "af955c59afc54adc8f1072d371b688bc",
      "5b4edc877e524610bb9ecc8103763250",
      "d3190da3507f4ab4be03027179566508",
      "7dac8e5e55ce4e638942c4299b377e3a",
      "41467eb21e744e66a47790e274defb66",
      "c0e50be8469a429fb8852fe3eb3a2ede"
     ]
    },
    "id": "uZlsJFZnx5Uo",
    "outputId": "e39c565b-e6cb-489b-a5ca-a565624b89cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████| 189740/189740 [02:14<00:00, 1414.08 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████| 23717/23717 [00:17<00:00, 1395.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████| 23718/23718 [00:16<00:00, 1434.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 189740\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23717\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23718\n",
      "    })\n",
      "})\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "batch_size = 32\n",
    "tokenized_datasets_dict = {}\n",
    "\n",
    "# Iterate over splits\n",
    "for split in raw_datasets.keys():\n",
    "    print(f\"Processing split: {split}\")\n",
    "\n",
    "    # Tokenize this split using your preprocessing function\n",
    "    tokenized_split = raw_datasets[split].map(\n",
    "        lambda x: preprocess_function(x, tokenizer=en_indic_tokenizer, ip=ip),\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=raw_datasets[split].column_names  # optional\n",
    "    )\n",
    "\n",
    "    tokenized_datasets_dict[split] = tokenized_split\n",
    "\n",
    "# Rebuild DatasetDict\n",
    "tokenized_datasets = DatasetDict(tokenized_datasets_dict)\n",
    "\n",
    "# Optional: inspect\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 189740\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23717\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23718\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/190 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   5%|▏  | 9/190 [00:00<00:02, 80.51ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  11%|▏ | 21/190 [00:00<00:01, 99.51ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  19%|▏| 36/190 [00:00<00:01, 119.69ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  29%|▎| 55/190 [00:00<00:00, 143.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  39%|▍| 74/190 [00:00<00:00, 159.40ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  49%|▍| 93/190 [00:00<00:00, 168.57ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  59%|▌| 112/190 [00:00<00:00, 173.88ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  69%|▋| 131/190 [00:00<00:00, 177.76ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  79%|▊| 150/190 [00:00<00:00, 180.59ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  89%|▉| 169/190 [00:01<00:00, 183.12ba/s\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 190/190 [00:01<00:00, 165.57ba/s\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):   2%|▎               |  553kB / 25.6MB,  461kB/s  \u001b[A\n",
      "Processing Files (0 / 1):   6%|█               | 1.66MB / 25.6MB, 1.04MB/s  \u001b[A\n",
      "Processing Files (0 / 1):   9%|█▍              | 2.21MB / 25.6MB, 1.11MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  13%|██              | 3.32MB / 25.6MB, 1.51MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  17%|██▊             | 4.43MB / 25.6MB, 1.84MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  30%|████▊           | 7.75MB / 25.6MB, 2.98MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  35%|█████▌          | 8.85MB / 25.6MB, 3.16MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  43%|██████▉         | 11.1MB / 25.6MB, 3.46MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  48%|███████▌        | 12.2MB / 25.6MB, 3.58MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  63%|██████████      | 16.0MB / 25.6MB, 4.46MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  99%|███████████████▉| 25.5MB / 25.6MB, 6.70MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 25.6MB / 25.6MB, 4.27MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 25.6MB / 25.6MB, 4.00MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 25.6MB / 25.6MB, 4.00MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:09<00:00,  9.54s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██| 24/24 [00:00<00:00, 164.18ba/s]\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):  90%|██████████████▍ | 2.89MB / 3.20MB, 14.4MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 3.20MB / 3.20MB, 5.32MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 3.20MB / 3.20MB, 3.20MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 3.20MB / 3.20MB, 3.20MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██| 24/24 [00:00<00:00, 167.33ba/s]\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):  87%|█████████████▉  | 2.81MB / 3.21MB, 14.1MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 3.21MB / 3.21MB, 5.35MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 3.21MB / 3.21MB, 3.21MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 3.21MB / 3.21MB, 3.21MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_en_indic_tokenized/commit/f4d544e29ed675955681bb2461a151e4aab509e9', commit_message='Upload dataset', commit_description='', oid='f4d544e29ed675955681bb2461a151e4aab509e9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_en_indic_tokenized', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/shalaka_iitb_en_indic_tokenized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.push_to_hub(\n",
    "    \"thenlpresearcher/shalaka_iitb_en_indic_tokenized\", \n",
    "    private=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|█| 189740/189740 [00:00<00:00, 726534.06 exampl\n",
      "Generating validation split: 100%|█| 23717/23717 [00:00<00:00, 722665.18 exa\n",
      "Generating test split: 100%|█| 23718/23718 [00:00<00:00, 734439.04 examples/\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tokenized_datasets = load_dataset(\"thenlpresearcher/shalaka_iitb_en_indic_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189740\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23717\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23718\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4kA3jpsHcFE"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooZLnaC1HhBq"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the AutoModelForSeq2SeqLM class. Like with the tokenizer, the from_pretrained method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPagNnmWHoQW"
   },
   "source": [
    "To instantiate a Seq2SeqTrainer, we will need to define three more things. The most important is the [Seq2SeqTrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "model_checkpoint = en_indic_ckpt_dir\n",
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "source_lang = 'eng_Ltn'\n",
    "target_lang = 'mar_Deva'\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-iitb-orig-punct-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=4000,                 # evaluate every 2000 training steps\n",
    "    save_steps=4000,                 # save checkpoint every 2000 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,                     # if GPU supports\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    \n",
    "    # --- Add these for pushing to Hub ---\n",
    "    push_to_hub=True,                                 # enables upload\n",
    "    hub_model_id=f\"thenlpresearcher/iitb_punct_orig_finetuned_{source_lang}_to_{target_lang}\",  # your repo name\n",
    "    hub_private_repo=False,                            # makes it private\n",
    "    hub_strategy=\"end\",                        # uploads at each save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk2p1KEwx5Ur",
    "outputId": "28118e90-17a6-460a-8694-3b50cc6660c5"
   },
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# args = Seq2SeqTrainingArguments(\n",
    "#     f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "#     eval_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=1,\n",
    "#     predict_with_generate=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9UUwvXCH8Op"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the cell and customize the weight decay. Since the Seq2SeqTrainer will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the predict_with_generate option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
    "\n",
    "Model will save under **{model_name}-finetuned-{source_lang}-to-{target_lang}** directory\n",
    "\n",
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "qV9wfuvZx5Us"
   },
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForSeq2Seq(en_indic_tokenizer, model=en_indic_model,\n",
    "#     padding=True,          # default: pad dynamically per batch\n",
    "#     return_tensors=\"pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw7by60eH_p5"
   },
   "source": [
    "The last thing to define for our Seq2SeqTrainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_indictrans_text(preds, labels, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    \"\"\"\n",
    "    Remove language prefix from predictions and labels for evaluation.\n",
    "    For decoder outputs, the prefix is TARGET_LANG SOURCE_LANG.\n",
    "    \"\"\"\n",
    "    prefix = f\"{target_lang} {source_lang}\"\n",
    "\n",
    "    def remove_prefix(text):\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    # Strip prefixes\n",
    "    preds = [remove_prefix(p) for p in preds]\n",
    "    labels = [[remove_prefix(l)] for l in labels]  # BLEU expects list of lists\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def compute_metrics_indictrans(eval_preds, tokenizer, metric, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "#     \"\"\"\n",
    "#     Decode model predictions and labels, remove prefixes, compute BLEU.\n",
    "#     \"\"\"\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "\n",
    "#     # Decode predictions\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Replace -100 with pad_token_id for labels and decode\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Remove prefixes or do custom postprocessing\n",
    "#     decoded_preds, decoded_labels = postprocess_indictrans_text(\n",
    "#         decoded_preds, decoded_labels, source_lang=source_lang, target_lang=target_lang\n",
    "#     )\n",
    "\n",
    "#     # BLEU expects references as list of lists\n",
    "#     decoded_labels = [[lbl] for lbl in decoded_labels]\n",
    "# #     print(decoded_labels)\n",
    "\n",
    "#     # Compute BLEU\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "#     # Average generated length (non-padding tokens)\n",
    "#     prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "#     result[\"gen_len\"] = float(np.mean(prediction_lens))\n",
    "\n",
    "#     # Round metrics for readability\n",
    "#     return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57347/3902879281.py:79: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentences: ['mar_Deva eng_Latn अत्यंत तातडीच्या रुग्णांची स्थिती दररोज अद्ययावत करावी. ( आणि ही माहिती एनओटिटीओ येथे पाठवावी )', 'mar_Deva eng_Latn संसर्ग सूचित करणार\\\\u093C्या त्याच्या लक्षणांमुळे तीव्र पुरस्थग्रंथीशोथाचे निदान करणे तुलनेने सोपे जाते.', 'mar_Deva eng_Latn हे सहसा चिवट व पारदर्शक कागदामध्ये गुंडाळले जाते आणि लहान गोळीच्या आकारात बांधले जाते.', 'mar_Deva eng_Latn डेंग्यू लेखनात याचा समावेश केला आहे.', 'mar_Deva eng_Latn शिवाय, भारतात पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचे प्रमाण अधिक असल्याचे आढळून आले आहे.', 'mar_Deva eng_Latn स्पोकेन व्हॅली हे वॉशिंग्टन राज्यातील एक शहर आहे.', 'mar_Deva eng_Latn आपण breast crawl ह्या स्पोकन ट्युटोरिअलच्या अंतिम टप्प्यात पोहोचलो आहोत.', 'mar_Deva eng_Latn कॉन्सोल वर दाखवल्याप्रमाणे, पॅनेल वर स्ट्रक्चर Human Pancreatic Glucokinase साठी substrate Glucose आहे.', \"mar_Deva eng_Latn आदर्शस्वरूप दुष्टीकोनातून पाहता हे शब्द'माझ्या देशाच्या सन्मानासाठी आणि खेळाच्या गौरवासाठी'ह्या ऑलिम्पिकच्या पारंपरिक शपथेसमान भावनाच धारण करतात.\", 'mar_Deva eng_Latn नाहीतर, तुम्ही वेगळे होण्याच्या प्रसंगाची अखेर मालकीहक्कावरूनच्या भांडाभांडीत होऊ शकते.']\n",
      "Generated translations: ['सुपर - उर्जित रुग्णांच्या स्थिती अद्ययावत करण्यासाठी दररोज आवश्यक आहे ( आणि माहिती नोंदवायला गेली पाहिजे ).', 'तीव्र प्रोस्टेटायटीस हे संसर्ग सूचित करणाऱ्या लक्षणांमुळे निदान करणे तुलनेने सोपे आहे.', 'हे सहसा सेलोफोन कागदावर गुंडाळलेले आणि लहान चेंडूच्या आकारात बांधलेले असते.', 'डेंग्यू राईट अपमध्ये तो समाविष्ट आहे.', 'पुढे, भारतातील पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचा दर जास्त असल्याचे आढळून आले आहे.', 'स्पोकेन व्हॅली हे वॉशिंग्टन राज्यातील एक शहर आहे.', 'या अभ्यासक्रमाच्या शेवटी आम्हाला धन्यवाद.', 'कन्सोलवर दर्शविल्याप्रमाणे, पॅनेलवरील रचना मानवी स्वादुपिंडाच्या ग्लुकोजिनाससह सब्सट्रेट ग्लुकोजसह आहे.', 'इन टर्म ऑफ अ आयडियल, हे शब्द पारंपारिक ऑलिम्पिक ओथप्रमाणेच त्याच भावना धारण करतातः \" फॉर द ऑनर ऑफ माय कंट्री अँड द ग्लोरी ऑफ स्पोर्ट \".', 'अन्यथा, तुम्ही वेगळ्या कार्यक्रमात मालकी मिळवण्याचा शेवट करू शकता.']\n",
      "Results: {'bleu': 17.0912, 'gen_len': 21.1}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load model & tokenizer\n",
    "# -----------------------------\n",
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir, trust_remote_code=True\n",
    "    )\n",
    "    if quantization == \"fp16\":\n",
    "        model = model.half()\n",
    "    model.to(DEVICE)\n",
    "    return tokenizer, model\n",
    "\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Prepare small validation batch\n",
    "# -----------------------------\n",
    "# tokenized_datasets must already exist (from previous preprocessing)\n",
    "small_val_dataset = tokenized_datasets[\"validation\"].select(range(10))\n",
    "\n",
    "input_ids = torch.tensor(small_val_dataset[\"input_ids\"]).to(DEVICE)\n",
    "attention_mask = torch.tensor(small_val_dataset[\"attention_mask\"]).to(DEVICE)\n",
    "labels = torch.tensor(small_val_dataset[\"labels\"]).to(DEVICE)\n",
    "\n",
    "# Decode reference sentences for metric\n",
    "reference_sentences = [\n",
    "    en_indic_tokenizer.decode(\n",
    "        np.where(l.cpu().numpy() != -100, l.cpu().numpy(), en_indic_tokenizer.pad_token_id),\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    for l in labels\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Generate translations\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    generated_tokens = en_indic_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=en_indic_model.config.decoder_start_token_id\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Define postprocessing\n",
    "# -----------------------------\n",
    "def postprocess_indictrans_text(preds, labels, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    \"\"\"\n",
    "    Remove language prefix from predictions and labels for evaluation.\n",
    "    BLEU expects list of references per prediction.\n",
    "    \"\"\"\n",
    "    prefix = f\"{target_lang} {source_lang}\"\n",
    "\n",
    "    def remove_prefix(text):\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    preds = [remove_prefix(p) for p in preds]\n",
    "    labels = [[remove_prefix(l)] for l in labels]  # BLEU expects list of lists\n",
    "    return preds, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Compute metrics\n",
    "# -----------------------------\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics_indictrans(eval_preds, tokenizer, metric, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Convert to numpy if tensors\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Replace -100 with pad_token_id for labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Optional postprocessing\n",
    "    decoded_preds, decoded_labels = postprocess_indictrans_text(decoded_preds, decoded_labels, source_lang, target_lang)\n",
    "\n",
    "    # Compute BLEU\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    # Average generated length (non-padding tokens)\n",
    "    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "    result[\"gen_len\"] = float(np.mean(prediction_lens))\n",
    "\n",
    "    # Round metrics for readability\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Evaluate on sample batch\n",
    "# -----------------------------\n",
    "results = compute_metrics_indictrans(\n",
    "    (generated_tokens, labels),\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "print(\"Reference sentences:\", reference_sentences)\n",
    "print(\"Generated translations:\", en_indic_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 100.0, 'gen_len': 1.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_preds = [[60726, 59836, 2134, 5172, 4, 2]]  # example prediction ids\n",
    "sample_labels = [[60726, 59836, 2134, 5172, 4, -100]]  # example label ids\n",
    "compute_metrics_indictrans((sample_preds, sample_labels), tokenizer=en_indic_tokenizer, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 189740\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23717\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 23718\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mar_Deva eng_Latn अत्यंत तातडीच्या रुग्णांची स्थिती दररोज अद्ययावत करावी. ( आणि ही माहिती एनओटिटीओ येथे पाठवावी )', 'mar_Deva eng_Latn संसर्ग सूचित करणार\\\\u093C्या त्याच्या लक्षणांमुळे तीव्र पुरस्थग्रंथीशोथाचे निदान करणे तुलनेने सोपे जाते.', 'mar_Deva eng_Latn हे सहसा चिवट व पारदर्शक कागदामध्ये गुंडाळले जाते आणि लहान गोळीच्या आकारात बांधले जाते.', 'mar_Deva eng_Latn डेंग्यू लेखनात याचा समावेश केला आहे.', 'mar_Deva eng_Latn शिवाय, भारतात पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचे प्रमाण अधिक असल्याचे आढळून आले आहे.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mar_Deva eng_Latn अत्यंत तातडीच्या रुग्णांची स्थिती दररोज अद्ययावत करावी. ( आणि ही माहिती एनओटिटीओ येथे पाठवावी )',\n",
       " 'mar_Deva eng_Latn संसर्ग सूचित करणार\\\\u093C्या त्याच्या लक्षणांमुळे तीव्र पुरस्थग्रंथीशोथाचे निदान करणे तुलनेने सोपे जाते.',\n",
       " 'mar_Deva eng_Latn हे सहसा चिवट व पारदर्शक कागदामध्ये गुंडाळले जाते आणि लहान गोळीच्या आकारात बांधले जाते.',\n",
       " 'mar_Deva eng_Latn डेंग्यू लेखनात याचा समावेश केला आहे.',\n",
       " 'mar_Deva eng_Latn शिवाय, भारतात पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचे प्रमाण अधिक असल्याचे आढळून आले आहे.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: pick 5 validation examples\n",
    "sample_batch = tokenized_datasets[\"validation\"][:5]\n",
    "# print(sample_batch)\n",
    "decode_tokenized_batch(en_indic_tokenizer, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mock Predictions vs References:\n",
      "\n",
      "Prediction: mar_Deva eng_Latn अत्यंत तातडीच्या रुग्णांची स्थिती दररोज अद्ययावत करावी. ( आणि ही माहिती एनओटिटीओ येथे पाठवावी )\n",
      "Reference:  mar_Deva eng_Latn अत्यंत तातडीच्या रुग्णांची स्थिती दररोज अद्ययावत करावी. ( आणि ही माहिती एनओटिटीओ येथे पाठवावी )\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn संसर्ग सूचित करणार\\u093C्या त्याच्या लक्षणांमुळे तीव्र पुरस्थग्रंथीशोथाचे निदान करणे तुलनेने सोपे जाते.\n",
      "Reference:  mar_Deva eng_Latn संसर्ग सूचित करणार\\u093C्या त्याच्या लक्षणांमुळे तीव्र पुरस्थग्रंथीशोथाचे निदान करणे तुलनेने सोपे जाते.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn हे सहसा चिवट व पारदर्शक कागदामध्ये गुंडाळले जाते आणि लहान गोळीच्या आकारात बांधले जाते.\n",
      "Reference:  mar_Deva eng_Latn हे सहसा चिवट व पारदर्शक कागदामध्ये गुंडाळले जाते आणि लहान गोळीच्या आकारात बांधले जाते.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn डेंग्यू लेखनात याचा समावेश केला आहे.\n",
      "Reference:  mar_Deva eng_Latn डेंग्यू लेखनात याचा समावेश केला आहे.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn शिवाय, भारतात पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचे प्रमाण अधिक असल्याचे आढळून आले आहे.\n",
      "Reference:  mar_Deva eng_Latn शिवाय, भारतात पुरुषांच्या तुलनेत महिलांमध्ये आत्महत्येचे प्रमाण अधिक असल्याचे आढळून आले आहे.\n",
      "-----\n",
      "Metrics: {'bleu': 100.0, 'gen_len': 31.4}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pick 5 examples from validation\n",
    "sample_batch = tokenized_datasets[\"validation\"].select(range(5))\n",
    "\n",
    "# Use labels as \"mock predictions\" (or you can modify them slightly)\n",
    "mock_preds = np.array(sample_batch[\"labels\"])\n",
    "\n",
    "# Prepare eval_preds tuple\n",
    "# Normally: (predictions, labels)\n",
    "eval_preds = (mock_preds, np.array(sample_batch[\"labels\"]))\n",
    "\n",
    "# Compute metrics\n",
    "results = compute_metrics_indictrans(\n",
    "    eval_preds,\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    metric=metric,\n",
    "    source_lang=\"eng_Latn\",\n",
    "    target_lang=\"mar_Deva\"\n",
    ")\n",
    "\n",
    "# Decode predictions and labels for inspection\n",
    "decoded_preds = en_indic_tokenizer.batch_decode(\n",
    "    mock_preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "decoded_labels = en_indic_tokenizer.batch_decode(\n",
    "    np.where(np.array(sample_batch[\"labels\"]) != -100,\n",
    "             np.array(sample_batch[\"labels\"]),\n",
    "             en_indic_tokenizer.pad_token_id),\n",
    "    skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Print for visual check\n",
    "print(\"Sample Mock Predictions vs References:\\n\")\n",
    "for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "print(\"Metrics:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XK7px-IDkC"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the Seq2SeqTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Data collator for seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    model=en_indic_model,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "# Seq2SeqTrainer setup (future-proof)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=en_indic_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    # tokenizer=en_indic_tokenizer,  # deprecated\n",
    "    processing_class=en_indic_tokenizer,  # use instead of tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_preds: compute_metrics_indictrans(\n",
    "        eval_preds,\n",
    "        tokenizer=en_indic_tokenizer,\n",
    "        metric=metric\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "c7zuF8rLx5Uy"
   },
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=en_indic_tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1TskjmQIGKd"
   },
   "source": [
    "We can now finetune our model by just calling the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_indic_model = en_indic_model.to(DEVICE)\n",
    "# if en_indic_model == \"cuda\":\n",
    "#     model.half()   # <-- this casts everything to FP16 manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 828, 14552, 58157, 36732, 11828, 7149, 28132, 254, 9726, 9537, 4, 22, 68, 59, 857, 1237, 79, 102, 609, 79, 3028, 1863, 55173, 21, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 25564, 10410, 5670, 18387, 7218, 4168, 30963, 4150, 539, 3244, 1564, 8107, 5806, 2213, 2424, 2351, 3534, 71, 304, 2663, 3822, 249, 8560, 3067, 7358, 46866, 9243, 1508, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 186, 35133, 11485, 138, 76, 17846, 24468, 4043, 37985, 293, 283, 1508, 68, 6712, 22362, 332, 2113, 359, 7698, 283, 1508, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 42255, 153, 1519, 2168, 6176, 4607, 1632, 1135, 38, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 11684, 5, 6566, 1098, 2315, 25511, 6322, 3182, 5753, 24864, 1426, 288, 6918, 35100, 1505, 38, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "small_eval_dataset = tokenized_datasets[\"validation\"].select(range(50))\n",
    "print(small_eval_dataset[\"labels\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_indictrans(eval_preds, tokenizer, metric, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Convert to numpy if tensors\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Replace -100 with pad_token_id for labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Optional postprocessing\n",
    "    decoded_preds, decoded_labels = postprocess_indictrans_text(decoded_preds, decoded_labels, source_lang, target_lang)\n",
    "\n",
    "    # Compute BLEU\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    # Average generated length (non-padding tokens)\n",
    "    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "    result[\"gen_len\"] = float(np.mean(prediction_lens))\n",
    "\n",
    "    # Round metrics for readability\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sonIRAfDx5Uz",
    "outputId": "8a0bc390-6900-424c-f92e-e3a841ead986"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4001' max='118590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4001/118590 13:25 < 6:24:50, 4.96 it/s, Epoch 0.34/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='494' max='1483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 494/1483 03:23 < 06:48, 2.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/thenlpresearcher/iitb_punct_orig_finetuned_eng_Ltn_to_mar_Deva:\n",
      "- configuration_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/thenlpresearcher/iitb_punct_orig_finetuned_eng_Ltn_to_mar_Deva:\n",
      "- modeling_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "checkpoint_dir = \"thenlpresearcher/iitb_punct_orig_finetuned_eng_Ltn_to_mar_Deva\"\n",
    "\n",
    "# Load model (custom class loaded via trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load tokenizer (custom tokenizer class loaded via trust_remote_code=True)\n",
    "tokenizer = en_indic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:22<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations saved to indictrans2_test_predictions.csv\n",
      "BLEU: 20.46\n",
      "chrF++: 58.73\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Adjust for your GPU memory\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "# Collect source and reference sentences\n",
    "src_sentences = raw_datasets[\"test\"][\"src\"][:1000]\n",
    "ref_sentences = raw_datasets[\"test\"][\"tgt\"][:1000]\n",
    "\n",
    "\n",
    "prefix = f\"{tgt_lang} {src_lang}\"\n",
    "\n",
    "def remove_prefix(text):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ip,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "p_t = []\n",
    "for t in all_translations:\n",
    "    p_t.append(remove_prefix(t))\n",
    "\n",
    "# Save translations alongside references\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"reference\": ref_sentences,\n",
    "    \"prediction\": p_t\n",
    "})\n",
    "results_df.to_csv(\"orig_indictrans2_test_predictions_test.csv\", index=False)\n",
    "\n",
    "print(\"Translations saved to indictrans2_test_predictions.csv\")\n",
    "\n",
    "# ---- Evaluation metrics ---- #\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "chrf = load_metric(\"chrf\")\n",
    "\n",
    "# Format for sacrebleu: references need to be list of lists\n",
    "bleu_score = bleu.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "chrf_score = chrf.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "\n",
    "print(f\"BLEU: {bleu_score['score']:.2f}\")\n",
    "print(f\"chrF++: {chrf_score['score']:.2f}\")\n",
    "\n",
    "# Optional: save metrics\n",
    "with open(\"indictrans2_test_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"BLEU: {bleu_score['score']:.2f}\\n\")\n",
    "    f.write(f\"chrF++: {chrf_score['score']:.2f}\\n\")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>reference</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Welsh has over 700,000 speakers in the whole o...</td>\n",
       "      <td>संपूर्ण युनायटेड किंगडममध्ये वेल्शचे ७००,००० प...</td>\n",
       "      <td>संपूर्ण युनायटेड किंगडममध्ये वेल्शचे &lt;ID1&gt; हून...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Let us select the first one: a is equal to b.</td>\n",
       "      <td>पहिला: a=b (‘a is equal to b’ )निवडू.</td>\n",
       "      <td>पहिला पर्याय निवडू. a is equal to b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Type a filename, choose a location and click o...</td>\n",
       "      <td>फाईलचे नाव टाईप करा. फाईल सेव्ह करण्याची जागा ...</td>\n",
       "      <td>फाईलचे नाव टाईप करून स्थान निवडा आणि वरती उजवी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't expect to find such a fantastic bike ...</td>\n",
       "      <td>सायकलच्या दुकानात गेल्यानंतर ५ मिनिटांच्या आत ...</td>\n",
       "      <td>सायकलच्या दुकानात जाताना 5 मिनिटांच्या आत माझ्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let us try it.</td>\n",
       "      <td>हे वापरून बघू.</td>\n",
       "      <td>हे करून पाहू.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  Welsh has over 700,000 speakers in the whole o...   \n",
       "1      Let us select the first one: a is equal to b.   \n",
       "2  Type a filename, choose a location and click o...   \n",
       "3  I didn't expect to find such a fantastic bike ...   \n",
       "4                                     Let us try it.   \n",
       "\n",
       "                                           reference  \\\n",
       "0  संपूर्ण युनायटेड किंगडममध्ये वेल्शचे ७००,००० प...   \n",
       "1              पहिला: a=b (‘a is equal to b’ )निवडू.   \n",
       "2  फाईलचे नाव टाईप करा. फाईल सेव्ह करण्याची जागा ...   \n",
       "3  सायकलच्या दुकानात गेल्यानंतर ५ मिनिटांच्या आत ...   \n",
       "4                                     हे वापरून बघू.   \n",
       "\n",
       "                                          prediction  \n",
       "0  संपूर्ण युनायटेड किंगडममध्ये वेल्शचे <ID1> हून...  \n",
       "1                पहिला पर्याय निवडू. a is equal to b  \n",
       "2  फाईलचे नाव टाईप करून स्थान निवडा आणि वरती उजवी...  \n",
       "3  सायकलच्या दुकानात जाताना 5 मिनिटांच्या आत माझ्...  \n",
       "4                                      हे करून पाहू.  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अफगाणिस्तानमध्ये हिंदू फारच कमी असले तरी मालदीवमध्ये हिंदू नाहीत.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122706\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))  # should match what you trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (0 / 0): |                                                                                       |  0.00B /  0.00B            \n",
      "Processing Files (2 / 2): 100%|███████████████████████████████████████████████████████████████████████████████████| 6.51MB / 6.51MB,  0.00B/s  \u001b[A\n",
      "New Data Upload: |                                                                                                |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thenlpresearcher/finetuned_indictrans2_1/commit/b5730ad76286e95d1ce6ed69f704e915664a8b1f', commit_message='Upload tokenizer', commit_description='', oid='b5730ad76286e95d1ce6ed69f704e915664a8b1f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/thenlpresearcher/finetuned_indictrans2_1', endpoint='https://huggingface.co', repo_type='model', repo_id='thenlpresearcher/finetuned_indictrans2_1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi\n",
    "\n",
    "# # Assuming your tokenizer is `en_indic_tokenizer`\n",
    "# en_indic_tokenizer.push_to_hub(\"thenlpresearcher/finetuned_indictrans2_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans with safety checks.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    # Safe access for decoder_start_token_id\n",
    "    decoder_start_token_id = getattr(model.config, \"decoder_start_token_id\", None)\n",
    "    pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "    eos_token_id = getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    if decoder_start_token_id is None:\n",
    "        print(\"[Warning] decoder_start_token_id is None. Using default generation behavior.\")\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch_preprocessed = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        if not isinstance(batch_preprocessed, list) or len(batch_preprocessed) == 0:\n",
    "            print(f\"[Warning] Preprocessed batch is empty at index {i}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Debug: print first 2 sentences after preprocessing\n",
    "#         print(f\"[Debug] Preprocessed batch sample: {batch_preprocessed[:2]}\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_preprocessed,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations with safety parameters\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_tokens = model.generate(\n",
    "                    **inputs,\n",
    "                    use_cache=True,\n",
    "                    min_length=5,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=5,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=decoder_start_token_id,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Generation failed for batch starting at index {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Debug: print first 2 decoded outputs\n",
    "#         print(f\"[Debug] Decoded sample: {decoded_texts[:2]}\")\n",
    "\n",
    "        # Postprocess translations\n",
    "        try:\n",
    "            postprocessed = ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "            translations += postprocessed\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Postprocessing failed for batch starting at index {i}: {e}\")\n",
    "            translations += decoded_texts  # fallback\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=5,  # ensure some minimum length\n",
    "                max_length=max_length,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True,\n",
    "                decoder_start_token_id=model.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Postprocess translations (remove language prefix, entity replacement, etc.)\n",
    "        translations += ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/thenlpresearcher/indictrans2-indic-indic-dist-320M-finetuned-eng_Ltn-to-mar_Deva:\n",
      "- configuration_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/thenlpresearcher/indictrans2-indic-indic-dist-320M-finetuned-eng_Ltn-to-mar_Deva:\n",
      "- modeling_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "ckpt_dir = \"thenlpresearcher/indictrans2-indic-indic-dist-320M-finetuned-eng_Ltn-to-mar_Deva\"\n",
    "quantization = None\n",
    "# Load model directly\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt_dir, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "    num_rows: 23482\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                       | 2/734 [00:23<2:20:19, 11.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(src_sentences), BATCH_SIZE)):\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m src_sentences[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[0;32m---> 22\u001b[0m     translations \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_translate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_lang\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mip\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     all_translations\u001b[38;5;241m.\u001b[39mextend(translations)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Save translations alongside references\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m, in \u001b[0;36mbatch_translate\u001b[0;34m(input_sentences, src_lang, tgt_lang, model, tokenizer, ip)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Generate translations using the model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 54\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Decode the generated tokens into text\u001b[39;00m\n\u001b[1;32m     64\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     65\u001b[0m     generated_tokens,\n\u001b[1;32m     66\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2644\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2637\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2638\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2639\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2640\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2641\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2642\u001b[0m     )\n\u001b[1;32m   2643\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2644\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2654\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   2655\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2656\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2657\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:4070\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   4067\u001b[0m beam_indices \u001b[38;5;241m=\u001b[39m running_beam_indices\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m   4069\u001b[0m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[0;32m-> 4070\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   4071\u001b[0m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[1;32m   4072\u001b[0m     flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[1;32m   4073\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2770\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2767\u001b[0m         result\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpast_key_values\u001b[38;5;241m.\u001b[39mto_legacy_cache()\n\u001b[1;32m   2768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 2770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;124;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[1;32m   2773\u001b[0m \u001b[38;5;124;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[1;32m   2774\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2775\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[1;32m   2776\u001b[0m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Adjust for your GPU memory\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "# Collect source and reference sentences\n",
    "src_sentences = raw_datasets[\"test\"][\"src\"]\n",
    "ref_sentences = raw_datasets[\"test\"][\"tgt\"]\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ip\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "# Save translations alongside references\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"reference\": ref_sentences,\n",
    "    \"prediction\": all_translations\n",
    "})\n",
    "results_df.to_csv(\"indictrans2_test_predictions1.csv\", index=False)\n",
    "\n",
    "print(\"Translations saved to indictrans2_test_predictions.csv\")\n",
    "\n",
    "# ---- Evaluation metrics ---- #\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "chrf = load_metric(\"chrf\")\n",
    "\n",
    "# Format for sacrebleu: references need to be list of lists\n",
    "bleu_score = bleu.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "chrf_score = chrf.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "\n",
    "print(f\"BLEU: {bleu_score['score']:.2f}\")\n",
    "print(f\"chrF++: {chrf_score['score']:.2f}\")\n",
    "\n",
    "# Optional: save metrics\n",
    "with open(\"indictrans2_test_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"BLEU: {bleu_score['score']:.2f}\\n\")\n",
    "    f.write(f\"chrF++: {chrf_score['score']:.2f}\\n\")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translations saved to indictrans2_test_predictions.csv\n",
      "BLEU: 0.00\n",
      "chrF++: 0.00\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32 # Adjust for your GPU memory\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "# Collect source and reference sentences\n",
    "src_sentences = raw_datasets[\"test\"][\"src\"][:35]\n",
    "ref_sentences = raw_datasets[\"test\"][\"tgt\"][:35]\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ip,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "# Save translations alongside references\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"reference\": ref_sentences,\n",
    "    \"prediction\": all_translations\n",
    "})\n",
    "results_df.to_csv(\"indictrans2_test_predictions1.csv\", index=False)\n",
    "\n",
    "print(\"Translations saved to indictrans2_test_predictions.csv\")\n",
    "\n",
    "# ---- Evaluation metrics ---- #\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "chrf = load_metric(\"chrf\")\n",
    "\n",
    "# Format for sacrebleu: references need to be list of lists\n",
    "bleu_score = bleu.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "chrf_score = chrf.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "\n",
    "print(f\"BLEU: {bleu_score['score']:.2f}\")\n",
    "print(f\"chrF++: {chrf_score['score']:.2f}\")\n",
    "\n",
    "# Optional: save metrics\n",
    "with open(\"indictrans2_test_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"BLEU: {bleu_score['score']:.2f}\\n\")\n",
    "    f.write(f\"chrF++: {chrf_score['score']:.2f}\\n\")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ੰ لیڪچرر कुरसीिसतानषणि रस्क ڄمی نڪرندی पणि</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer(\"eng_Latn mar_Deva Though there are very less Hindus...\")[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-SSyPwox5U1",
    "outputId": "c06f1ef4-8a5e-49a7-ef48-3c4997d3623f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('opus-mt-en-ro-finetuned-en-to-ro'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeLJF9PJINyR"
   },
   "source": [
    "Our fine tuned model already saved under *opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000*\n",
    "\n",
    "Load the model and translate some text from english to romanian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kByQ_vxTx5U2",
    "outputId": "c2fef526-afee-4309-9bb5-aef945b31b56"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformers.tokenization_utils.PreTrainedTokenizer.__init__() got multiple values for keyword argument 'src_vocab_file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m src_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMy name is Sarah and I live in London\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindictrans2-indic-indic-dist-320M-finetuned-eng_Ltn-to-mar_Deva/checkpoint-2000\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39msupported_language_codes)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:1035\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m     _ \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1034\u001b[0m     tokenizer_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class()\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m     tokenizer_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2014\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2012\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2260\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2260\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2262\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2265\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/checkpoint-2000/tokenization_indictrans.py:126\u001b[0m, in \u001b[0;36mIndicTransTokenizer.__init__\u001b[0;34m(self, src_vocab_fp, tgt_vocab_fp, src_spm_fp, tgt_spm_fp, unk_token, bos_token, eos_token, pad_token, do_lower_case, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_encoder[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos_token]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_encoder[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos_token]\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    127\u001b[0m     src_vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_vocab_fp,\n\u001b[1;32m    128\u001b[0m     tgt_vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_vocab_fp,\n\u001b[1;32m    129\u001b[0m     do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case,\n\u001b[1;32m    130\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[1;32m    131\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[1;32m    132\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[1;32m    133\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    135\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.tokenization_utils.PreTrainedTokenizer.__init__() got multiple values for keyword argument 'src_vocab_file'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "src_text = ['My name is Sarah and I live in London']\n",
    "\n",
    "checkpoint_dir = \"indictrans2-indic-indic-dist-320M-finetuned-eng_Ltn-to-mar_Deva/checkpoint-2000\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, trust_remote_code=True)\n",
    "\n",
    "print(tokenizer.supported_language_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4h_jIFZgx5U2",
    "outputId": "77d69535-6d1e-486b-df14-382718414698"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type IndicTrans to instantiate a model of type marian. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([122672, 512]) from checkpoint, the shape in current model is torch.Size([122672, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMarianMTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m translated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer(src_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(t, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:4839\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4830\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4832\u001b[0m     (\n\u001b[1;32m   4833\u001b[0m         model,\n\u001b[1;32m   4834\u001b[0m         missing_keys,\n\u001b[1;32m   4835\u001b[0m         unexpected_keys,\n\u001b[1;32m   4836\u001b[0m         mismatched_keys,\n\u001b[1;32m   4837\u001b[0m         offload_index,\n\u001b[1;32m   4838\u001b[0m         error_msgs,\n\u001b[0;32m-> 4839\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4848\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4857\u001b[0m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[1;32m   4858\u001b[0m model\u001b[38;5;241m.\u001b[39m_tp_size \u001b[38;5;241m=\u001b[39m tp_size\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5302\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5299\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5302\u001b[0m         _error_msgs, disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5303\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5305\u001b[0m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:933\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 933\u001b[0m     disk_offload_index, cpu_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:845\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[1;32m    843\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 845\u001b[0m     \u001b[43m_load_parameter_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(\n\u001b[1;32m    849\u001b[0m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[1;32m    850\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:733\u001b[0m, in \u001b[0;36m_load_parameter_into_model\u001b[0;34m(model, param_name, tensor)\u001b[0m\n\u001b[1;32m    731\u001b[0m module, param_type \u001b[38;5;241m=\u001b[39m get_module_from_name(model, param_name)\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# This will check potential shape mismatch if skipped before\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2629\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2621\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2622\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2623\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2625\u001b[0m             ),\n\u001b[1;32m   2626\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2631\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2632\u001b[0m         )\n\u001b[1;32m   2633\u001b[0m     )\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([122672, 512]) from checkpoint, the shape in current model is torch.Size([122672, 1024])."
     ]
    }
   ],
   "source": [
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjn5FnzPI0cS"
   },
   "source": [
    "our fine tune model is doing far better than pre-trained model and close to google translator\n",
    "\n",
    "**input text** -> My name is Sarah and I live in London\n",
    "\n",
    "**pre-trained model prediction** -> Numele meu este Sarah şi locuiesc în Londra.\n",
    "\n",
    "**fine-tune model prediction** -> Numele meu este Sarah şi locuiesc la Londra\n",
    "\n",
    "**google translator prediction** -> Numele meu este Sarah şi locuiesc la Londra\n",
    "\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001c2cf8d21d4703bc190149a82d06ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c6c16b336c404fa84cba2ee42debde",
       "IPY_MODEL_31bbbf7e34a14335b628add663c71642",
       "IPY_MODEL_a23d83e9f0ea4ac8b57ec5f1614414dc"
      ],
      "layout": "IPY_MODEL_c8c29e7a13804b6c9e97c3ff425bd81d"
     }
    },
    "04672fbdc8754babbb7bb3bfb7bcfd2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "064aa5db0924410bb17646220304faea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c2955e8cff44b5ba998ca584e485e54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12e50a27f9f24575bd86305f3b609095": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_330c9a9e97c9436f9d29bb12c586253a",
      "placeholder": "​",
      "style": "IPY_MODEL_382f2bc5fc944482861293a496fb03ed",
      "value": "100%"
     }
    },
    "13c6c16b336c404fa84cba2ee42debde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7616cd08bb494b8f988b9e893bb8bb25",
      "placeholder": "​",
      "style": "IPY_MODEL_ae117dc6e1fb41eca964e34601c6aa6c",
      "value": "Downloading: 100%"
     }
    },
    "17ecb55f59dd411084d8692b80cc8cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1962a04347894250993d2eb3d2946e5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23e6dd2b833d44aabc5b78a83d7ef136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24cc9a4f2e4a411296dd6822329e5815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b4edc877e524610bb9ecc8103763250",
      "max": 611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af955c59afc54adc8f1072d371b688bc",
      "value": 611
     }
    },
    "31bbbf7e34a14335b628add663c71642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e574d69ffb426b87d3445d26e02926",
      "max": 300887193,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_064aa5db0924410bb17646220304faea",
      "value": 300887193
     }
    },
    "330c9a9e97c9436f9d29bb12c586253a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fd15427c7d4f4189c9ce38924107b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53ec358cc4784c25a50e17bd8764fd56",
       "IPY_MODEL_c916612f156b457f828410fdc85c8def",
       "IPY_MODEL_de1e4b0f94f74b4380de2cb4447352f1"
      ],
      "layout": "IPY_MODEL_04672fbdc8754babbb7bb3bfb7bcfd2e"
     }
    },
    "382f2bc5fc944482861293a496fb03ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4295f3dc0a494198b776f682b327924d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad65c758c1114adab13e6dbf29f660aa",
      "placeholder": "​",
      "style": "IPY_MODEL_eb10554a79a04ecf92517ba96ca956d9",
      "value": " 3.19k/? [00:00&lt;00:00, 53.1kB/s]"
     }
    },
    "44db6a1a111148ceaa804f7302b0629b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48eb8b6feb8a47b79fbd924a7a5c6df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec358cc4784c25a50e17bd8764fd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed5983a37cba4faea852cf2e13b355b4",
      "placeholder": "​",
      "style": "IPY_MODEL_c04bdf05ef684c8fb12ea45833394e77",
      "value": "Downloading: "
     }
    },
    "5600aa1b802c46b183417910f0e18cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b4edc877e524610bb9ecc8103763250": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7616cd08bb494b8f988b9e893bb8bb25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b69f1226cf5457aa164146a890a2c5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44db6a1a111148ceaa804f7302b0629b",
      "placeholder": "​",
      "style": "IPY_MODEL_8c32a5f5cb724610a4ea8a5b42be5b28",
      "value": " 42.0/42.0 [00:00&lt;00:00, 792B/s]"
     }
    },
    "7dac8e5e55ce4e638942c4299b377e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aff67fa78c640bf83598dd7fccc5b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1962a04347894250993d2eb3d2946e5c",
      "placeholder": "​",
      "style": "IPY_MODEL_d5ce787580584a40883fdac45df7917a",
      "value": "Downloading: 100%"
     }
    },
    "8c32a5f5cb724610a4ea8a5b42be5b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97ed312717014144af97d250fcd74d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b689909adc244204af714b9a19e158c0",
      "max": 42,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5e1f379a1084c6e978a633698b4b02a",
      "value": 42
     }
    },
    "98b513fb020744bfbbe0678daa33f2aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c2955e8cff44b5ba998ca584e485e54",
      "max": 1520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1d75f47a76d4bf8b2941fe543e46242",
      "value": 1520
     }
    },
    "a23d83e9f0ea4ac8b57ec5f1614414dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48eb8b6feb8a47b79fbd924a7a5c6df1",
      "placeholder": "​",
      "style": "IPY_MODEL_c2c7fbedceba45aa822c96d7f1cb6134",
      "value": " 287M/287M [00:10&lt;00:00, 29.0MB/s]"
     }
    },
    "a434f279213545da8a92da73bc320504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9aa1538562840f2b969340f1cbf8bbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad65c758c1114adab13e6dbf29f660aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adbad0b85f78492583b2c5b291341030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae117dc6e1fb41eca964e34601c6aa6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af955c59afc54adc8f1072d371b688bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b689909adc244204af714b9a19e158c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b982f93475414c0c8a92601717138dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a434f279213545da8a92da73bc320504",
      "placeholder": "​",
      "style": "IPY_MODEL_17ecb55f59dd411084d8692b80cc8cad",
      "value": "Downloading: "
     }
    },
    "c04bdf05ef684c8fb12ea45833394e77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e574d69ffb426b87d3445d26e02926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c7fbedceba45aa822c96d7f1cb6134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8c29e7a13804b6c9e97c3ff425bd81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c916612f156b457f828410fdc85c8def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adbad0b85f78492583b2c5b291341030",
      "max": 1405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f12dcd6d164513bae3d4d7835cc45e",
      "value": 1405
     }
    },
    "d3190da3507f4ab4be03027179566508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5ce787580584a40883fdac45df7917a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5e1f379a1084c6e978a633698b4b02a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8499a1a46144034b56489025aaee86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db4d60783a2b4cc9ac647f53524e9201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aff67fa78c640bf83598dd7fccc5b97",
       "IPY_MODEL_97ed312717014144af97d250fcd74d9d",
       "IPY_MODEL_7b69f1226cf5457aa164146a890a2c5e"
      ],
      "layout": "IPY_MODEL_23e6dd2b833d44aabc5b78a83d7ef136"
     }
    },
    "de1e4b0f94f74b4380de2cb4447352f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5600aa1b802c46b183417910f0e18cc4",
      "placeholder": "​",
      "style": "IPY_MODEL_a9aa1538562840f2b969340f1cbf8bbb",
      "value": " 2.81k/? [00:00&lt;00:00, 57.6kB/s]"
     }
    },
    "e1d75f47a76d4bf8b2941fe543e46242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb10554a79a04ecf92517ba96ca956d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed5983a37cba4faea852cf2e13b355b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef09f1189d75431591bebf33d288dacc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56b5b8abcfc415fbf46ca0f2fe619d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12e50a27f9f24575bd86305f3b609095",
       "IPY_MODEL_24cc9a4f2e4a411296dd6822329e5815",
       "IPY_MODEL_f83d5ec459fe4dfd9ebf46fcca9895ee"
      ],
      "layout": "IPY_MODEL_ef09f1189d75431591bebf33d288dacc"
     }
    },
    "f83d5ec459fe4dfd9ebf46fcca9895ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dac8e5e55ce4e638942c4299b377e3a",
      "placeholder": "​",
      "style": "IPY_MODEL_d3190da3507f4ab4be03027179566508",
      "value": " 611/611 [04:36&lt;00:00,  2.78ba/s]"
     }
    },
    "f8f12dcd6d164513bae3d4d7835cc45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd705f2d192c41f8bbbf5c6270e6ffb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b982f93475414c0c8a92601717138dc4",
       "IPY_MODEL_98b513fb020744bfbbe0678daa33f2aa",
       "IPY_MODEL_4295f3dc0a494198b776f682b327924d"
      ],
      "layout": "IPY_MODEL_d8499a1a46144034b56489025aaee86f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
