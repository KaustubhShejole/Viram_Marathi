{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a841bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 237175\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Make sure you have logged in using:\n",
    "# huggingface-cli login\n",
    "# to access this dataset if it requires authentication.\n",
    "\n",
    "# Load the BPCC dataset with the specific config\n",
    "ds = load_dataset(\"thenlpresearcher/iitb_eng_mar_dataset\")\n",
    "\n",
    "# Inspect available splits\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d83032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['src_lang', 'tgt_lang', 'src', 'tgt']\n",
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example, let’s rename Sheet 2 as “Dump”.', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n"
     ]
    }
   ],
   "source": [
    "# Check the first example from the training split\n",
    "mar_ds = ds['train']\n",
    "\n",
    "# Optional: quick check\n",
    "print(mar_ds.column_names)\n",
    "print(mar_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc837779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example, let’s rename Sheet 2 as “Dump”.', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n",
      "['src_lang', 'tgt_lang', 'src', 'tgt']\n"
     ]
    }
   ],
   "source": [
    "print(mar_ds[0])\n",
    "\n",
    "# Optional: see column names\n",
    "print(mar_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435a701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 237175/237175 [00:08<00:00, 27559.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'For example, let’s rename Sheet 2 as “Dump”.', 'tgt': 'उदाहरणार्थ Sheet 2 चे नाव बदलून “Dump” करू.'}\n",
      "{'src_lang': 'eng_Latn', 'tgt_lang': 'mar_Deva', 'src': 'The Options dialogbox appears', 'tgt': 'Options चा डायलॉग बॉक्स उघडेल.'}\n",
      "Total rows: 237175\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# Toggle to alternate keeping/removing punctuation\n",
    "keep_punct = True\n",
    "\n",
    "for ex in tqdm(mar_ds):\n",
    "    if keep_punct:\n",
    "        all_rows.append({\n",
    "            \"src_lang\": ex[\"src_lang\"],\n",
    "            \"tgt_lang\": ex[\"tgt_lang\"],\n",
    "            \"src\": ex[\"src\"],\n",
    "            \"tgt\": ex[\"tgt\"]\n",
    "        })\n",
    "    else:\n",
    "        all_rows.append({\n",
    "            \"src_lang\": ex[\"src_lang\"],\n",
    "            \"tgt_lang\": ex[\"tgt_lang\"],\n",
    "            \"src\": ex[\"src\"].translate(punct_table),\n",
    "            \"tgt\": ex[\"tgt\"]\n",
    "        })\n",
    "    keep_punct = not keep_punct  # alternate for next example\n",
    "\n",
    "# Create a new Dataset\n",
    "mar_ds_expanded = Dataset.from_list(all_rows)\n",
    "\n",
    "# Inspect\n",
    "print(mar_ds_expanded[0])\n",
    "print(mar_ds_expanded[1])\n",
    "print(\"Total rows:\", len(mar_ds_expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423102d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1ee123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/238 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|▎| 79/238 [00:00<00:00, 781.10ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  66%|▋| 158/238 [00:00<00:00, 722.16ba/s\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 238/238 [00:00<00:00, 661.81ba/s\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):   3%|▍               | 1.05MB / 38.4MB,  752kB/s  \u001b[A\n",
      "Processing Files (0 / 1):   4%|▋               | 1.58MB / 38.4MB,  790kB/s  \u001b[A\n",
      "Processing Files (0 / 1):   5%|▉               | 2.11MB / 38.4MB,  958kB/s  \u001b[A\n",
      "Processing Files (0 / 1):   8%|█▎              | 3.16MB / 38.4MB, 1.32MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  11%|█▊              | 4.21MB / 38.4MB, 1.62MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  12%|█▉              | 4.74MB / 38.4MB, 1.69MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  16%|██▋             | 6.32MB / 38.4MB, 2.11MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  18%|██▊             | 6.85MB / 38.4MB, 2.14MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  26%|████▏           | 10.0MB / 38.4MB, 2.94MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  29%|████▌           | 11.1MB / 38.4MB, 3.07MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  47%|███████▍        | 17.9MB / 38.4MB, 4.71MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  49%|███████▉        | 19.0MB / 38.4MB, 4.74MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  63%|██████████      | 24.2MB / 38.4MB, 5.77MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  84%|█████████████▍  | 32.1MB / 38.4MB, 7.30MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  96%|███████████████▎| 36.9MB / 38.4MB, 8.01MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  99%|███████████████▊| 37.9MB / 38.4MB, 7.90MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 38.4MB / 38.4MB, 5.48MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 38.4MB / 38.4MB, 5.19MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 38.4MB / 38.4MB, 5.19MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:09<00:00,  9.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_marathi_punct/commit/fc43236983dee8a6dacaca80e37b65408b520e2c', commit_message='Upload dataset', commit_description='', oid='fc43236983dee8a6dacaca80e37b65408b520e2c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_marathi_punct', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/shalaka_iitb_marathi_punct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, Repository\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# If you have multiple splits, wrap in DatasetDict\n",
    "ds_dict = DatasetDict({\n",
    "    \"marathi_punct\": mar_ds_expanded\n",
    "})\n",
    "\n",
    "# Push to HF\n",
    "ds_dict.push_to_hub(\"thenlpresearcher/shalaka_iitb_marathi_punct\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a5049",
   "metadata": {},
   "source": [
    "### Made validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3215e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192a0ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating marathi_punct split: 100%|█| 237175/237175 [00:00<00:00, 478580.4\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "mar_ds = datasets.load_dataset(\"thenlpresearcher/shalaka_iitb_marathi_punct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666c2edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    marathi_punct: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 237175\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mar_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad58e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 189740\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 23717\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
      "        num_rows: 23718\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming your DatasetDict is called `dataset_dict`\n",
    "dataset = mar_ds['marathi_punct']\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Define split ratios\n",
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "# Compute sizes\n",
    "total = len(dataset)\n",
    "train_size = int(train_frac * total)\n",
    "val_size = int(val_frac * total)\n",
    "test_size = total - train_size - val_size  # ensure all samples are used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset = dataset.select(range(0, train_size))\n",
    "val_dataset = dataset.select(range(train_size, train_size + val_size))\n",
    "test_dataset = dataset.select(range(train_size + val_size, total))\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "split_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0da353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/190 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   2%|   | 4/190 [00:00<00:05, 36.06ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   4%|▏  | 8/190 [00:00<00:04, 37.29ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   6%|▏ | 12/190 [00:00<00:04, 36.94ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   8%|▏ | 16/190 [00:00<00:04, 37.83ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  11%|▏ | 20/190 [00:00<00:04, 38.54ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  13%|▎ | 24/190 [00:00<00:04, 38.89ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  15%|▎ | 28/190 [00:00<00:04, 39.20ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  17%|▎ | 33/190 [00:00<00:03, 39.55ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  19%|▍ | 37/190 [00:00<00:03, 39.64ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  22%|▍ | 41/190 [00:01<00:03, 39.74ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  24%|▍ | 46/190 [00:01<00:03, 39.94ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  27%|▌ | 51/190 [00:01<00:03, 40.07ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  29%|▌ | 56/190 [00:01<00:03, 40.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  32%|▋ | 61/190 [00:01<00:03, 39.12ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  34%|▋ | 65/190 [00:01<00:03, 38.87ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|▋ | 69/190 [00:01<00:03, 38.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  38%|▊ | 73/190 [00:01<00:02, 39.08ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  41%|▊ | 77/190 [00:01<00:02, 38.97ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  43%|▊ | 81/190 [00:02<00:03, 35.80ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  45%|▉ | 85/190 [00:02<00:03, 33.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  47%|▉ | 89/190 [00:02<00:03, 31.95ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  49%|▉ | 93/190 [00:02<00:02, 33.95ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  51%|█ | 97/190 [00:02<00:02, 35.54ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  53%|▌| 101/190 [00:02<00:02, 36.20ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|▌| 105/190 [00:02<00:02, 37.08ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  58%|▌| 110/190 [00:02<00:02, 38.13ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  61%|▌| 115/190 [00:03<00:01, 38.80ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  63%|▋| 120/190 [00:03<00:01, 39.23ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  65%|▋| 124/190 [00:03<00:01, 39.43ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  67%|▋| 128/190 [00:03<00:01, 39.57ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  69%|▋| 132/190 [00:03<00:01, 39.68ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  72%|▋| 136/190 [00:03<00:01, 39.74ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  74%|▋| 140/190 [00:03<00:01, 39.58ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  76%|▊| 144/190 [00:03<00:01, 37.75ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  78%|▊| 149/190 [00:03<00:01, 38.58ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  81%|▊| 153/190 [00:04<00:00, 38.41ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  83%|▊| 157/190 [00:04<00:00, 38.21ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  85%|▊| 161/190 [00:04<00:00, 38.49ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  87%|▊| 165/190 [00:04<00:00, 35.96ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  89%|▉| 169/190 [00:04<00:00, 35.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  91%|▉| 173/190 [00:04<00:00, 36.86ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  93%|▉| 177/190 [00:04<00:00, 37.56ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  95%|▉| 181/190 [00:04<00:00, 37.94ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  97%|▉| 185/190 [00:04<00:00, 38.34ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 190/190 [00:05<00:00, 37.89ba/s]\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):  54%|████████▋       | 18.4MB / 33.9MB, 46.1MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  76%|████████████▏   | 25.8MB / 33.9MB, 43.0MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  79%|████████████▋   | 26.8MB / 33.9MB, 33.5MB/s  \u001b[A\n",
      "Processing Files (0 / 1):  99%|███████████████▊| 33.6MB / 33.9MB, 33.7MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 33.9MB / 33.9MB, 9.43MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 33.9MB / 33.9MB, 8.48MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 33.9MB / 33.9MB, 8.48MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:10<00:00, 10.57s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  12%|▌   | 3/24 [00:00<00:00, 28.68ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  25%|█   | 6/24 [00:00<00:00, 29.06ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  38%|█▌  | 9/24 [00:00<00:00, 29.12ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  54%|█▋ | 13/24 [00:00<00:00, 26.47ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  71%|██▏| 17/24 [00:00<00:00, 29.80ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|███| 24/24 [00:00<00:00, 31.83ba/s]\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1): 100%|███████████████▉| 4.20MB / 4.22MB, 21.0MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 4.22MB / 4.22MB, 4.21MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 4.22MB / 4.22MB, 3.01MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 4.22MB / 4.22MB, 3.01MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:03<00:00,  3.08s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/24 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  12%|▌   | 3/24 [00:00<00:00, 28.74ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  25%|█   | 6/24 [00:00<00:00, 29.26ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  42%|█▎ | 10/24 [00:00<00:00, 31.91ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  58%|█▊ | 14/24 [00:00<00:00, 34.17ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  75%|██▎| 18/24 [00:00<00:00, 32.27ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|███| 24/24 [00:00<00:00, 32.42ba/s]\u001b[A\n",
      "Processing Files (0 / 0): |                    |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):  99%|███████████████▊| 4.20MB / 4.24MB, 21.0MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 4.24MB / 4.24MB, 4.24MB/s  \u001b[A\n",
      "Processing Files (1 / 1): 100%|████████████████| 4.24MB / 4.24MB, 3.03MB/s  \u001b[A\n",
      "New Data Upload: 100%|█████████████████████████| 4.24MB / 4.24MB, 3.03MB/s  \n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:03<00:00,  3.06s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_marathi_punct/commit/2ff26a7643d37d597244363f2d50632506de8979', commit_message='Upload dataset', commit_description='', oid='2ff26a7643d37d597244363f2d50632506de8979', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/shalaka_iitb_marathi_punct', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/shalaka_iitb_marathi_punct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dataset.push_to_hub(\n",
    "    \"thenlpresearcher/shalaka_iitb_marathi_punct\", \n",
    "    private=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9dd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
