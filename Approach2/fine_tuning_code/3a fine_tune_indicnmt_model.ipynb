{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV-pf35571ap"
   },
   "source": [
    "### If you dont want to use Wandb, disable Wandb otherwise optional\n",
    "\n",
    "references for WANDB\n",
    "https://analyticsindiamag.com/hands-on-guide-to-weights-and-biases-wandb-with-python-implementation/\n",
    "\n",
    "https://docs.wandb.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 21 11:51:32 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   53C    P0              69W / 300W |  12746MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   48C    P0              94W / 300W |  46342MiB / 81920MiB |     30%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   58C    P0              76W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             276W / 300W |  12472MiB / 81920MiB |     52%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/bert_punct_model_final/data/Notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tmYJBBbwyiX3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1hOFHi-x5UZ"
   },
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqADZ2Sgx5Ua",
    "outputId": "60c95827-a686-45eb-c201-e23bc8b03b2b"
   },
   "outputs": [],
   "source": [
    "# ! pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN5RtJdM8xi5"
   },
   "source": [
    "Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xKFwtNEx5Uc",
    "outputId": "1206f370-b7a8-4a34-bc6d-36f1a731be6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.53.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKkJGYY19I11"
   },
   "source": [
    "# Fine-tuning a model on a translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MX7DsEmOx5Ud"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu45aGHIAQqM"
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "We will use the [datasets](https://github.com/huggingface/datasets/tree/master/datasets/wmt16) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric. We use the English/Romanian part of the WMT dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "34fd15427c7d4f4189c9ce38924107b5",
      "04672fbdc8754babbb7bb3bfb7bcfd2e",
      "53ec358cc4784c25a50e17bd8764fd56",
      "c916612f156b457f828410fdc85c8def",
      "de1e4b0f94f74b4380de2cb4447352f1",
      "c04bdf05ef684c8fb12ea45833394e77",
      "ed5983a37cba4faea852cf2e13b355b4",
      "f8f12dcd6d164513bae3d4d7835cc45e",
      "adbad0b85f78492583b2c5b291341030",
      "a9aa1538562840f2b969340f1cbf8bbb",
      "5600aa1b802c46b183417910f0e18cc4",
      "fd705f2d192c41f8bbbf5c6270e6ffb0",
      "d8499a1a46144034b56489025aaee86f",
      "b982f93475414c0c8a92601717138dc4",
      "98b513fb020744bfbbe0678daa33f2aa",
      "4295f3dc0a494198b776f682b327924d",
      "17ecb55f59dd411084d8692b80cc8cad",
      "a434f279213545da8a92da73bc320504",
      "e1d75f47a76d4bf8b2941fe543e46242",
      "0c2955e8cff44b5ba998ca584e485e54",
      "eb10554a79a04ecf92517ba96ca956d9",
      "ad65c758c1114adab13e6dbf29f660aa",
      "e3ada02e2bf34b5699475b1eb7801282",
      "9c9af690e08d490dbba5524e35fa6fcb",
      "c5d0a0ee946745609e561ffece8cde76",
      "9d13fcfc7b4e4ef182ef48df08253af4",
      "88c725d411d34463b632ec85fa67ed4a",
      "c4298a4942db4790a357a342c7cc13e7",
      "4d9080a460f34c8abdb3af1fdf0eddce",
      "a8744f13ff43450db14031275a7ba769",
      "edf107491f434e7486765ebda2ad938e",
      "922bfc020eb345c584ab31721a10922c",
      "f9c351511a33475d993fee0d6c1cddba",
      "4b5219e4a1a1400a982acc3065b46e2c"
     ]
    },
    "id": "biPo8vFTx5Ue",
    "outputId": "402f80b1-801c-4452-f502-dfcb9d9e9326"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "raw_datasets = load_dataset(\"thenlpresearcher/iitb_marathi_punct_variants\")\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"sacrebleu\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = evaluate.load(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 Results:\n",
      "Keys: ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "BLEU Score: 100.0\n",
      "\n",
      "Example 2 Results:\n",
      "Keys: ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "BLEU Score: 39.8\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Example 1: perfect match\n",
    "predictions_1 = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references_1 = [\n",
    "    [\"hello there general kenobi\", \"hello there !\"],\n",
    "    [\"foo bar foobar\", \"foo bar foobar\"]\n",
    "]\n",
    "\n",
    "metric = load(\"sacrebleu\")\n",
    "results_1 = metric.compute(predictions=predictions_1, references=references_1)\n",
    "\n",
    "print(\"Example 1 Results:\")\n",
    "print(\"Keys:\", list(results_1.keys()))\n",
    "print(\"BLEU Score:\", round(results_1[\"score\"], 1))\n",
    "print()\n",
    "\n",
    "# Example 2: partial match\n",
    "predictions_2 = [\n",
    "    \"hello there general kenobi\",\n",
    "    \"on our way to ankh morpork\"\n",
    "]\n",
    "references_2 = [\n",
    "    [\"hello there general kenobi\", \"hello there !\"],\n",
    "    [\"goodbye ankh morpork\", \"ankh morpork\"]\n",
    "]\n",
    "\n",
    "results_2 = metric.compute(predictions=predictions_2, references=references_2)\n",
    "\n",
    "print(\"Example 2 Results:\")\n",
    "print(\"Keys:\", list(results_2.keys()))\n",
    "print(\"BLEU Score:\", round(results_2[\"score\"], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"pyarrow<17\" \"datasets>=2.14,<3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow: 16.1.0\n",
      "datasets: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa, datasets\n",
    "print(\"pyarrow:\", pa.__version__)\n",
    "print(\"datasets:\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LqXNw9_BFQe"
   },
   "source": [
    "The dataset object itself is [datasetdict](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "379sQa4Ix5Uf",
    "outputId": "8d3baded-0cec-4bbf-fdc4-cdcc7944ee9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 379480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['src_lang', 'tgt_lang', 'src', 'tgt'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkiu4ITXx5Ug",
    "outputId": "1eb6d334-c138-4ddb-c625-9a7f817fdb2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_lang': 'eng_Latn',\n",
       " 'tgt_lang': 'mar_Deva',\n",
       " 'src': 'Could you use a less expensive alternative or a generic or house brand item',\n",
       " 'tgt': 'तुम्ही कमी खर्चाचा पर्याय किंवा सामान्य किंवा घरगुती छापाचा नग वापरू शकता का?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6hsuFL1Bwa6"
   },
   "source": [
    "To get a sense of how the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "1OE_CXoxx5Uh",
    "outputId": "c25cfc10-df85-49b7-bd06-524ce871ba9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_lang</th>\n",
       "      <th>tgt_lang</th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>However, the next three seasons after that, the Bulls won the championship.</td>\n",
       "      <td>मात्र, त्यानंतरच्या तीन हंगामांमध्ये बुल्सने विजेतेपद पटकावले.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>Some of the factors are believed to come from genetics.</td>\n",
       "      <td>काही घटक अनुवांशिकतेतून येतात असे मानले जाते.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>She reveals that her family is moving abroad to escape the scandal and that she may never see him again</td>\n",
       "      <td>तिने उघड केले की तिचे कुटुंब या घोटाळ्यापासून वाचण्यासाठी परदेशात जात आहे आणि ती त्याला पुन्हा कधीही पाहू शकणार नाही.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>It summarizes the Spoken-Tutorial project.</td>\n",
       "      <td>ज्यामध्ये तुम्हाला प्रॉजेक्टचा सारांश मिळेल.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eng_Latn</td>\n",
       "      <td>mar_Deva</td>\n",
       "      <td>Examples provided in this article will use TestNG syntax and annotations.</td>\n",
       "      <td>या लेखात प्रदान केलेली उदाहरणे टेस्टएनजी सिंटॅक्स आणि भाष्ये वापरतील.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6OLK8GQB8lK"
   },
   "source": [
    "You can call its compute method with your predictions and labels, which need to be list of decoded strings (list of list for the labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G24hShAwx5Uj",
    "outputId": "163c8294-85e6-4fc5-ff66-3f6cc62affe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 100.00000000000004\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load metric\n",
    "metric = load(\"sacrebleu\")\n",
    "\n",
    "preds = [\"hello there general kenobi\", \"you are a bold one\"]\n",
    "refs  = [[\"hello there general kenobi\"], [\"you are a bold one\"]]\n",
    "result = metric.compute(predictions=preds, references=refs)\n",
    "print(\"BLEU =\", result[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6HmkA7uCWRa"
   },
   "source": [
    "# Preprocessing the data\n",
    "\n",
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "we download the vocabulary used when pretraining this specific checkpoint.\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell.\n",
    "\n",
    "If you downloaded the model manually, you can provide model present directory instead of model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "db4d60783a2b4cc9ac647f53524e9201",
      "23e6dd2b833d44aabc5b78a83d7ef136",
      "8aff67fa78c640bf83598dd7fccc5b97",
      "97ed312717014144af97d250fcd74d9d",
      "7b69f1226cf5457aa164146a890a2c5e",
      "d5ce787580584a40883fdac45df7917a",
      "1962a04347894250993d2eb3d2946e5c",
      "d5e1f379a1084c6e978a633698b4b02a",
      "b689909adc244204af714b9a19e158c0",
      "8c32a5f5cb724610a4ea8a5b42be5b28",
      "44db6a1a111148ceaa804f7302b0629b",
      "7e8253e0de7f426c9f9af4b15775c660",
      "adfbc16f0b6144b6ab06544ed060f8fe",
      "1425686f748e45b9921c501d0e5ce904",
      "632fb54ffc9b44b98d082d097c17d7de"
     ]
    },
    "id": "agDlgrOix5Uj",
    "outputId": "d52e9acb-10d0-464f-c9f1-1947b7a7b0e6"
   },
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "\n",
    "    # Quantization setup\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    # Move to device and optionally convert to half precision\n",
    "    if qconfig is None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    # Make sure model is in training mode for fine-tuning\n",
    "    model.train()\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n",
      "- tokenization_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n",
      "- configuration_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-dist-200M:\n",
      "- modeling_indictrans.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbih6yhKG9JB"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9XTBM27x5Uk",
    "outputId": "2bd42d79-8437-41b1-e1b4-80723a18f614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentences: ['eng_Latn mar_Deva Hello , this is one sentence !', 'eng_Latn mar_Deva This is another sentence .']\n"
     ]
    }
   ],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "sentences = [\"Hello, this is one sentence!\", \"This is another sentence.\"]\n",
    "\n",
    "# Preprocess sentences\n",
    "preprocessed = ip.preprocess_batch(sentences, src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\")\n",
    "print(\"Preprocessed sentences:\", preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   4,   29, 7951,    7,   36,   13,   75, 4534,   74,    2],\n",
      "        [   1,    1,    4,   29,   67,   13,  309, 4534,    5,    2]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the batch and generate input encodings\n",
    "inputs = en_indic_tokenizer(\n",
    "    preprocessed,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf2HfvKdHCeS"
   },
   "source": [
    "To prepare the targets for our model, we need to tokenize them inside the as_target_tokenizer context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvYg6BXJx5Ul",
    "outputId": "06d8f836-7a16-4fdc-a23e-9d0973135a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed sentences: ['eng_Latn mar_Deva Hello , this is one sentence !', 'eng_Latn mar_Deva This is another sentence .']\n",
      "{'input_ids': tensor([[67156, 59836,  3233, 11007,  7130, 60726, 59836,  2134,  5172, 43144,\n",
      "         36485, 64643,     5, 45158, 14444, 46728, 78577,    40,     2],\n",
      "        [67156, 59836,  3233, 11007,  7130, 60726, 59836,  2134,  5172, 43144,\n",
      "         56649, 14444, 79991, 78577,     4,     2,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Hello, this is one sentence!\", \"This is another sentence.\"]\n",
    "\n",
    "# Preprocess sentences\n",
    "preprocessed = ip.preprocess_batch(sentences, src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\")\n",
    "print(\"Preprocessed sentences:\", preprocessed)\n",
    "\n",
    "with en_indic_tokenizer.as_target_tokenizer():\n",
    "    print(en_indic_tokenizer(preprocessed,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "def decode_tokenized_batch(tokenizer, batch: Dict[str, torch.Tensor], ip=None, target_lang=None):\n",
    "    \"\"\"\n",
    "    Decode a batch of token IDs (input_ids or labels) into readable text.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        batch: dictionary containing 'input_ids' or 'labels' tensors\n",
    "        ip: IndicProcessor instance (optional, for postprocessing)\n",
    "        target_lang: target language code, required if ip is used\n",
    "    \n",
    "    Returns:\n",
    "        List of decoded strings\n",
    "    \"\"\"\n",
    "    # Choose which field to decode\n",
    "    if \"labels\" in batch:\n",
    "        tokens_to_decode = batch[\"labels\"]\n",
    "    elif \"input_ids\" in batch:\n",
    "        tokens_to_decode = batch[\"input_ids\"]\n",
    "    else:\n",
    "        raise ValueError(\"Batch must contain 'labels' or 'input_ids'.\")\n",
    "\n",
    "    # Convert to list of lists (batch of sequences)\n",
    "    if isinstance(tokens_to_decode, torch.Tensor):\n",
    "        tokens_to_decode = tokens_to_decode.tolist()\n",
    "\n",
    "    # Decode using tokenizer\n",
    "    decoded_texts = tokenizer.batch_decode(\n",
    "        tokens_to_decode,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    print(decoded_texts)\n",
    "    \n",
    "    # Postprocess with IndicProcessor if provided\n",
    "    if ip is not None and target_lang is not None:\n",
    "        decoded_texts = ip.postprocess_batch(decoded_texts, lang=target_lang)\n",
    "\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_lang': ['eng_Latn', 'eng_Latn'],\n",
       " 'tgt_lang': ['mar_Deva', 'mar_Deva'],\n",
       " 'src': ['Could you use a less expensive alternative or a generic or house brand item',\n",
       "  'If you do not round this number, you will get a different final result than in the example.'],\n",
       " 'tgt': ['तुम्ही कमी खर्चाचा पर्याय किंवा सामान्य किंवा घरगुती छापाचा नग वापरू शकता का?',\n",
       "  'जर तुम्ही हा आकडा पूर्णांक केला नाही तर तुम्हाला उदाहरणापेक्षा एखादा वेगळा अंतिम निकाल मिळेल.']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define preprocessing ----\n",
    "def preprocess_function(examples, tokenizer, ip):\n",
    "    # Step 1: Preprocess both source and target\n",
    "    src_texts = ip.preprocess_batch(\n",
    "        examples[\"src\"], src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\"\n",
    "    )\n",
    "    tgt_texts = ip.preprocess_batch(\n",
    "        examples[\"tgt\"], src_lang=\"mar_Deva\", tgt_lang=\"eng_Latn\"\n",
    "    )\n",
    "\n",
    "    # Step 2: Tokenize (new Transformers ≥v4.30 style)\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        text_target=tgt_texts,\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        return_tensors=None,        # return lists for Dataset.map\n",
    "    )\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mar_Deva eng_Latn तुम्ही कमी खर्चाचा पर्याय किंवा सामान्य किंवा घरगुती छापाचा नग वापरू शकता का?', 'mar_Deva eng_Latn जर तुम्ही हा आकडा पूर्णांक केला नाही तर तुम्हाला उदाहरणापेक्षा एखादा वेगळा अंतिम निकाल मिळेल.']\n",
      "['mar_Deva eng_Latn तुम्ही कमी खर्चाचा पर्याय किंवा सामान्य किंवा घरगुती छापाचा नग वापरू शकता का?', 'mar_Deva eng_Latn जर तुम्ही हा आकडा पूर्णांक केला नाही तर तुम्हाला उदाहरणापेक्षा एखादा वेगळा अंतिम निकाल मिळेल.']\n"
     ]
    }
   ],
   "source": [
    "example_batch = preprocess_function(raw_datasets['train'][:2], en_indic_tokenizer, ip)\n",
    "print(decode_tokenized_batch(en_indic_tokenizer, example_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ZDByENHTtO"
   },
   "source": [
    "To apply this function on all the pairs of sentences in our dataset, we just use the map method of our dataset object we created earlier. This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "f56b5b8abcfc415fbf46ca0f2fe619d0",
      "ef09f1189d75431591bebf33d288dacc",
      "12e50a27f9f24575bd86305f3b609095",
      "24cc9a4f2e4a411296dd6822329e5815",
      "f83d5ec459fe4dfd9ebf46fcca9895ee",
      "382f2bc5fc944482861293a496fb03ed",
      "330c9a9e97c9436f9d29bb12c586253a",
      "af955c59afc54adc8f1072d371b688bc",
      "5b4edc877e524610bb9ecc8103763250",
      "d3190da3507f4ab4be03027179566508",
      "7dac8e5e55ce4e638942c4299b377e3a",
      "41467eb21e744e66a47790e274defb66",
      "c0e50be8469a429fb8852fe3eb3a2ede"
     ]
    },
    "id": "uZlsJFZnx5Uo",
    "outputId": "e39c565b-e6cb-489b-a5ca-a565624b89cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f83d829db40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|█████████████████| 379480/379480 [04:31<00:00, 1397.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████| 47435/47435 [00:33<00:00, 1410.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████| 47435/47435 [00:34<00:00, 1381.62 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 379480\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 47435\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 47435\n",
      "    })\n",
      "})\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "batch_size = 32\n",
    "tokenized_datasets_dict = {}\n",
    "\n",
    "# Iterate over splits\n",
    "for split in raw_datasets.keys():\n",
    "    print(f\"Processing split: {split}\")\n",
    "\n",
    "    # Tokenize this split using your preprocessing function\n",
    "    tokenized_split = raw_datasets[split].map(\n",
    "        lambda x: preprocess_function(x, tokenizer=en_indic_tokenizer, ip=ip),\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=raw_datasets[split].column_names  # optional\n",
    "    )\n",
    "\n",
    "    tokenized_datasets_dict[split] = tokenized_split\n",
    "\n",
    "# Rebuild DatasetDict\n",
    "tokenized_datasets = DatasetDict(tokenized_datasets_dict)\n",
    "\n",
    "# Optional: inspect\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 379480\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 47435\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 47435\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|           | 0/380 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   4%| | 17/380 [00:00<00:02, 162.49ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:   9%| | 34/380 [00:00<00:02, 161.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  14%|▏| 52/380 [00:00<00:01, 169.21ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  19%|▏| 71/380 [00:00<00:01, 173.69ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  23%|▏| 89/380 [00:00<00:01, 175.80ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  28%|▎| 107/380 [00:00<00:01, 170.62ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  33%|▎| 125/380 [00:00<00:01, 167.54ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  37%|▎| 142/380 [00:00<00:01, 165.70ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  42%|▍| 159/380 [00:00<00:01, 164.12ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  46%|▍| 176/380 [00:01<00:01, 163.24ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  51%|▌| 193/380 [00:01<00:01, 163.16ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  55%|▌| 210/380 [00:01<00:01, 159.48ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  60%|▌| 229/380 [00:01<00:00, 166.21ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  65%|▋| 248/380 [00:01<00:00, 171.74ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  70%|▋| 267/380 [00:01<00:00, 174.43ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  75%|▊| 286/380 [00:01<00:00, 176.60ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  81%|▊| 306/380 [00:01<00:00, 181.43ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  86%|▊| 325/380 [00:01<00:00, 182.80ba/s\u001b[A\n",
      "Creating parquet from Arrow format:  91%|▉| 344/380 [00:01<00:00, 184.38ba/s\u001b[A\n",
      "Creating parquet from Arrow format: 100%|█| 380/380 [00:02<00:00, 173.82ba/s\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:10<00:00, 10.89s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/48 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  33%|▋ | 16/48 [00:00<00:00, 155.63ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  67%|█▎| 32/48 [00:00<00:00, 153.18ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██| 48/48 [00:00<00:00, 150.92ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "Uploading the dataset shards:   0%|                   | 0/1 [00:00<?, ?it/s]\n",
      "Creating parquet from Arrow format:   0%|            | 0/48 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  35%|▋ | 17/48 [00:00<00:00, 162.42ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██| 48/48 [00:00<00:00, 163.78ba/s]\u001b[A\n",
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n",
      "Uploading the dataset shards: 100%|███████████| 1/1 [00:03<00:00,  3.40s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/thenlpresearcher/iitb_en_indic_marathi_punct_variants_tokenized/commit/c6dd175a36f88435dc55e4fd8bd54d1cf9368524', commit_message='Upload dataset', commit_description='', oid='c6dd175a36f88435dc55e4fd8bd54d1cf9368524', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/thenlpresearcher/iitb_en_indic_marathi_punct_variants_tokenized', endpoint='https://huggingface.co', repo_type='dataset', repo_id='thenlpresearcher/iitb_en_indic_marathi_punct_variants_tokenized'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.push_to_hub(\n",
    "    \"thenlpresearcher/iitb_en_indic_marathi_punct_variants_tokenized\", \n",
    "    private=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████████████| 589/589 [00:00<00:00, 2.62kB/s]\n",
      "Downloading data: 100%|████████████████| 51.2M/51.2M [00:02<00:00, 19.3MB/s]\n",
      "Downloading data: 100%|████████████████| 6.40M/6.40M [00:00<00:00, 8.13MB/s]\n",
      "Downloading data: 100%|████████████████| 6.40M/6.40M [00:00<00:00, 8.30MB/s]\n",
      "Generating train split: 100%|█| 379480/379480 [00:00<00:00, 486342.03 exampl\n",
      "Generating validation split: 100%|█| 47435/47435 [00:00<00:00, 542059.05 exa\n",
      "Generating test split: 100%|█| 47435/47435 [00:00<00:00, 536073.77 examples/\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tokenized_datasets = load_dataset(\"thenlpresearcher/iitb_en_indic_marathi_punct_variants_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 379480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4kA3jpsHcFE"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooZLnaC1HhBq"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the AutoModelForSeq2SeqLM class. Like with the tokenizer, the from_pretrained method will download and cache the model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPagNnmWHoQW"
   },
   "source": [
    "To instantiate a Seq2SeqTrainer, we will need to define three more things. The most important is the [Seq2SeqTrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using `include_inputs_for_metrics` is deprecated and will be removed in version 5 of 🤗 Transformers. Please use `include_for_metrics` list argument instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "model_checkpoint = en_indic_ckpt_dir\n",
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "source_lang = 'eng_Ltn'\n",
    "target_lang = 'mar_Deva'\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"{model_name}-iitb-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=50,                 # evaluate every 2000 training steps\n",
    "    save_steps=50,                 # save checkpoint every 2000 steps\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,                     # if GPU supports\n",
    "    logging_steps=100,\n",
    "    seed=42,\n",
    "    include_inputs_for_metrics=True,\n",
    "    \n",
    "    # --- Add these for pushing to Hub ---\n",
    "    push_to_hub=True,                                 # enables upload\n",
    "    hub_model_id=f\"thenlpresearcher/iitb_punct_robust_finetuned_{source_lang}_to_{target_lang}\",  # your repo name\n",
    "    hub_private_repo=False,                            # makes it private\n",
    "    hub_strategy=\"end\",                        # uploads at each save\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk2p1KEwx5Ur",
    "outputId": "28118e90-17a6-460a-8694-3b50cc6660c5"
   },
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# args = Seq2SeqTrainingArguments(\n",
    "#     f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "#     eval_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     weight_decay=0.01,\n",
    "#     save_total_limit=3,\n",
    "#     num_train_epochs=1,\n",
    "#     predict_with_generate=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9UUwvXCH8Op"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the batch_size defined at the top of the cell and customize the weight decay. Since the Seq2SeqTrainer will save the model regularly and our dataset is quite large, we tell it to make three saves maximum. Lastly, we use the predict_with_generate option (to properly generate summaries) and activate mixed precision training (to go a bit faster).\n",
    "\n",
    "Model will save under **{model_name}-finetuned-{source_lang}-to-{target_lang}** directory\n",
    "\n",
    "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qV9wfuvZx5Us"
   },
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForSeq2Seq(en_indic_tokenizer, model=en_indic_model,\n",
    "#     padding=True,          # default: pad dynamically per batch\n",
    "#     return_tensors=\"pt\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw7by60eH_p5"
   },
   "source": [
    "The last thing to define for our Seq2SeqTrainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_indictrans_text(preds, labels, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    \"\"\"\n",
    "    Remove language prefix from predictions and labels for evaluation.\n",
    "    For decoder outputs, the prefix is TARGET_LANG SOURCE_LANG.\n",
    "    \"\"\"\n",
    "    prefix = f\"{target_lang} {source_lang}\"\n",
    "\n",
    "    def remove_prefix(text):\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    # Strip prefixes\n",
    "    preds = [remove_prefix(p) for p in preds]\n",
    "    labels = [[remove_prefix(l)] for l in labels]  # BLEU expects list of lists\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def compute_metrics_indictrans(eval_preds, tokenizer, metric, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "#     \"\"\"\n",
    "#     Decode model predictions and labels, remove prefixes, compute BLEU.\n",
    "#     \"\"\"\n",
    "#     preds, labels = eval_preds\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "\n",
    "#     # Decode predictions\n",
    "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Replace -100 with pad_token_id for labels and decode\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "#     # Remove prefixes or do custom postprocessing\n",
    "#     decoded_preds, decoded_labels = postprocess_indictrans_text(\n",
    "#         decoded_preds, decoded_labels, source_lang=source_lang, target_lang=target_lang\n",
    "#     )\n",
    "\n",
    "#     # BLEU expects references as list of lists\n",
    "#     decoded_labels = [[lbl] for lbl in decoded_labels]\n",
    "# #     print(decoded_labels)\n",
    "\n",
    "#     # Compute BLEU\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "#     # Average generated length (non-padding tokens)\n",
    "#     prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "#     result[\"gen_len\"] = float(np.mean(prediction_lens))\n",
    "\n",
    "#     # Round metrics for readability\n",
    "#     return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4216/3091177656.py:79: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference sentences: ['mar_Deva eng_Latn आता सुरुवात करू. मी आधीच माझ्या सिस्टिमवर Koha इन्स्टॉल केले आहे.', 'mar_Deva eng_Latn फक्त दोन खरोखरच्या दुहेरी कॉन्सर्टोपैकी एक तो आहे, जे दोन्ही मोत्झार्तने लिहिले.', 'mar_Deva eng_Latn थोडक्यात,', 'mar_Deva eng_Latn ह्या पुस्तकात आपण ह्या संकल्पनांचा वापर केला नाही पाहिजे आणि ज्यांना पुढे वाचण्याची इच्छा आहे, त्यांच्या उपयोगासाठी फक्त हा उल्लेख केला पाहिजे.', 'mar_Deva eng_Latn ती युक्रेनियन नॅशनल फिलहारमोनिकची एकटी वादक होती.', 'mar_Deva eng_Latn तो Alex किंवा phpacademy कडून आला असे म्हणू शकतो.', 'mar_Deva eng_Latn 2015 मध्ये त्याने वॉर्सा येथे दुसरे नाव्ह्याचे दुकान आणि सोपोटमध्ये एक बार उघडला.', 'mar_Deva eng_Latn उन्हाळी परीक्षांसाठी पूर्वविमोचन साहित्य दरवर्षी 31 मार्चपासून उपलब्ध असते ( आणि ते जानेवारीच्या परीक्षांसाठी 1 नोव्हेंबरला देखील उपलब्ध असेल ).', 'mar_Deva eng_Latn ह्यूजचा न्यूयॉर्क शहरात नैसर्गिक कारणांमुळे मृत्यू झाला.', 'mar_Deva eng_Latn विंडो शॉपिंगच्या उद्देशाप्रमाणे खिडकीत प्रदर्शित केलेल्या वस्तू बघताना तुम्हाला कदाचित काही संक्षिप्त नोंदी कराव्याशा वाटतील.']\n",
      "Generated translations: ['चला सुरुवात करूया. मी आधीच माझ्या प्रणालीवर कोहा स्थापित केला आहे.', 'मोझार्टने लिहिलेल्या केवळ दोन खऱ्या दुहेरी कॉन्सर्टोंपैकी हा एक आहे.', 'आपण जे शिकलो त्याचा सारांश घेऊया.', 'आम्ही या मजकुरात या संज्ञा वापरणार नाही आणि याचा उल्लेख केवळ त्या वाचकांच्या फायद्यासाठी करणार आहोत ज्यांना पुढील वाचनाचा अभ्यासक्रम सुरू ठेवायचा आहे.', 'ती युक्रेनियन नॅशनल फिलहार्मोनिकची एकल गायिका होती.', 'तुम्ही हे अलेक्सकडून किंवा पीएच. पी. ए. ए. डी. एम. आय. कडून असे ठेवू शकता.', '2015 मध्ये त्याने वॉर्सा येथे दुसरे नाईचे दुकान आणि सोपोट येथे बार उघडले.', 'उन्हाळी परीक्षांसाठी दरवर्षी 31 मार्चपासून पूर्व - प्रकाशन साहित्य उपलब्ध आहे, तसेच जानेवारीच्या परीक्षांसाठी 1 नोव्हेंबरला एक असेल.', 'ह्यूजेस यांचा न्यूयॉर्क शहरात नैसर्गिक कारणांमुळे मृत्यू झाला.', 'तुमच्या विंडो शॉपिंगच्या उद्दिष्टांनुसार, तुम्ही विंडो डिस्प्ले पाहत असताना तुम्हाला काही संक्षिप्त नोंदी घ्यायच्या असतील.']\n",
      "Results: {'bleu': 17.2681, 'gen_len': 21.4}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_metric\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load model & tokenizer\n",
    "# -----------------------------\n",
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def initialize_model_and_tokenizer(ckpt_dir, quantization=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir, trust_remote_code=True\n",
    "    )\n",
    "    if quantization == \"fp16\":\n",
    "        model = model.half()\n",
    "    model.to(DEVICE)\n",
    "    return tokenizer, model\n",
    "\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir)\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Prepare small validation batch\n",
    "# -----------------------------\n",
    "# tokenized_datasets must already exist (from previous preprocessing)\n",
    "small_val_dataset = tokenized_datasets[\"validation\"].select(range(10))\n",
    "\n",
    "input_ids = torch.tensor(small_val_dataset[\"input_ids\"]).to(DEVICE)\n",
    "attention_mask = torch.tensor(small_val_dataset[\"attention_mask\"]).to(DEVICE)\n",
    "labels = torch.tensor(small_val_dataset[\"labels\"]).to(DEVICE)\n",
    "\n",
    "# Decode reference sentences for metric\n",
    "reference_sentences = [\n",
    "    en_indic_tokenizer.decode(\n",
    "        np.where(l.cpu().numpy() != -100, l.cpu().numpy(), en_indic_tokenizer.pad_token_id),\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    for l in labels\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Generate translations\n",
    "# -----------------------------\n",
    "with torch.no_grad():\n",
    "    generated_tokens = en_indic_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=128,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=en_indic_model.config.decoder_start_token_id\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Define postprocessing\n",
    "# -----------------------------\n",
    "def postprocess_indictrans_text(preds, labels, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    \"\"\"\n",
    "    Remove language prefix from predictions and labels for evaluation.\n",
    "    BLEU expects list of references per prediction.\n",
    "    \"\"\"\n",
    "    prefix = f\"{target_lang} {source_lang}\"\n",
    "\n",
    "    def remove_prefix(text):\n",
    "        if text.startswith(prefix):\n",
    "            return text[len(prefix):].strip()\n",
    "        return text.strip()\n",
    "\n",
    "    preds = [remove_prefix(p) for p in preds]\n",
    "    labels = [[remove_prefix(l)] for l in labels]  # BLEU expects list of lists\n",
    "    return preds, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Compute metrics\n",
    "# -----------------------------\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics_indictrans(eval_preds, tokenizer, metric, source_lang=\"eng_Latn\", target_lang=\"mar_Deva\"):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Convert to numpy if tensors\n",
    "    if isinstance(preds, torch.Tensor):\n",
    "        preds = preds.cpu().numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "\n",
    "    # Decode predictions\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Replace -100 with pad_token_id for labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Optional postprocessing\n",
    "    decoded_preds, decoded_labels = postprocess_indictrans_text(decoded_preds, decoded_labels, source_lang, target_lang)\n",
    "\n",
    "    # Compute BLEU\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    # Average generated length (non-padding tokens)\n",
    "    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "    result[\"gen_len\"] = float(np.mean(prediction_lens))\n",
    "\n",
    "    # Round metrics for readability\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# -----------------------------\n",
    "# 6️⃣ Evaluate on sample batch\n",
    "# -----------------------------\n",
    "results = compute_metrics_indictrans(\n",
    "    (generated_tokens, labels),\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "print(\"Reference sentences:\", reference_sentences)\n",
    "print(\"Generated translations:\", en_indic_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 100.0, 'gen_len': 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_preds = [[60726, 59836, 2134, 5172, 4, 2]]  # example prediction ids\n",
    "sample_labels = [[60726, 59836, 2134, 5172, 4, -100]]  # example label ids\n",
    "compute_metrics_indictrans((sample_preds, sample_labels), tokenizer=en_indic_tokenizer, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 379480\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 47435\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mar_Deva eng_Latn आता सुरुवात करू. मी आधीच माझ्या सिस्टिमवर Koha इन्स्टॉल केले आहे.', 'mar_Deva eng_Latn फक्त दोन खरोखरच्या दुहेरी कॉन्सर्टोपैकी एक तो आहे, जे दोन्ही मोत्झार्तने लिहिले.', 'mar_Deva eng_Latn थोडक्यात,', 'mar_Deva eng_Latn ह्या पुस्तकात आपण ह्या संकल्पनांचा वापर केला नाही पाहिजे आणि ज्यांना पुढे वाचण्याची इच्छा आहे, त्यांच्या उपयोगासाठी फक्त हा उल्लेख केला पाहिजे.', 'mar_Deva eng_Latn ती युक्रेनियन नॅशनल फिलहारमोनिकची एकटी वादक होती.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mar_Deva eng_Latn आता सुरुवात करू. मी आधीच माझ्या सिस्टिमवर Koha इन्स्टॉल केले आहे.',\n",
       " 'mar_Deva eng_Latn फक्त दोन खरोखरच्या दुहेरी कॉन्सर्टोपैकी एक तो आहे, जे दोन्ही मोत्झार्तने लिहिले.',\n",
       " 'mar_Deva eng_Latn थोडक्यात,',\n",
       " 'mar_Deva eng_Latn ह्या पुस्तकात आपण ह्या संकल्पनांचा वापर केला नाही पाहिजे आणि ज्यांना पुढे वाचण्याची इच्छा आहे, त्यांच्या उपयोगासाठी फक्त हा उल्लेख केला पाहिजे.',\n",
       " 'mar_Deva eng_Latn ती युक्रेनियन नॅशनल फिलहारमोनिकची एकटी वादक होती.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: pick 5 validation examples\n",
    "sample_batch = tokenized_datasets[\"validation\"][:5]\n",
    "# print(sample_batch)\n",
    "decode_tokenized_batch(en_indic_tokenizer, sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mock Predictions vs References:\n",
      "\n",
      "Prediction: mar_Deva eng_Latn आता सुरुवात करू. मी आधीच माझ्या सिस्टिमवर Koha इन्स्टॉल केले आहे.\n",
      "Reference:  mar_Deva eng_Latn आता सुरुवात करू. मी आधीच माझ्या सिस्टिमवर Koha इन्स्टॉल केले आहे.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn फक्त दोन खरोखरच्या दुहेरी कॉन्सर्टोपैकी एक तो आहे, जे दोन्ही मोत्झार्तने लिहिले.\n",
      "Reference:  mar_Deva eng_Latn फक्त दोन खरोखरच्या दुहेरी कॉन्सर्टोपैकी एक तो आहे, जे दोन्ही मोत्झार्तने लिहिले.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn थोडक्यात,\n",
      "Reference:  mar_Deva eng_Latn थोडक्यात,\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn ह्या पुस्तकात आपण ह्या संकल्पनांचा वापर केला नाही पाहिजे आणि ज्यांना पुढे वाचण्याची इच्छा आहे, त्यांच्या उपयोगासाठी फक्त हा उल्लेख केला पाहिजे.\n",
      "Reference:  mar_Deva eng_Latn ह्या पुस्तकात आपण ह्या संकल्पनांचा वापर केला नाही पाहिजे आणि ज्यांना पुढे वाचण्याची इच्छा आहे, त्यांच्या उपयोगासाठी फक्त हा उल्लेख केला पाहिजे.\n",
      "-----\n",
      "Prediction: mar_Deva eng_Latn ती युक्रेनियन नॅशनल फिलहारमोनिकची एकटी वादक होती.\n",
      "Reference:  mar_Deva eng_Latn ती युक्रेनियन नॅशनल फिलहारमोनिकची एकटी वादक होती.\n",
      "-----\n",
      "Metrics: {'bleu': 100.0, 'gen_len': 28.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pick 5 examples from validation\n",
    "sample_batch = tokenized_datasets[\"validation\"].select(range(5))\n",
    "\n",
    "# Use labels as \"mock predictions\" (or you can modify them slightly)\n",
    "mock_preds = np.array(sample_batch[\"labels\"])\n",
    "\n",
    "# Prepare eval_preds tuple\n",
    "# Normally: (predictions, labels)\n",
    "eval_preds = (mock_preds, np.array(sample_batch[\"labels\"]))\n",
    "\n",
    "# Compute metrics\n",
    "results = compute_metrics_indictrans(\n",
    "    eval_preds,\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    metric=metric,\n",
    "    source_lang=\"eng_Latn\",\n",
    "    target_lang=\"mar_Deva\"\n",
    ")\n",
    "\n",
    "# Decode predictions and labels for inspection\n",
    "decoded_preds = en_indic_tokenizer.batch_decode(\n",
    "    mock_preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "decoded_labels = en_indic_tokenizer.batch_decode(\n",
    "    np.where(np.array(sample_batch[\"labels\"]) != -100,\n",
    "             np.array(sample_batch[\"labels\"]),\n",
    "             en_indic_tokenizer.pad_token_id),\n",
    "    skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Print for visual check\n",
    "print(\"Sample Mock Predictions vs References:\\n\")\n",
    "for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "    print(f\"Prediction: {pred}\")\n",
    "    print(f\"Reference:  {ref}\")\n",
    "    print(\"-----\")\n",
    "\n",
    "print(\"Metrics:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XK7px-IDkC"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the Seq2SeqTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_indictrans(\n",
    "    eval_preds, \n",
    "    tokenizer, \n",
    "    metric_bleu,\n",
    "    metric_chrf=None,\n",
    "    metric_comet=None,\n",
    "    metric_bleurt=None,\n",
    "    source_lang=\"eng_Latn\",\n",
    "    target_lang=\"mar_Deva\"\n",
    "):\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    inputs = eval_preds.inputs    # <-- needed for COMET\n",
    "\n",
    "    # Convert \n",
    "    preds = preds.cpu().numpy() if isinstance(preds, torch.Tensor) else preds\n",
    "    labels = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "    inputs = inputs.cpu().numpy() if isinstance(inputs, torch.Tensor) else inputs\n",
    "\n",
    "    # Decode all three\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(np.where(labels!=-100, labels, tokenizer.pad_token_id),\n",
    "                                            skip_special_tokens=True)\n",
    "    decoded_sources = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "\n",
    "    # IndicTrans cleanup\n",
    "    decoded_preds, decoded_labels = postprocess_indictrans_text(\n",
    "        decoded_preds, decoded_labels, source_lang, target_lang\n",
    "    )\n",
    "\n",
    "    # BLEU\n",
    "    bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels)[\"score\"]\n",
    "\n",
    "    # ChrF++\n",
    "    chrfpp = None\n",
    "    if metric_chrf is not None:\n",
    "        chrfpp = metric_chrf.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            word_order=2\n",
    "        )[\"score\"]\n",
    "    \n",
    "    flat_refs = [r[0] if isinstance(r, list) else r for r in decoded_labels]\n",
    "    # COMET\n",
    "    comet = None\n",
    "    if metric_comet is not None:\n",
    "        comet = metric_comet.compute(\n",
    "            sources=decoded_sources,\n",
    "            predictions=decoded_preds,\n",
    "            references=flat_refs\n",
    "        )[\"mean_score\"]\n",
    "\n",
    "    # BLEURT\n",
    "    bleurt = None\n",
    "    if metric_bleurt is not None:\n",
    "        bleurt_scores = metric_bleurt.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels\n",
    "        )[\"scores\"]\n",
    "        bleurt = float(np.mean(bleurt_scores))\n",
    "\n",
    "    # Length\n",
    "    prediction_lens = [np.count_nonzero(p != tokenizer.pad_token_id) for p in preds]\n",
    "    gen_len = float(np.mean(prediction_lens))\n",
    "\n",
    "    return {\n",
    "        \"bleu\": round(bleu, 4),\n",
    "        \"chrfpp\": round(chrfpp, 4) if chrfpp else None,\n",
    "        \"comet\": round(comet, 4) if comet else None,\n",
    "        \"bleurt\": round(bleurt, 4) if bleurt else None,\n",
    "        \"gen_len\": round(gen_len, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting git+https://github.com/google-research/bleurt.git\n",
      "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-jx2lf9km\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-jx2lf9km\n",
      "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (2.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.24.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (1.13.1)\n",
      "Collecting tensorflow (from BLEURT==0.0.2)\n",
      "  Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting tf-slim>=1.1 (from BLEURT==0.0.2)\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from BLEURT==0.0.2) (0.2.1)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->BLEURT==0.0.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->BLEURT==0.0.2) (1.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (0.5.4)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages/opt_einsum-3.3.0-py3.10.egg (from tensorflow->BLEURT==0.0.2) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (24.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (68.2.2)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (4.12.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->BLEURT==0.0.2) (1.62.1)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting numpy (from BLEURT==0.0.2)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow->BLEURT==0.0.2)\n",
      "  Downloading ml_dtypes-0.5.4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.20.0->tensorflow->BLEURT==0.0.2) (3.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.20.0->tensorflow->BLEURT==0.0.2) (10.3.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard~=2.20.0->tensorflow->BLEURT==0.0.2) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.43.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.10.0->tensorflow->BLEURT==0.0.2) (13.7.1)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow->BLEURT==0.0.2)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.10.0->tensorflow->BLEURT==0.0.2) (0.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow->BLEURT==0.0.2) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.10.0->tensorflow->BLEURT==0.0.2) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.10.0->tensorflow->BLEURT==0.0.2) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n",
      "Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Downloading tensorflow-2.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.4/620.4 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.0.1-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (113 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Building wheels for collected packages: BLEURT\n",
      "  Building wheel for BLEURT (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for BLEURT: filename=bleurt-0.0.2-py3-none-any.whl size=16456842 sha256=a375a617932765f77b661db85e3813c7811e5627a0a55cf7b3ca8c151a508239\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dn0rgqiv/wheels/64/f4/2c/509a6c31b8ebde891a81029fd94f199b1b92f0e7cfc20d417a\n",
      "Successfully built BLEURT\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, tf-slim, termcolor, tensorboard-data-server, protobuf, numpy, google_pasta, tensorboard, ml_dtypes, h5py, keras, tensorflow, BLEURT\n",
      "\u001b[2K  Attempting uninstall: tensorboard-data-server━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [tf-slim]]\n",
      "\u001b[2K    Found existing installation: tensorboard-data-server 0.6.1\u001b[0m \u001b[32m 4/16\u001b[0m [tf-slim]\n",
      "\u001b[2K    Uninstalling tensorboard-data-server-0.6.1:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [tf-slim]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-data-server-0.6.1━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K  Attempting uninstall: protobufm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Found existing installation: protobuf 4.24.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K    Uninstalling protobuf-4.24.4:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K      Successfully uninstalled protobuf-4.24.4━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [tensorboard-data-server]\n",
      "\u001b[2K  Attempting uninstall: numpy\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [protobuf]ata-server]\n",
      "\u001b[2K    Found existing installation: numpy 1.24.4━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-1.24.4:╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.24.4[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: tensorboard0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: tensorboard 2.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling tensorboard-2.9.0:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.9.0[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [tensorboard]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [BLEURT]15/16\u001b[0m [BLEURT]low]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.4.0 requires protobuf<5,>=3.20, but you have protobuf 6.33.1 which is incompatible.\n",
      "cudf 24.4.0 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
      "unbabel-comet 2.2.7 requires protobuf<5.0.0,>=4.24.4, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed BLEURT-0.0.2 flatbuffers-25.9.23 google_pasta-0.2.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 ml_dtypes-0.5.4 namex-0.1.0 numpy-1.26.4 protobuf-6.33.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 tf-slim-1.1.0 wrapt-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|█████████████████████| 5/5 [00:00<00:00, 3855.06it/s]\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../root/.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "metric_bleu   = load(\"sacrebleu\")\n",
    "metric_chrf   = load(\"chrf\")\n",
    "metric_comet  = load(\"comet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.20.0\n",
      "Uninstalling tensorflow-2.20.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/import_pb_to_tensorboard\n",
      "    /usr/local/bin/saved_model_cli\n",
      "    /usr/local/bin/tensorboard\n",
      "    /usr/local/bin/tf_upgrade_v2\n",
      "    /usr/local/bin/tflite_convert\n",
      "    /usr/local/bin/toco\n",
      "    /usr/local/lib/python3.10/dist-packages/tensorflow-2.20.0.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/tensorflow/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Data collator for seq2seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=en_indic_tokenizer,\n",
    "    model=en_indic_model,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "# Seq2SeqTrainer setup (future-proof)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=en_indic_model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"].select(range(50)),\n",
    "    # tokenizer=en_indic_tokenizer,  # deprecated\n",
    "    processing_class=en_indic_tokenizer,  # use instead of tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_preds: compute_metrics_indictrans(\n",
    "        eval_preds,\n",
    "        tokenizer=en_indic_tokenizer,\n",
    "        metric_bleu=metric_bleu,\n",
    "        metric_chrf=metric_chrf,\n",
    "        metric_comet=metric_comet\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "c7zuF8rLx5Uy"
   },
   "outputs": [],
   "source": [
    "# trainer = Seq2SeqTrainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=en_indic_tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1TskjmQIGKd"
   },
   "source": [
    "We can now finetune our model by just calling the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_indic_model = en_indic_model.to(DEVICE)\n",
    "# if en_indic_model == \"cuda\":\n",
    "#     model.half()   # <-- this casts everything to FP16 manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 1139, 5590, 1711, 4, 125, 22815, 2353, 8980, 1496, 402, 62106, 16945, 7791, 5221, 975, 38, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 1597, 1764, 12003, 332, 45551, 4830, 3985, 1006, 53473, 11313, 24, 51, 38, 5, 169, 3166, 644, 183, 21560, 1394, 61, 27220, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 45924, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 3473, 50201, 520, 3473, 10526, 1860, 317, 2539, 1135, 135, 4424, 68, 40007, 4998, 4591, 5502, 2569, 38, 5, 2269, 232, 2545, 1597, 407, 2160, 1135, 4424, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [60726, 59836, 2134, 5172, 43144, 67156, 59836, 3233, 11007, 7130, 440, 11170, 16801, 3503, 27155, 6984, 3584, 10792, 226, 362, 38146, 76, 16532, 430, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "small_eval_dataset = tokenized_datasets[\"validation\"].select(range(50))\n",
    "print(small_eval_dataset[\"labels\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sonIRAfDx5Uz",
    "outputId": "8a0bc390-6900-424c-f92e-e3a841ead986"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='101' max='237180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   101/237180 00:18 < 12:20:02, 5.34 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrfpp</th>\n",
       "      <th>Comet</th>\n",
       "      <th>Bleurt</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.211616</td>\n",
       "      <td>6.519700</td>\n",
       "      <td>30.601400</td>\n",
       "      <td>0.535000</td>\n",
       "      <td>None</td>\n",
       "      <td>20.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>0.790579</td>\n",
       "      <td>7.039900</td>\n",
       "      <td>30.181100</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>None</td>\n",
       "      <td>20.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Trainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/bleurt\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Trainer is attempting to log a value of \"None\" of type <class 'NoneType'> for key \"eval/bleurt\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2197\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2204\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2623\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2623\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3103\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3100\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3211\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;241m=\u001b[39m best_checkpoint_dir\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3210\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_scaler(output_dir)\n\u001b[1;32m   3213\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3333\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3335\u001b[0m     )\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3337\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3343\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:670\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 670\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:901\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;66;03m# given that we copy things around anyway, we might use storage.cpu()\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# this means to that to get tensors serialized, you need to implement\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# .cpu() on the underlying Storage\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m storage\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 901\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    903\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py:137\u001b[0m, in \u001b[0;36m_StorageBase.cpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "checkpoint_dir = \"thenlpresearcher/iitb_punct_robust_finetuned_eng_Ltn_to_mar_Deva\"\n",
    "\n",
    "# Load model (custom class loaded via trust_remote_code=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Load tokenizer (custom tokenizer class loaded via trust_remote_code=True)\n",
    "tokenizer = en_indic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "BATCH_SIZE = 32 # Adjust for your GPU memory\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "# Collect source and reference sentences\n",
    "src_sentences = raw_datasets[\"test\"][\"src\"][:1000]\n",
    "ref_sentences = raw_datasets[\"test\"][\"tgt\"][:1000]\n",
    "\n",
    "\n",
    "prefix = f\"{tgt_lang} {src_lang}\"\n",
    "\n",
    "def remove_prefix(text):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        ip,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "p_t = []\n",
    "for t in all_translations:\n",
    "    p_t.append(remove_prefix(t))\n",
    "\n",
    "# Save translations alongside references\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"reference\": ref_sentences,\n",
    "    \"prediction\": p_t\n",
    "})\n",
    "results_df.to_csv(\"indictrans2_robust_test_predictions_test.csv\", index=False)\n",
    "\n",
    "print(\"Translations saved to indictrans2_test_predictions.csv\")\n",
    "\n",
    "# ---- Evaluation metrics ---- #\n",
    "bleu = load_metric(\"sacrebleu\")\n",
    "chrf = load_metric(\"chrf\")\n",
    "\n",
    "# Format for sacrebleu: references need to be list of lists\n",
    "bleu_score = bleu.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "chrf_score = chrf.compute(predictions=all_translations, references=[[r] for r in ref_sentences])\n",
    "\n",
    "print(f\"BLEU: {bleu_score['score']:.2f}\")\n",
    "print(f\"chrF++: {chrf_score['score']:.2f}\")\n",
    "\n",
    "# Optional: save metrics\n",
    "with open(\"indictrans2_test_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"BLEU: {bleu_score['score']:.2f}\\n\")\n",
    "    f.write(f\"chrF++: {chrf_score['score']:.2f}\\n\")\n",
    "\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अफगाणिस्तानमध्ये हिंदू फारच कमी असले तरी मालदीवमध्ये हिंदू नाहीत.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['prediction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122706\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))  # should match what you trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans with safety checks.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    # Safe access for decoder_start_token_id\n",
    "    decoder_start_token_id = getattr(model.config, \"decoder_start_token_id\", None)\n",
    "    pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "    eos_token_id = getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    if decoder_start_token_id is None:\n",
    "        print(\"[Warning] decoder_start_token_id is None. Using default generation behavior.\")\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch_preprocessed = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        if not isinstance(batch_preprocessed, list) or len(batch_preprocessed) == 0:\n",
    "            print(f\"[Warning] Preprocessed batch is empty at index {i}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Debug: print first 2 sentences after preprocessing\n",
    "#         print(f\"[Debug] Preprocessed batch sample: {batch_preprocessed[:2]}\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_preprocessed,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations with safety parameters\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_tokens = model.generate(\n",
    "                    **inputs,\n",
    "                    use_cache=True,\n",
    "                    min_length=5,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=5,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=decoder_start_token_id,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Generation failed for batch starting at index {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Debug: print first 2 decoded outputs\n",
    "#         print(f\"[Debug] Decoded sample: {decoded_texts[:2]}\")\n",
    "\n",
    "        # Postprocess translations\n",
    "        try:\n",
    "            postprocessed = ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "            translations += postprocessed\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Postprocessing failed for batch starting at index {i}: {e}\")\n",
    "            translations += decoded_texts  # fallback\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001c2cf8d21d4703bc190149a82d06ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c6c16b336c404fa84cba2ee42debde",
       "IPY_MODEL_31bbbf7e34a14335b628add663c71642",
       "IPY_MODEL_a23d83e9f0ea4ac8b57ec5f1614414dc"
      ],
      "layout": "IPY_MODEL_c8c29e7a13804b6c9e97c3ff425bd81d"
     }
    },
    "04672fbdc8754babbb7bb3bfb7bcfd2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "064aa5db0924410bb17646220304faea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c2955e8cff44b5ba998ca584e485e54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12e50a27f9f24575bd86305f3b609095": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_330c9a9e97c9436f9d29bb12c586253a",
      "placeholder": "​",
      "style": "IPY_MODEL_382f2bc5fc944482861293a496fb03ed",
      "value": "100%"
     }
    },
    "13c6c16b336c404fa84cba2ee42debde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7616cd08bb494b8f988b9e893bb8bb25",
      "placeholder": "​",
      "style": "IPY_MODEL_ae117dc6e1fb41eca964e34601c6aa6c",
      "value": "Downloading: 100%"
     }
    },
    "17ecb55f59dd411084d8692b80cc8cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1962a04347894250993d2eb3d2946e5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23e6dd2b833d44aabc5b78a83d7ef136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24cc9a4f2e4a411296dd6822329e5815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b4edc877e524610bb9ecc8103763250",
      "max": 611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af955c59afc54adc8f1072d371b688bc",
      "value": 611
     }
    },
    "31bbbf7e34a14335b628add663c71642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e574d69ffb426b87d3445d26e02926",
      "max": 300887193,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_064aa5db0924410bb17646220304faea",
      "value": 300887193
     }
    },
    "330c9a9e97c9436f9d29bb12c586253a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fd15427c7d4f4189c9ce38924107b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53ec358cc4784c25a50e17bd8764fd56",
       "IPY_MODEL_c916612f156b457f828410fdc85c8def",
       "IPY_MODEL_de1e4b0f94f74b4380de2cb4447352f1"
      ],
      "layout": "IPY_MODEL_04672fbdc8754babbb7bb3bfb7bcfd2e"
     }
    },
    "382f2bc5fc944482861293a496fb03ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4295f3dc0a494198b776f682b327924d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad65c758c1114adab13e6dbf29f660aa",
      "placeholder": "​",
      "style": "IPY_MODEL_eb10554a79a04ecf92517ba96ca956d9",
      "value": " 3.19k/? [00:00&lt;00:00, 53.1kB/s]"
     }
    },
    "44db6a1a111148ceaa804f7302b0629b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48eb8b6feb8a47b79fbd924a7a5c6df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec358cc4784c25a50e17bd8764fd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed5983a37cba4faea852cf2e13b355b4",
      "placeholder": "​",
      "style": "IPY_MODEL_c04bdf05ef684c8fb12ea45833394e77",
      "value": "Downloading: "
     }
    },
    "5600aa1b802c46b183417910f0e18cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b4edc877e524610bb9ecc8103763250": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7616cd08bb494b8f988b9e893bb8bb25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b69f1226cf5457aa164146a890a2c5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44db6a1a111148ceaa804f7302b0629b",
      "placeholder": "​",
      "style": "IPY_MODEL_8c32a5f5cb724610a4ea8a5b42be5b28",
      "value": " 42.0/42.0 [00:00&lt;00:00, 792B/s]"
     }
    },
    "7dac8e5e55ce4e638942c4299b377e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aff67fa78c640bf83598dd7fccc5b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1962a04347894250993d2eb3d2946e5c",
      "placeholder": "​",
      "style": "IPY_MODEL_d5ce787580584a40883fdac45df7917a",
      "value": "Downloading: 100%"
     }
    },
    "8c32a5f5cb724610a4ea8a5b42be5b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97ed312717014144af97d250fcd74d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b689909adc244204af714b9a19e158c0",
      "max": 42,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5e1f379a1084c6e978a633698b4b02a",
      "value": 42
     }
    },
    "98b513fb020744bfbbe0678daa33f2aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c2955e8cff44b5ba998ca584e485e54",
      "max": 1520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1d75f47a76d4bf8b2941fe543e46242",
      "value": 1520
     }
    },
    "a23d83e9f0ea4ac8b57ec5f1614414dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48eb8b6feb8a47b79fbd924a7a5c6df1",
      "placeholder": "​",
      "style": "IPY_MODEL_c2c7fbedceba45aa822c96d7f1cb6134",
      "value": " 287M/287M [00:10&lt;00:00, 29.0MB/s]"
     }
    },
    "a434f279213545da8a92da73bc320504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9aa1538562840f2b969340f1cbf8bbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad65c758c1114adab13e6dbf29f660aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adbad0b85f78492583b2c5b291341030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae117dc6e1fb41eca964e34601c6aa6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af955c59afc54adc8f1072d371b688bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b689909adc244204af714b9a19e158c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b982f93475414c0c8a92601717138dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a434f279213545da8a92da73bc320504",
      "placeholder": "​",
      "style": "IPY_MODEL_17ecb55f59dd411084d8692b80cc8cad",
      "value": "Downloading: "
     }
    },
    "c04bdf05ef684c8fb12ea45833394e77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e574d69ffb426b87d3445d26e02926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c7fbedceba45aa822c96d7f1cb6134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8c29e7a13804b6c9e97c3ff425bd81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c916612f156b457f828410fdc85c8def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adbad0b85f78492583b2c5b291341030",
      "max": 1405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f12dcd6d164513bae3d4d7835cc45e",
      "value": 1405
     }
    },
    "d3190da3507f4ab4be03027179566508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5ce787580584a40883fdac45df7917a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5e1f379a1084c6e978a633698b4b02a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8499a1a46144034b56489025aaee86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db4d60783a2b4cc9ac647f53524e9201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aff67fa78c640bf83598dd7fccc5b97",
       "IPY_MODEL_97ed312717014144af97d250fcd74d9d",
       "IPY_MODEL_7b69f1226cf5457aa164146a890a2c5e"
      ],
      "layout": "IPY_MODEL_23e6dd2b833d44aabc5b78a83d7ef136"
     }
    },
    "de1e4b0f94f74b4380de2cb4447352f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5600aa1b802c46b183417910f0e18cc4",
      "placeholder": "​",
      "style": "IPY_MODEL_a9aa1538562840f2b969340f1cbf8bbb",
      "value": " 2.81k/? [00:00&lt;00:00, 57.6kB/s]"
     }
    },
    "e1d75f47a76d4bf8b2941fe543e46242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb10554a79a04ecf92517ba96ca956d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed5983a37cba4faea852cf2e13b355b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef09f1189d75431591bebf33d288dacc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56b5b8abcfc415fbf46ca0f2fe619d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12e50a27f9f24575bd86305f3b609095",
       "IPY_MODEL_24cc9a4f2e4a411296dd6822329e5815",
       "IPY_MODEL_f83d5ec459fe4dfd9ebf46fcca9895ee"
      ],
      "layout": "IPY_MODEL_ef09f1189d75431591bebf33d288dacc"
     }
    },
    "f83d5ec459fe4dfd9ebf46fcca9895ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dac8e5e55ce4e638942c4299b377e3a",
      "placeholder": "​",
      "style": "IPY_MODEL_d3190da3507f4ab4be03027179566508",
      "value": " 611/611 [04:36&lt;00:00,  2.78ba/s]"
     }
    },
    "f8f12dcd6d164513bae3d4d7835cc45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd705f2d192c41f8bbbf5c6270e6ffb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b982f93475414c0c8a92601717138dc4",
       "IPY_MODEL_98b513fb020744bfbbe0678daa33f2aa",
       "IPY_MODEL_4295f3dc0a494198b776f682b327924d"
      ],
      "layout": "IPY_MODEL_d8499a1a46144034b56489025aaee86f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
