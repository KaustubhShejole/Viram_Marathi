{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3344ba0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers==4.50.0\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.50.0) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (4.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.50.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.50.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.50.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.50.0) (2024.6.2)\n",
      "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.2\n",
      "    Uninstalling transformers-4.53.2:\n",
      "      Successfully uninstalled transformers-4.53.2\n",
      "Successfully installed transformers-4.50.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.50.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b53308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065d15b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f110ef5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bff9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_punct_model_name = \"thenlpresearcher/iitb-en-indic-without-punct\"\n",
    "with_punct_model_name = \"thenlpresearcher/iitb-en-indic-only-punct\"\n",
    "combined_punct_model_name = \"thenlpresearcher/iitb_en_indic_robust_punctuation_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a8db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"thenlpresearcher/test_data_marathi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f7f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- LOAD DATA --------------------\n",
    "src_sentences = raw_datasets['test'][\"sent_written\"]\n",
    "ref_gt     = raw_datasets['test'][\"gt_marathi\"]\n",
    "ref_gem    = raw_datasets['test'][\"gemini_out\"]\n",
    "ref_cfilt  = raw_datasets['test'][\"cfilt_out\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902e5c2",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441bbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ai4bharat/indictrans2-en-indic-dist-200M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b75a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_Latn: Chanting the choir raised the volume as the celebrant intoned the prayer.\n",
      "mar_Deva: ‡§â‡§§‡•ç‡§∏‡§µ‡•Ä ‡§ó‡§æ‡§Ø‡§ï‡§æ‡§Ç‡§®‡•Ä ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§â‡§ö‡•ç‡§ö‡§æ‡§∞ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ß‡•ç‡§µ‡§®‡•Ä‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ‡§§ ‡§µ‡§æ‡§¢ ‡§ù‡§æ‡§≤‡•Ä..........................................................................................................................................................................................................................................\n",
      "eng_Latn: A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth.\n",
      "mar_Deva: ‡§ú‡§®‡•ç‡§Æ‡§æ‡§®‡§Ç‡§§‡§∞ ‡§≤‡§ó‡•á‡§ö‡§ö ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ‡§æ‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ö‡§æ‡§∞‡§π‡•Ä ‡§™‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§≤‡§Ç‡§ó‡§°‡•á‡§™‡§£‡§æ ‡§¶‡§∞‡•ç‡§∂‡§µ‡§ø‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§∏‡§π‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§µ‡§æ‡§∏‡§∞‡§æ‡§≤‡§æ ‡§§‡§™‡§æ‡§∏‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§∏‡§æ‡§¶‡§∞ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•á....................................................................................................................................................................................................................................\n",
      "eng_Latn: Planning authorities should provide alternative locations for small businesses which are or would be offensive in a residential area.\n",
      "mar_Deva: ‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§±‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä ‡§õ‡•ã‡§ü‡•ç‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø‡•Ä ‡§†‡§ø‡§ï‡§æ‡§£‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§ï‡§∞‡•Ç‡§® ‡§¶‡§ø‡§≤‡•Ä ‡§™‡§æ‡§π‡§ø‡§ú‡•á‡§§ ‡§ú‡•Ä ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä ‡§≠‡§æ‡§ó‡§æ‡§§ ‡§Ü‡§ï‡•ç‡§∑‡•á‡§™‡§æ‡§∞‡•ç‡§π ‡§Ü‡§π‡•á‡§§ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§∏‡§§‡•Ä‡§≤...........................................................................................................................................................................................................................................\n",
      "eng_Latn: As the machine develops the forms we use to record data from past projects will be amended.\n",
      "mar_Deva: ‡§ú‡§∏‡§ú‡§∏‡•á ‡§Æ‡§∂‡•Ä‡§® ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§π‡•ã‡§à‡§≤ ‡§§‡§∏‡§§‡§∏‡•á ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§Æ‡§æ‡§ó‡•Ä‡§≤ ‡§™‡•ç‡§∞‡§ï‡§≤‡•ç‡§™‡§æ‡§Ç‡§Æ‡§ß‡•Ä‡§≤ ‡§°‡•á‡§ü‡§æ ‡§∞‡•á‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§µ‡§æ‡§™‡§∞‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§´‡•â‡§∞‡•ç‡§Æ‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§∏‡•Å‡§ß‡§æ‡§∞‡§£‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§à‡§≤........................................................................................................................................................................................................................................\n",
      "eng_Latn: As mentioned, first impressions can be misleading.\n",
      "mar_Deva: ‡§®‡§Æ‡•Ç‡§¶ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á, ‡§™‡§π‡§ø‡§≤‡•Ä ‡§õ‡§æ‡§™ ‡§¶‡§ø‡§∂‡§æ‡§≠‡•Ç‡§≤ ‡§ï‡§∞‡§£‡§æ‡§∞‡•Ä ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•á...................................................................................................................................................................................................................................................\n",
      "eng_Latn: To get a clean assembly load the assembled equals table before the assembly is run.\n",
      "mar_Deva: ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§≤‡•ã‡§° ‡§Æ‡§ø‡§≥‡§µ‡§ø‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§ö‡§æ‡§≤‡§µ‡§£‡•ç‡§Ø‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡§≤ ‡§ï‡•á‡§≤‡•á‡§≤‡•á ‡§∏‡§Æ‡§æ‡§® ‡§ü‡•á‡§¨‡§≤...............................................................................................................................................................................................................................................\n",
      "eng_Latn: Executors delay giving information about substantial deviations from agreed dates. Because of this action cannot be taken in time.\n",
      "mar_Deva: ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§§‡§æ‡§∞‡§ñ‡§æ‡§Ç‡§™‡§æ‡§∏‡•Ç‡§® ‡§≤‡§ï‡•ç‡§∑‡§£‡•Ä‡§Ø ‡§µ‡§ø‡§ö‡§≤‡§®‡§æ‡§Ç‡§ö‡•Ä ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ‡§µ‡§ø‡§≤‡§Ç‡§¨ ‡§ï‡§∞‡§§‡§æ‡§§. ‡§§‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§π‡•Ä ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à ‡§µ‡•á‡§≥‡•á‡§µ‡§∞ ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§ä ‡§∂‡§ï‡§§ ‡§®‡§æ‡§π‡•Ä, ‡§Ö‡§∏‡•á ‡§∏‡§æ‡§Ç‡§ó‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•á.................................................................................................................................................................................................................................\n",
      "eng_Latn: These glycans are poorly transferred to proteins resulting in unoccupied glycosylation sequons.\n",
      "mar_Deva: ‡§π‡•á ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•á‡§®‡•ç‡§∏ ‡§™‡•ç‡§∞‡§•‡§ø‡§®‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ñ‡§∞‡§æ‡§¨‡§∞‡§ø‡§§‡•ç‡§Ø‡§æ ‡§π‡§∏‡•ç‡§§‡§æ‡§Ç‡§§‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡§æ‡§§, ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡•Ä ‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•ã‡§∏‡§ø‡§≤‡•á‡§∂‡§® ‡§∏‡§ø‡§ï‡•ç‡§µ‡§æ‡§Ç‡§∏ ‡§§‡§Ø‡§æ‡§∞ ‡§π‡•ã‡§§‡§æ‡§§...................................................................................................................................................................................................................................\n",
      "eng_Latn: X is an effective acute, oral treatment for migraine with a rapid onset of action\n",
      "mar_Deva: ‡§è‡§ï‡•ç‡§∏ ‡§π‡§æ ‡§Ö‡§∞‡•ç‡§ß‡§∂‡§ø‡§∂‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§è‡§ï ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§§‡•Ä‡§µ‡•ç‡§∞, ‡§§‡•ã‡§Ç‡§°‡•Ä ‡§â‡§™‡§ö‡§æ‡§∞ ‡§Ü‡§π‡•á, ‡§ú‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§ï‡•É‡§§‡•Ä ‡§ú‡§≤‡§¶ ‡§∏‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡•á..........................................................................................................................................................................................................................................\n",
      "eng_Latn: No newspaper is completely unbiased in my expert opinion.\n",
      "mar_Deva: ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§§‡§ú‡•ç‡§ú‡•ç‡§û‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§§‡•á ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§®‡§™‡§§‡•ç‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§£‡§™‡§£‡•á ‡§®‡§ø‡§É‡§™‡§ï‡•ç‡§∑‡§™‡§æ‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä...................................................................................................................................................................................................................................................\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "from tqdm import tqdm\n",
    "# recommended to run this on a gpu with flash_attn installed\n",
    "# don't set attn_implemetation if you don't have flash_attn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "tokenizer_name =  \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "input_sentences = src_sentences\n",
    "\n",
    "import torch\n",
    "\n",
    "def batch_translate(\n",
    "    input_sentences,\n",
    "    src_lang,\n",
    "    tgt_lang,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    ip,\n",
    "    device=\"cuda\",\n",
    "    batch_size=16,\n",
    "):\n",
    "    all_translations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(input_sentences), batch_size)):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "\n",
    "        # Preprocess (handles entity mapping etc.)\n",
    "        batch = ip.preprocess_batch(\n",
    "            batch,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang\n",
    "        )\n",
    "\n",
    "        # Tokenize on device\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate translations\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.batch_decode(\n",
    "            generated,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Postprocess (restore entities)\n",
    "        decoded = ip.postprocess_batch(decoded, lang=tgt_lang)\n",
    "\n",
    "        all_translations.extend(decoded)\n",
    "\n",
    "    return all_translations\n",
    "\n",
    "translations = batch_translate(\n",
    "    input_sentences,\n",
    "    src_lang=\"eng_Latn\",\n",
    "    tgt_lang=\"mar_Deva\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    ip=ip,\n",
    "    device=DEVICE,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "for input_sentence, translation in zip(input_sentences[:10], translations[:10]):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c33a94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§â‡§§‡•ç‡§∏‡§µ‡•Ä ‡§ó‡§æ‡§Ø‡§ï‡§æ‡§Ç‡§®‡•Ä ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§â‡§ö‡•ç‡§ö‡§æ‡§∞ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ß‡•ç‡§µ‡§®‡•Ä‡§Æ‡•Å‡§¶‡•ç‡§∞‡§æ‡§§ ‡§µ‡§æ‡§¢ ‡§ù‡§æ‡§≤‡•Ä.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_prefix(translations, prefix):\n",
    "    ans = []\n",
    "    for t in translations:\n",
    "        t = t.strip()\n",
    "        if t.startswith(prefix):\n",
    "            t = t[len(prefix):]\n",
    "        t = re.sub(r'\\.+', '.', t)\n",
    "        ans.append(t)\n",
    "    return ans\n",
    "\n",
    "translations = remove_prefix(translations, \"mar_Deva eng_Latn \")\n",
    "print(translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a99b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7482aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved predictions to original_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined ‚Üí BLEU: 69.09, chrF++: 84.82\n",
      "GT Marathi ‚Üí BLEU: 57.12\n",
      "Gemini    ‚Üí BLEU: 21.83\n",
      "CFILT     ‚Üí BLEU: 51.14\n",
      "\n",
      "üéØ BEST REFERENCE = GT (by highest BLEU)\n",
      "Metrics written to punct_original_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"prediction\": translations,\n",
    "    \"gt\": ref_gt,\n",
    "    \"gemini\": ref_gem,\n",
    "    \"cfilt\": ref_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"‚úî Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(translations, ref_gt, ref_gem, ref_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=translations, references=[[r] for r in ref_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=translations, references=[[r] for r in ref_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=translations, references=[[r] for r in ref_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined ‚Üí BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi ‚Üí BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    ‚Üí BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     ‚Üí BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nüéØ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined ‚Üí BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81f68d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f9ca57a",
   "metadata": {},
   "source": [
    "### Model Fine-tuned on data without punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4f6a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = without_punct_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e676de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_Latn: Chanting the choir raised the volume as the celebrant intoned the prayer.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§â‡§ö‡•ç‡§ö‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ‡§ö ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§™‡§†‡§£‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§µ‡§≤‡§æ.\n",
      "eng_Latn: A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§®‡•ç‡§Æ‡§æ‡§®‡§Ç‡§§‡§∞ ‡§≤‡§ó‡•á‡§ö‡§ö ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ‡§æ‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§∞‡•ç‡§µ ‡§ö‡§æ‡§∞‡§π‡•Ä ‡§™‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§≤‡§Ç‡§ó‡§°‡•á‡§™‡§£‡§æ ‡§¶‡§∞‡•ç‡§∂‡§µ‡§ø‡§£‡§æ‡§∞‡•Ä ‡§∏‡§π‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§è‡§ï ‡§µ‡§æ‡§∏‡§∞ ‡§§‡§™‡§æ‡§∏‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§∏‡§æ‡§¶‡§∞ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•Ä.\n",
      "eng_Latn: Planning authorities should provide alternative locations for small businesses which are or would be offensive in a residential area.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£‡§æ‡§Ç‡§®‡•Ä ‡§õ‡•ã‡§ü‡•ç‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø‡•Ä ‡§†‡§ø‡§ï‡§æ‡§£‡•á ‡§™‡•Å‡§∞‡§µ‡§≤‡•Ä ‡§™‡§æ‡§π‡§ø‡§ú‡•á‡§§ ‡§ú‡•Ä ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§Ü‡§ï‡•ç‡§∑‡•á‡§™‡§æ‡§∞‡•ç‡§π ‡§Ü‡§π‡•á‡§§ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§∏‡§§‡•Ä‡§≤.\n",
      "eng_Latn: As the machine develops the forms we use to record data from past projects will be amended.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§∏‡§ú‡§∏‡•á ‡§Æ‡§∂‡•Ä‡§® ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§π‡•ã‡§§‡•á, ‡§§‡§∏‡§§‡§∏‡•á ‡§Ü‡§™‡§£ ‡§Æ‡§æ‡§ó‡•Ä‡§≤ ‡§™‡•ç‡§∞‡§ï‡§≤‡•ç‡§™‡§æ‡§Ç‡§Æ‡§ß‡•Ä‡§≤ ‡§°‡•á‡§ü‡§æ ‡§∞‡•á‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§µ‡§æ‡§™‡§∞‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•á ‡§´‡•â‡§∞‡•ç‡§Æ ‡§∏‡•Å‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡•Ä‡§≤.\n",
      "eng_Latn: As mentioned, first impressions can be misleading.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§Æ‡•Ç‡§¶ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á, ‡§™‡§π‡§ø‡§≤‡•Ä ‡§õ‡§æ‡§™ ‡§≠‡•ç‡§∞‡§æ‡§Æ‡§ï ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•á.\n",
      "eng_Latn: To get a clean assembly load the assembled equals table before the assembly is run.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§≠‡§æ‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§ø‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä, ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§ö‡§æ‡§≤‡§µ‡§£‡•ç‡§Ø‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡§≤ ‡§ï‡•á‡§≤‡•á‡§≤‡•á ‡§ü‡•á‡§¨‡§≤ ‡§∏‡§Æ‡§æ‡§® ‡§Ü‡§π‡•á.\n",
      "eng_Latn: Executors delay giving information about substantial deviations from agreed dates. Because of this action cannot be taken in time.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§§‡§æ‡§∞‡§ñ‡§æ‡§Ç‡§™‡§æ‡§∏‡•Ç‡§® ‡§≤‡§ï‡•ç‡§∑‡§£‡•Ä‡§Ø ‡§µ‡§ø‡§ö‡§≤‡§®‡§æ‡§Ç‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ‡§µ‡§ø‡§≤‡§Ç‡§¨ ‡§ï‡§∞‡§§‡§æ‡§§. ‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§µ‡•á‡§≥‡•á‡§µ‡§∞ ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§ä ‡§∂‡§ï‡§§ ‡§®‡§æ‡§π‡•Ä.\n",
      "eng_Latn: These glycans are poorly transferred to proteins resulting in unoccupied glycosylation sequons.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§π‡•á ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡§®‡•ç‡§∏ ‡§™‡•ç‡§∞‡§•‡§ø‡§®‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ñ‡§∞‡§æ‡§¨‡§∞‡§ø‡§§‡•ç‡§Ø‡§æ ‡§π‡§∏‡•ç‡§§‡§æ‡§Ç‡§§‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡§æ‡§§, ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡•Ä ‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•ã‡§∏‡§ø‡§≤‡•á‡§∂‡§® ‡§∏‡§ø‡§ï‡•ç‡§µ‡•á‡§®‡•ç‡§∏ ‡§§‡§Ø‡§æ‡§∞ ‡§π‡•ã‡§§‡§æ‡§§.\n",
      "eng_Latn: X is an effective acute, oral treatment for migraine with a rapid onset of action\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï‡•ç‡§∏ ‡§π‡§æ ‡§Æ‡§æ‡§Ø‡§ó‡•ç‡§∞‡•á‡§®‡§∏‡§æ‡§†‡•Ä ‡§è‡§ï ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä, ‡§§‡•Ä‡§µ‡•ç‡§∞, ‡§§‡•ã‡§Ç‡§°‡•Ä ‡§â‡§™‡§ö‡§æ‡§∞ ‡§Ü‡§π‡•á, ‡§ú‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§ï‡•É‡§§‡•Ä ‡§ú‡§≤‡§¶ ‡§∏‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡•á.\n",
      "eng_Latn: No newspaper is completely unbiased in my expert opinion.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§§‡§ú‡•ç‡§û‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§§‡•á ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä ‡§µ‡•É‡§§‡•ç‡§§‡§™‡§§‡•ç‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§£‡§™‡§£‡•á ‡§®‡§ø‡§É‡§™‡§ï‡•ç‡§∑‡§™‡§æ‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "# recommended to run this on a gpu with flash_attn installed\n",
    "# don't set attn_implemetation if you don't have flash_attn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "tokenizer_name =  \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "input_sentences = src_sentences\n",
    "\n",
    "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "# Tokenize the sentences and generate input encodings\n",
    "inputs = tokenizer(\n",
    "    batch,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Generate translations using the model\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        min_length=0,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "generated_tokens = tokenizer.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "\n",
    "# Postprocess the translations, including entity replacement\n",
    "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "for input_sentence, translation in zip(input_sentences[:10], translations[:10]):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "272092a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§§‡•á‡§µ‡•ç‡§π‡§æ ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§≤‡§æ.\n"
     ]
    }
   ],
   "source": [
    "def remove_prefix(translations, prefix):\n",
    "    ans = []\n",
    "    for t in translations:\n",
    "        t = t.strip()\n",
    "        if t.startswith(prefix):\n",
    "            t = t[len(prefix):]\n",
    "        ans.append(t)\n",
    "    return ans\n",
    "\n",
    "translations = remove_prefix(translations, \"mar_Deva eng_Latn \")\n",
    "print(translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "307b2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d70be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved predictions to combined_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined ‚Üí BLEU: 60.76, chrF++: 80.54\n",
      "GT Marathi ‚Üí BLEU: 42.34\n",
      "Gemini    ‚Üí BLEU: 21.21\n",
      "CFILT     ‚Üí BLEU: 49.06\n",
      "\n",
      "üéØ BEST REFERENCE = CFILT (by highest BLEU)\n",
      "Metrics written to punct_combined_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"prediction\": translations,\n",
    "    \"gt\": ref_gt,\n",
    "    \"gemini\": ref_gem,\n",
    "    \"cfilt\": ref_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"‚úî Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(translations, ref_gt, ref_gem, ref_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=translations, references=[[r] for r in ref_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=translations, references=[[r] for r in ref_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=translations, references=[[r] for r in ref_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined ‚Üí BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi ‚Üí BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    ‚Üí BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     ‚Üí BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nüéØ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined ‚Üí BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e043ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = without_punct_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92778b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_Latn: Chanting the choir raised the volume as the celebrant intoned the prayer.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§â‡§ö‡•ç‡§ö‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ‡§ö ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§™‡§†‡§£‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§µ‡§≤‡§æ.\n",
      "eng_Latn: A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§®‡•ç‡§Æ‡§æ‡§®‡§Ç‡§§‡§∞ ‡§≤‡§ó‡•á‡§ö‡§ö ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ‡§æ‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§∞‡•ç‡§µ ‡§ö‡§æ‡§∞‡§π‡•Ä ‡§™‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§≤‡§Ç‡§ó‡§°‡•á‡§™‡§£‡§æ ‡§¶‡§∞‡•ç‡§∂‡§µ‡§ø‡§£‡§æ‡§∞‡•Ä ‡§∏‡§π‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§è‡§ï ‡§µ‡§æ‡§∏‡§∞ ‡§§‡§™‡§æ‡§∏‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§∏‡§æ‡§¶‡§∞ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•Ä.\n",
      "eng_Latn: Planning authorities should provide alternative locations for small businesses which are or would be offensive in a residential area.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£‡§æ‡§Ç‡§®‡•Ä ‡§õ‡•ã‡§ü‡•ç‡§Ø‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø‡•Ä ‡§†‡§ø‡§ï‡§æ‡§£‡•á ‡§™‡•Å‡§∞‡§µ‡§≤‡•Ä ‡§™‡§æ‡§π‡§ø‡§ú‡•á‡§§ ‡§ú‡•Ä ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§Ü‡§ï‡•ç‡§∑‡•á‡§™‡§æ‡§∞‡•ç‡§π ‡§Ü‡§π‡•á‡§§ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§∏‡§§‡•Ä‡§≤.\n",
      "eng_Latn: As the machine develops the forms we use to record data from past projects will be amended.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§∏‡§ú‡§∏‡•á ‡§Æ‡§∂‡•Ä‡§® ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§π‡•ã‡§§‡•á, ‡§§‡§∏‡§§‡§∏‡•á ‡§Ü‡§™‡§£ ‡§Æ‡§æ‡§ó‡•Ä‡§≤ ‡§™‡•ç‡§∞‡§ï‡§≤‡•ç‡§™‡§æ‡§Ç‡§Æ‡§ß‡•Ä‡§≤ ‡§°‡•á‡§ü‡§æ ‡§∞‡•á‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§µ‡§æ‡§™‡§∞‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•á ‡§´‡•â‡§∞‡•ç‡§Æ ‡§∏‡•Å‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡•Ä‡§≤.\n",
      "eng_Latn: As mentioned, first impressions can be misleading.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§Æ‡•Ç‡§¶ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á, ‡§™‡§π‡§ø‡§≤‡•Ä ‡§õ‡§æ‡§™ ‡§≠‡•ç‡§∞‡§æ‡§Æ‡§ï ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•á.\n",
      "eng_Latn: To get a clean assembly load the assembled equals table before the assembly is run.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§≠‡§æ‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§ø‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä, ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§ö‡§æ‡§≤‡§µ‡§£‡•ç‡§Ø‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡§≤ ‡§ï‡•á‡§≤‡•á‡§≤‡•á ‡§ü‡•á‡§¨‡§≤ ‡§∏‡§Æ‡§æ‡§® ‡§Ü‡§π‡•á.\n",
      "eng_Latn: Executors delay giving information about substantial deviations from agreed dates. Because of this action cannot be taken in time.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§§‡§æ‡§∞‡§ñ‡§æ‡§Ç‡§™‡§æ‡§∏‡•Ç‡§® ‡§≤‡§ï‡•ç‡§∑‡§£‡•Ä‡§Ø ‡§µ‡§ø‡§ö‡§≤‡§®‡§æ‡§Ç‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ‡§µ‡§ø‡§≤‡§Ç‡§¨ ‡§ï‡§∞‡§§‡§æ‡§§. ‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§µ‡•á‡§≥‡•á‡§µ‡§∞ ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§ä ‡§∂‡§ï‡§§ ‡§®‡§æ‡§π‡•Ä.\n",
      "eng_Latn: These glycans are poorly transferred to proteins resulting in unoccupied glycosylation sequons.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§π‡•á ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡§®‡•ç‡§∏ ‡§™‡•ç‡§∞‡§•‡§ø‡§®‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ñ‡§∞‡§æ‡§¨‡§∞‡§ø‡§§‡•ç‡§Ø‡§æ ‡§π‡§∏‡•ç‡§§‡§æ‡§Ç‡§§‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡§æ‡§§, ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡•Ä ‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•ã‡§∏‡§ø‡§≤‡•á‡§∂‡§® ‡§∏‡§ø‡§ï‡•ç‡§µ‡•á‡§®‡•ç‡§∏ ‡§§‡§Ø‡§æ‡§∞ ‡§π‡•ã‡§§‡§æ‡§§.\n",
      "eng_Latn: X is an effective acute, oral treatment for migraine with a rapid onset of action\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï‡•ç‡§∏ ‡§π‡§æ ‡§Æ‡§æ‡§Ø‡§ó‡•ç‡§∞‡•á‡§®‡§∏‡§æ‡§†‡•Ä ‡§è‡§ï ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä, ‡§§‡•Ä‡§µ‡•ç‡§∞, ‡§§‡•ã‡§Ç‡§°‡•Ä ‡§â‡§™‡§ö‡§æ‡§∞ ‡§Ü‡§π‡•á, ‡§ú‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§ï‡•É‡§§‡•Ä ‡§ú‡§≤‡§¶ ‡§∏‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡•á.\n",
      "eng_Latn: No newspaper is completely unbiased in my expert opinion.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§§‡§ú‡•ç‡§û‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§§‡•á ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä ‡§µ‡•É‡§§‡•ç‡§§‡§™‡§§‡•ç‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§£‡§™‡§£‡•á ‡§®‡§ø‡§É‡§™‡§ï‡•ç‡§∑‡§™‡§æ‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "# recommended to run this on a gpu with flash_attn installed\n",
    "# don't set attn_implemetation if you don't have flash_attn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "tokenizer_name =  \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "input_sentences = src_sentences\n",
    "\n",
    "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "# Tokenize the sentences and generate input encodings\n",
    "inputs = tokenizer(\n",
    "    batch,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Generate translations using the model\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        min_length=0,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "generated_tokens = tokenizer.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "\n",
    "# Postprocess the translations, including entity replacement\n",
    "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "for input_sentence, translation in zip(input_sentences[:10], translations[:10]):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b01cdcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§â‡§ö‡•ç‡§ö‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ‡§ö ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§™‡§†‡§£‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§µ‡§≤‡§æ.\n"
     ]
    }
   ],
   "source": [
    "def remove_prefix(translations, prefix):\n",
    "    ans = []\n",
    "    for t in translations:\n",
    "        t = t.strip()\n",
    "        if t.startswith(prefix):\n",
    "            t = t[len(prefix):]\n",
    "        ans.append(t)\n",
    "    return ans\n",
    "\n",
    "translations = remove_prefix(translations, \"mar_Deva eng_Latn \")\n",
    "print(translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be55725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae1d4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved predictions to without_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined ‚Üí BLEU: 63.10, chrF++: 80.97\n",
      "GT Marathi ‚Üí BLEU: 43.28\n",
      "Gemini    ‚Üí BLEU: 24.66\n",
      "CFILT     ‚Üí BLEU: 51.86\n",
      "\n",
      "üéØ BEST REFERENCE = CFILT (by highest BLEU)\n",
      "Metrics written to punct_without_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"prediction\": translations,\n",
    "    \"gt\": ref_gt,\n",
    "    \"gemini\": ref_gem,\n",
    "    \"cfilt\": ref_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"‚úî Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(translations, ref_gt, ref_gem, ref_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=translations, references=[[r] for r in ref_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=translations, references=[[r] for r in ref_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=translations, references=[[r] for r in ref_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined ‚Üí BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi ‚Üí BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    ‚Üí BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     ‚Üí BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nüéØ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined ‚Üí BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d697c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff28707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1156dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8f2c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = with_punct_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b855939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_Latn: Chanting the choir raised the volume as the celebrant intoned the prayer.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§§‡•á‡§µ‡•ç‡§π‡§æ ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§≤‡§æ.\n",
      "eng_Latn: A six-month-old calf was submitted for examination, showing lameness in all four legs which had been present since soon after birth.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§®‡•ç‡§Æ‡§æ‡§®‡§Ç‡§§‡§∞ ‡§≤‡§ó‡•á‡§ö‡§ö ‡§Ö‡§∏‡•ç‡§§‡§ø‡§§‡•ç‡§µ‡§æ‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ö‡§æ‡§∞‡§π‡•Ä ‡§™‡§æ‡§Ø‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§≤‡§Ç‡§ó‡§°‡•á‡§™‡§£‡§æ ‡§¶‡§ø‡§∏‡•Ç‡§® ‡§Ø‡•á‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§∏‡§π‡§æ ‡§Æ‡§π‡§ø‡§®‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§µ‡§æ‡§∏‡•Ç‡§≤‡§æ ‡§§‡§™‡§æ‡§∏‡§£‡•Ä‡§∏‡§æ‡§†‡•Ä ‡§∏‡§æ‡§¶‡§∞ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡•á.\n",
      "eng_Latn: Planning authorities should provide alternative locations for small businesses which are or would be offensive in a residential area.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£‡§æ‡§Ç‡§®‡•Ä ‡§®‡§ø‡§µ‡§æ‡§∏‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§æ‡§§ ‡§Ü‡§ï‡•ç‡§∑‡•á‡§™‡§æ‡§∞‡•ç‡§π ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§∏‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ ‡§≤‡§π‡§æ‡§® ‡§µ‡•ç‡§Ø‡§µ‡§∏‡§æ‡§Ø‡§æ‡§Ç‡§∏‡§æ‡§†‡•Ä ‡§™‡§∞‡•ç‡§Ø‡§æ‡§Ø‡•Ä ‡§†‡§ø‡§ï‡§æ‡§£‡•á ‡§™‡•Å‡§∞‡§µ‡§≤‡•Ä ‡§™‡§æ‡§π‡§ø‡§ú‡•á‡§§.\n",
      "eng_Latn: As the machine develops the forms we use to record data from past projects will be amended.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§ú‡§∏‡§ú‡§∏‡•á ‡§Æ‡§∂‡•Ä‡§® ‡§µ‡§ø‡§ï‡§∏‡§ø‡§§ ‡§π‡•ã‡§à‡§≤ ‡§§‡§∏‡§§‡§∏‡•á ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§Æ‡§æ‡§ó‡•Ä‡§≤ ‡§™‡•ç‡§∞‡§ï‡§≤‡•ç‡§™‡§æ‡§Ç‡§Æ‡§ß‡•Ä‡§≤ ‡§°‡•á‡§ü‡§æ ‡§∞‡•á‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§µ‡§æ‡§™‡§∞‡§§ ‡§Ö‡§∏‡§≤‡•á‡§≤‡•á ‡§´‡•â‡§∞‡•ç‡§Æ ‡§∏‡•Å‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡•Ä‡§≤.\n",
      "eng_Latn: As mentioned, first impressions can be misleading.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§®‡§Æ‡•Ç‡§¶ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡•á, ‡§™‡§π‡§ø‡§≤‡•Ä ‡§õ‡§æ‡§™ ‡§≠‡•ç‡§∞‡§æ‡§Æ‡§ï ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡•á.\n",
      "eng_Latn: To get a clean assembly load the assembled equals table before the assembly is run.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï ‡§∏‡•ç‡§µ‡§ö‡•ç‡§õ ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§≠‡§æ‡§∞ ‡§Æ‡§ø‡§≥‡§µ‡§ø‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡•ç‡§≤‡•Ä ‡§ö‡§æ‡§≤‡§µ‡§£‡•ç‡§Ø‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡•Ä ‡§Ö‡§∏‡•á‡§Ç‡§¨‡§≤ ‡§ï‡•á‡§≤‡•á‡§≤‡•á ‡§ü‡•á‡§¨‡§≤ ‡§∏‡§Æ‡§æ‡§® ‡§ü‡•á‡§¨‡§≤.\n",
      "eng_Latn: Executors delay giving information about substantial deviations from agreed dates. Because of this action cannot be taken in time.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ï‡•á‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§§‡§æ‡§∞‡§ñ‡§æ‡§Ç‡§™‡§æ‡§∏‡•Ç‡§® ‡§≤‡§ï‡•ç‡§∑‡§£‡•Ä‡§Ø ‡§µ‡§ø‡§ö‡§≤‡§®‡§æ‡§Ç‡§¨‡§¶‡•ç‡§¶‡§≤ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä ‡§¶‡•á‡§£‡•ç‡§Ø‡§æ‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ‡§µ‡§ø‡§≤‡§Ç‡§¨ ‡§ï‡§∞‡§§‡§æ‡§§. ‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§µ‡•á‡§≥‡•á‡§µ‡§∞ ‡§ï‡§æ‡§∞‡§µ‡§æ‡§à ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§ä ‡§∂‡§ï‡§§ ‡§®‡§æ‡§π‡•Ä.\n",
      "eng_Latn: These glycans are poorly transferred to proteins resulting in unoccupied glycosylation sequons.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§π‡•á ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•á‡§®‡•ç‡§∏ ‡§™‡•ç‡§∞‡§•‡§ø‡§®‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ñ‡§∞‡§æ‡§¨‡§∞‡§ø‡§§‡•ç‡§Ø‡§æ ‡§π‡§∏‡•ç‡§§‡§æ‡§Ç‡§§‡§∞‡§ø‡§§ ‡§ï‡•á‡§≤‡•á ‡§ú‡§æ‡§§‡§æ‡§§ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡•Ä ‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•ç‡§≤‡§æ‡§Ø‡§ï‡•ã‡§∏‡§ø‡§≤‡•á‡§∂‡§® ‡§∏‡§ø‡§ï‡•ç‡§µ‡•á‡§®‡•ç‡§∏ ‡§§‡§Ø‡§æ‡§∞ ‡§π‡•ã‡§§‡§æ‡§§.\n",
      "eng_Latn: X is an effective acute, oral treatment for migraine with a rapid onset of action\n",
      "mar_Deva: mar_Deva eng_Latn ‡§è‡§ï‡•ç‡§∏ ‡§π‡§æ ‡§Æ‡§æ‡§Ø‡§ó‡•ç‡§∞‡•á‡§®‡§∏‡§æ‡§†‡•Ä ‡§è‡§ï ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§§‡•Ä‡§µ‡•ç‡§∞, ‡§§‡•ã‡§Ç‡§°‡•Ä ‡§â‡§™‡§ö‡§æ‡§∞ ‡§Ü‡§π‡•á ‡§ú‡•ç‡§Ø‡§æ‡§ö‡•Ä ‡§ï‡•É‡§§‡•Ä ‡§ú‡§≤‡§¶ ‡§ó‡§§‡•Ä‡§®‡•á ‡§∏‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡•á.\n",
      "eng_Latn: No newspaper is completely unbiased in my expert opinion.\n",
      "mar_Deva: mar_Deva eng_Latn ‡§Æ‡§æ‡§ù‡•ç‡§Ø‡§æ ‡§§‡§ú‡•ç‡§û‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ ‡§Æ‡§§‡•á ‡§ï‡•ã‡§£‡§§‡•á‡§π‡•Ä ‡§µ‡•É‡§§‡•ç‡§§‡§™‡§§‡•ç‡§∞ ‡§™‡•Ç‡§∞‡•ç‡§£‡§™‡§£‡•á ‡§®‡§ø‡§É‡§™‡§ï‡•ç‡§∑‡§™‡§æ‡§§‡•Ä ‡§®‡§æ‡§π‡•Ä.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "# recommended to run this on a gpu with flash_attn installed\n",
    "# don't set attn_implemetation if you don't have flash_attn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "tokenizer_name =  \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "input_sentences = src_sentences\n",
    "\n",
    "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "# Tokenize the sentences and generate input encodings\n",
    "inputs = tokenizer(\n",
    "    batch,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Generate translations using the model\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        min_length=0,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "generated_tokens = tokenizer.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "\n",
    "# Postprocess the translations, including entity replacement\n",
    "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "for input_sentence, translation in zip(input_sentences[:10], translations[:10]):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e0c2bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡§æ ‡§ï‡•á‡§≤‡•Ä ‡§§‡•á‡§µ‡•ç‡§π‡§æ ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§≤‡§æ.\n"
     ]
    }
   ],
   "source": [
    "def remove_prefix(translations, prefix):\n",
    "    ans = []\n",
    "    for t in translations:\n",
    "        t = t.strip()\n",
    "        if t.startswith(prefix):\n",
    "            t = t[len(prefix):]\n",
    "        ans.append(t)\n",
    "    return ans\n",
    "\n",
    "translations = remove_prefix(translations, \"mar_Deva eng_Latn \")\n",
    "print(translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "272ebbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d8b81bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved predictions to only_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined ‚Üí BLEU: 60.76, chrF++: 80.54\n",
      "GT Marathi ‚Üí BLEU: 42.34\n",
      "Gemini    ‚Üí BLEU: 21.21\n",
      "CFILT     ‚Üí BLEU: 49.06\n",
      "\n",
      "üéØ BEST REFERENCE = CFILT (by highest BLEU)\n",
      "Metrics written to punct_only_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"prediction\": translations,\n",
    "    \"gt\": ref_gt,\n",
    "    \"gemini\": ref_gem,\n",
    "    \"cfilt\": ref_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"‚úî Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(translations, ref_gt, ref_gem, ref_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=translations, references=[[r] for r in ref_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=translations, references=[[r] for r in ref_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=translations, references=[[r] for r in ref_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined ‚Üí BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi ‚Üí BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    ‚Üí BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     ‚Üí BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nüéØ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined ‚Üí BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853e48e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e30546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e1399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e247dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = combined_punct_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ff9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "# recommended to run this on a gpu with flash_attn installed\n",
    "# don't set attn_implemetation if you don't have flash_attn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "tokenizer_name =  \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16, # performance might slightly vary for bfloat16\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").to(DEVICE)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "input_sentences = src_sentences\n",
    "\n",
    "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "# Tokenize the sentences and generate input encodings\n",
    "inputs = tokenizer(\n",
    "    batch,\n",
    "    truncation=True,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Generate translations using the model\n",
    "with torch.no_grad():\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        use_cache=True,\n",
    "        min_length=0,\n",
    "        max_length=256,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "# Decode the generated tokens into text\n",
    "generated_tokens = tokenizer.batch_decode(\n",
    "    generated_tokens,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True,\n",
    ")\n",
    "\n",
    "# Postprocess the translations, including entity replacement\n",
    "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "for input_sentence, translation in zip(input_sentences[:10], translations[:10]):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1ae697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡§â‡§§‡•ç‡§∏‡§µ ‡§∏‡§æ‡§ú‡§∞‡§æ ‡§ï‡§∞‡§£‡§æ‡§∞\\u093C‡•ç‡§Ø‡§æ‡§®‡•á ‡§™‡•ç‡§∞‡§æ‡§∞‡•ç‡§•‡§®‡•á‡§ö‡§æ ‡§∏‡•Ç‡§∞ ‡§µ‡§æ‡§ú‡§µ‡§≤‡•ç‡§Ø‡§æ‡§Æ‡•Å‡§≥‡•á ‡§ó‡§æ‡§Ø‡§ï‡§µ‡•É‡§Ç‡§¶‡§æ‡§ö‡§æ ‡§ú‡§™ ‡§ï‡•á‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§µ‡§æ‡§¢‡§≤‡§æ.\n"
     ]
    }
   ],
   "source": [
    "def remove_prefix(translations, prefix):\n",
    "    ans = []\n",
    "    for t in translations:\n",
    "        t = t.strip()\n",
    "        if t.startswith(prefix):\n",
    "            t = t[len(prefix):]\n",
    "        ans.append(t)\n",
    "    return ans\n",
    "\n",
    "translations = remove_prefix(translations, \"mar_Deva eng_Latn \")\n",
    "print(translations[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12c43a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a98d6398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Saved predictions to combined_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined ‚Üí BLEU: 64.73, chrF++: 81.78\n",
      "GT Marathi ‚Üí BLEU: 43.30\n",
      "Gemini    ‚Üí BLEU: 25.18\n",
      "CFILT     ‚Üí BLEU: 54.13\n",
      "\n",
      "üéØ BEST REFERENCE = CFILT (by highest BLEU)\n",
      "Metrics written to punct_combined_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": src_sentences,\n",
    "    \"prediction\": translations,\n",
    "    \"gt\": ref_gt,\n",
    "    \"gemini\": ref_gem,\n",
    "    \"cfilt\": ref_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"‚úî Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(translations, ref_gt, ref_gem, ref_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=translations, references=[[r] for r in ref_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=translations, references=[[r] for r in ref_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=translations, references=[[r] for r in ref_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined ‚Üí BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi ‚Üí BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    ‚Üí BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     ‚Üí BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\nüéØ BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined ‚Üí BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32ea4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b597b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
