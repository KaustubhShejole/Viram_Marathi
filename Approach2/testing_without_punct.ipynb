{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cV-pf35571ap"
   },
   "source": [
    "### If you dont want to use Wandb, disable Wandb otherwise optional\n",
    "\n",
    "references for WANDB\n",
    "https://analyticsindiamag.com/hands-on-guide-to-weights-and-biases-wandb-with-python-implementation/\n",
    "\n",
    "https://docs.wandb.ai/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 23 06:39:49 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.5     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   55C    P0              70W / 300W |   2192MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   49C    P0              77W / 300W |  65872MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   60C    P0              78W / 300W |  56538MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   49C    P0             297W / 300W |  65524MiB / 81920MiB |     64%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `llm_finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `llm_finetuning`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tmYJBBbwyiX3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"]=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1hOFHi-x5UZ"
   },
   "source": [
    "# Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SqADZ2Sgx5Ua",
    "outputId": "60c95827-a686-45eb-c201-e23bc8b03b2b"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.50.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xKFwtNEx5Uc",
    "outputId": "1206f370-b7a8-4a34-bc6d-36f1a731be6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.50.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX7DsEmOx5Ud"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu45aGHIAQqM"
   },
   "source": [
    "### Loading the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424,
     "referenced_widgets": [
      "34fd15427c7d4f4189c9ce38924107b5",
      "04672fbdc8754babbb7bb3bfb7bcfd2e",
      "53ec358cc4784c25a50e17bd8764fd56",
      "c916612f156b457f828410fdc85c8def",
      "de1e4b0f94f74b4380de2cb4447352f1",
      "c04bdf05ef684c8fb12ea45833394e77",
      "ed5983a37cba4faea852cf2e13b355b4",
      "f8f12dcd6d164513bae3d4d7835cc45e",
      "adbad0b85f78492583b2c5b291341030",
      "a9aa1538562840f2b969340f1cbf8bbb",
      "5600aa1b802c46b183417910f0e18cc4",
      "fd705f2d192c41f8bbbf5c6270e6ffb0",
      "d8499a1a46144034b56489025aaee86f",
      "b982f93475414c0c8a92601717138dc4",
      "98b513fb020744bfbbe0678daa33f2aa",
      "4295f3dc0a494198b776f682b327924d",
      "17ecb55f59dd411084d8692b80cc8cad",
      "a434f279213545da8a92da73bc320504",
      "e1d75f47a76d4bf8b2941fe543e46242",
      "0c2955e8cff44b5ba998ca584e485e54",
      "eb10554a79a04ecf92517ba96ca956d9",
      "ad65c758c1114adab13e6dbf29f660aa",
      "e3ada02e2bf34b5699475b1eb7801282",
      "9c9af690e08d490dbba5524e35fa6fcb",
      "c5d0a0ee946745609e561ffece8cde76",
      "9d13fcfc7b4e4ef182ef48df08253af4",
      "88c725d411d34463b632ec85fa67ed4a",
      "c4298a4942db4790a357a342c7cc13e7",
      "4d9080a460f34c8abdb3af1fdf0eddce",
      "a8744f13ff43450db14031275a7ba769",
      "edf107491f434e7486765ebda2ad938e",
      "922bfc020eb345c584ab31721a10922c",
      "f9c351511a33475d993fee0d6c1cddba",
      "4b5219e4a1a1400a982acc3065b46e2c"
     ]
    },
    "id": "biPo8vFTx5Ue",
    "outputId": "402f80b1-801c-4452-f502-dfcb9d9e9326"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "raw_datasets = load_dataset(\"thenlpresearcher/test_data_marathi\")\n",
    "metric = load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Approach2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LqXNw9_BFQe"
   },
   "source": [
    "The dataset object itself is [datasetdict](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "379sQa4Ix5Uf",
    "outputId": "8d3baded-0cec-4bbf-fdc4-cdcc7944ee9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'punct_type', 'sent_written', 'sent_meant', 'gt_marathi', 'gemini_out', 'cfilt_out'],\n",
       "        num_rows: 54\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nkiu4ITXx5Ug",
    "outputId": "1eb6d334-c138-4ddb-c625-9a7f817fdb2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 0,\n",
       " 'punct_type': 'Comma',\n",
       " 'sent_written': 'Chanting the choir raised the volume as the celebrant intoned the prayer.',\n",
       " 'sent_meant': 'Chanting, the choir raised the volume as the celebrant intoned the prayer.',\n",
       " 'gt_marathi': 'जल्लोष करणाऱ्या व्यक्तीने प्रार्थनेचा उच्चार करताच, गायनाने आवाज वाढवला.',\n",
       " 'gemini_out': 'धर्मगुरू प्रार्थना म्हणत असताना, घोष करणाऱ्या गायकवृंदाने आवाज वाढवला.',\n",
       " 'cfilt_out': 'उत्सवी प्रार्थनेचा उच्चार करत असताना, जप करत, गायकवृंदाने आवाज वाढवला.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6hsuFL1Bwa6"
   },
   "source": [
    "To get a sense of how the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "1OE_CXoxx5Uh",
    "outputId": "c25cfc10-df85-49b7-bd06-524ce871ba9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>punct_type</th>\n",
       "      <th>sent_written</th>\n",
       "      <th>sent_meant</th>\n",
       "      <th>gt_marathi</th>\n",
       "      <th>gemini_out</th>\n",
       "      <th>cfilt_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Comma</td>\n",
       "      <td>No newspaper is completely unbiased in my expert opinion.</td>\n",
       "      <td>No newspaper is completely unbiased, in my expert opinion.</td>\n",
       "      <td>माझ्या तज्ज्ञांच्या मते, कोणतेही वृत्तपत्र पूर्णपणे निःपक्षपाती नाही.</td>\n",
       "      <td>माझ्या तज्ञांच्या मते, कोणतेही वृत्तपत्र पूर्णपणे निःपक्षपाती नसते.</td>\n",
       "      <td>तज्ज्ञांच्या मते कोणतेही वृत्तपत्र पूर्णपणे निःपक्षपाती नाही.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>Comma</td>\n",
       "      <td>X is an effective acute, oral treatment for migraine with a rapid onset of action</td>\n",
       "      <td>X is an effective acute, oral treatment for migraine, with a rapid onset of action</td>\n",
       "      <td>एक्स हा अर्धशिशीसाठी एक प्रभावी तीव्र, तोंडी उपचार आहे, ज्याची कृती जलद गतीने सुरू होते.</td>\n",
       "      <td>एक्स हे मायग्रेनसाठी एक प्रभावी तीव्र, तोंडी उपचार आहे, ज्याची क्रिया जलद सुरु होते.</td>\n",
       "      <td>एक्स हा अर्धशिशीसाठी एक प्रभावी तीव्र, तोंडी उपचार आहे, ज्यात कृतीची जलद सुरुवात होते.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>Quotes</td>\n",
       "      <td>One should read G. V. Carey’s chapter Proof Correction in Mind the Stop.</td>\n",
       "      <td>One should read G. V. Carey’s chapter ‘Proof Correction’ in Mind the Stop.</td>\n",
       "      <td>जी. व्ही. कॅरी यांचा'प्रूफ करेक्शन'हा अध्याय'माईंड द स्टॉप'मध्ये वाचला पाहिजे.</td>\n",
       "      <td>जी. व्ही. कॅरीचा 'माईंड द स्टॉप' मधील 'प्रूफ करेक्शन' हा अध्याय वाचला पाहिजे.</td>\n",
       "      <td>माइंड द स्टॉप मधील जी. व्ही. कॅरी यांचा 'प्रूफ करेक्शन' हा अध्याय वाचला पाहिजे.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43</td>\n",
       "      <td>Comma</td>\n",
       "      <td>This produces hard copy that can be retained but printed sheets are bulky to handle, so digital storage and viewing are often preferred.</td>\n",
       "      <td>This produces hard copy that can be retained, but printed sheets are bulky to handle, so digital storage and viewing are often preferred.</td>\n",
       "      <td>यामुळे हार्ड कॉपी तयार होते जी राखून ठेवली जाऊ शकते परंतु छापील पत्रके हाताळण्यासाठी अवजड असतात, त्यामुळे डिजिटल संचयन आणि पाहणे यांना अनेकदा प्राधान्य दिले जाते.</td>\n",
       "      <td>यामुळे जतन करता येणारी हार्ड कॉपी तयार होते, पण छापलेली पाने हाताळण्यास जाडजूड असल्याने, डिजिटल साठवण आणि पाहण्याला अनेकदा प्राधान्य दिले जाते.</td>\n",
       "      <td>यामुळे हार्ड कॉपी तयार होते जी राखून ठेवता येते, परंतु मुद्रित पत्रके हाताळण्यासाठी अवजड असतात, म्हणून डिजिटल स्टोरेज आणि पाहण्याला अनेकदा प्राधान्य दिले जाते.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Comma</td>\n",
       "      <td>Chanting the choir raised the volume as the celebrant intoned the prayer.</td>\n",
       "      <td>Chanting, the choir raised the volume as the celebrant intoned the prayer.</td>\n",
       "      <td>जल्लोष करणाऱ्या व्यक्तीने प्रार्थनेचा उच्चार करताच, गायनाने आवाज वाढवला.</td>\n",
       "      <td>धर्मगुरू प्रार्थना म्हणत असताना, घोष करणाऱ्या गायकवृंदाने आवाज वाढवला.</td>\n",
       "      <td>उत्सवी प्रार्थनेचा उच्चार करत असताना, जप करत, गायकवृंदाने आवाज वाढवला.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(raw_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(ckpt_dir, device, quantization=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "\n",
    "    # Quantization setup\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "#     # Load tokenizer\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    # Move to device and optionally convert to half precision\n",
    "    if qconfig is None:\n",
    "        model = model.to(device)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    # Make sure model is in training mode for fine-tuning\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "en_indic_ckpt_dir = \"thenlpresearcher/iitb-en-indic-without-punct\"\n",
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(ckpt_dir, device, quantization=None):\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "    import torch\n",
    "\n",
    "    # Quantization setup\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    # Move to device and optionally convert to half precision\n",
    "    if qconfig is None:\n",
    "        model = model.to_empty(device=device)\n",
    "        if device == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    # Make sure model is in training mode for fine-tuning\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"thenlpresearcher/iitb_en_indic_robust_punctuation_model\", trust_remote_code=True)\n",
    "en_indic_model = initialize_model(model_checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_XK7px-IDkC"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the Seq2SeqTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = en_indic_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32322"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndicTransTokenizer(name_or_path='ai4bharat/indictrans2-en-indic-dist-200M', vocab_size=32322, model_max_length=256, is_fast=False, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'punct_type', 'sent_written', 'sent_meant', 'gt_marathi', 'gemini_out', 'cfilt_out'],\n",
       "    num_rows: 54\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IndicTransToolkit.processor import IndicProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = IndicProcessor(inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------- LOAD DATA --------------------\n",
    "src_sentences = raw_datasets['test'][\"sent_written\"]\n",
    "ref_gt     = raw_datasets['test'][\"gt_marathi\"]\n",
    "ref_gem    = raw_datasets['test'][\"gemini_out\"]\n",
    "ref_cfilt  = raw_datasets['test'][\"cfilt_out\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "54\n",
      "54\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(src_sentences))\n",
    "print(len(ref_gt))\n",
    "print(len(ref_gem))\n",
    "print(len(ref_cfilt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eng_Latn mar_Deva Chanting the choir raised ...']\n"
     ]
    }
   ],
   "source": [
    "print(ip.preprocess_batch([\"Chanting the choir raised…\"], src_lang=\"eng_Latn\", tgt_lang=\"mar_Deva\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def debug_translate_one(\n",
    "    sentence: str,\n",
    "    src_lang: str,\n",
    "    tgt_lang: str,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    ip,\n",
    "    device: str = \"cuda\",\n",
    "    max_length: int = 256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Translate a single sentence using IndicTrans2 with detailed debug output.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): Input sentence to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model (IndicTrans2).\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: IndicProcessor object for preprocessing/postprocessing.\n",
    "        device (str): Torch device (\"cuda\" or \"cpu\").\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        str: Postprocessed translation.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"\\n=== INPUT SENTENCE ===\")\n",
    "    print(sentence)\n",
    "\n",
    "    # ------------------------ Preprocessing ------------------------\n",
    "    preprocessed = ip.preprocess_batch([sentence], src_lang=src_lang, tgt_lang=tgt_lang)[0]\n",
    "    print(\"\\n=== PREPROCESSED ===\")\n",
    "    print(preprocessed)\n",
    "\n",
    "    # ------------------------ Tokenization ------------------------\n",
    "    inputs = tokenizer(\n",
    "        [preprocessed],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    print(\"\\n=== TOKEN IDS (first 30) ===\")\n",
    "    print(inputs[\"input_ids\"][0][:30])\n",
    "\n",
    "    print(\"\\n=== TOKENIZED INPUT TOKENS ===\")\n",
    "    print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][:30]))\n",
    "\n",
    "    # ------------------------ Decoder token fix ------------------------\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        fixed_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "        model.config.decoder_start_token_id = fixed_id\n",
    "        print(f\"[Fix] decoder_start_token_id set to: {fixed_id}\")\n",
    "    else:\n",
    "        fixed_id = model.config.decoder_start_token_id\n",
    "    \n",
    "    print(f\"Fixed id: {fixed_id}\")\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    # ------------------------ Generation ------------------------\n",
    "    print(\"\\n=== GENERATING... ===\")\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            min_length=5,            # ensure meaningful output\n",
    "            num_beams=5,\n",
    "            num_return_sequences=1,\n",
    "            early_stopping=True,\n",
    "            decoder_start_token_id=fixed_id,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        \n",
    "    print(\"\\n=== GENERATED TOKEN IDS ===\")\n",
    "    print(generated_tokens)\n",
    "\n",
    "    # ------------------------ Decode ------------------------\n",
    "    decoded_text = tokenizer.decode(\n",
    "        generated_tokens[0],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    print(\"\\n=== DECODED RAW ===\")\n",
    "    print(decoded_text)\n",
    "\n",
    "    # ------------------------ Postprocess ------------------------\n",
    "    try:\n",
    "        post = ip.postprocess_batch([decoded_text], lang=tgt_lang)[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[Warning] Postprocess failed: {e}\")\n",
    "        post = decoded_text\n",
    "\n",
    "    print(\"\\n=== FINAL POSTPROCESSED TRANSLATION ===\")\n",
    "    print(post)\n",
    "\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_tokens = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eng_Latn - mar_Deva\n",
      "eng_Latn: When I was young, I used to go to the park every day.\n",
      "mar_Deva: मी लहान होतो तेव्हा मी दररोज उद्यानात जायचो.\n",
      "eng_Latn: He has many old books, which he inherited from his ancestors.\n",
      "mar_Deva: त्यांच्याकडे अनेक जुनी पुस्तके आहेत, जी त्यांना त्यांच्या पूर्वजांकडून वारशाने मिळाली आहेत.\n",
      "eng_Latn: I can't figure out how to solve my problem.\n",
      "mar_Deva: माझी समस्या कशी सोडवायची हे मला समजत नाही.\n",
      "eng_Latn: She is very hardworking and intelligent, which is why she got all the good marks.\n",
      "mar_Deva: ती खूप मेहनती आणि हुशार आहे, म्हणूनच तिला सर्व चांगले गुण मिळाले.\n",
      "eng_Latn: We watched a new movie last week, which was very inspiring.\n",
      "mar_Deva: आम्ही गेल्या आठवड्यात एक नवीन चित्रपट पाहिला, जो खूप प्रेरणादायी होता.\n",
      "eng_Latn: If you had met me at that time, we would have gone out to eat.\n",
      "mar_Deva: त्यावेळी तुम्ही मला भेटलात तर आम्ही जेवायला बाहेर गेलो असतो.\n",
      "eng_Latn: She went to the market with her sister to buy a new sari.\n",
      "mar_Deva: ती तिच्या बहिणीसोबत नवीन साडी विकत घेण्यासाठी बाजारात गेली होती.\n",
      "eng_Latn: Raj told me that he is going to his grandmother's house next month.\n",
      "mar_Deva: राजने मला सांगितले की तो पुढच्या महिन्यात त्याच्या आजीच्या घरी जाणार आहे.\n",
      "eng_Latn: All the kids were having fun at the party and were eating lots of sweets.\n",
      "mar_Deva: सर्व मुले पार्टीमध्ये मजा करत होती आणि भरपूर मिठाई खात होती.\n",
      "eng_Latn: My friend has invited me to his birthday party, and I will give him a gift.\n",
      "mar_Deva: माझ्या मित्राने मला त्याच्या वाढदिवसाच्या पार्टीला आमंत्रित केले आहे आणि मी त्याला एक भेटवस्तू देईन.\n",
      "eng_Latn: Let’s eat, Grandma.\n",
      "mar_Deva: चला, आजी, जेवूया.\n"
     ]
    }
   ],
   "source": [
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "en_sents = [\n",
    "    \"When I was young, I used to go to the park every day.\",\n",
    "    \"He has many old books, which he inherited from his ancestors.\",\n",
    "    \"I can't figure out how to solve my problem.\",\n",
    "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
    "    \"We watched a new movie last week, which was very inspiring.\",\n",
    "    \"If you had met me at that time, we would have gone out to eat.\",\n",
    "    \"She went to the market with her sister to buy a new sari.\",\n",
    "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
    "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
    "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
    "    \"Let’s eat, Grandma.\"\n",
    "]\n",
    "\n",
    "# en_sents = sentences_meant\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
    "for input_sentence, translation in zip(en_sents, hi_translations):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del en_indic_tokenizer, en_indic_model\n",
    "\n",
    "# df['gt -- marathi'] = hi_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(device=DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eng_Latn - mar_Deva\n",
      "eng_Latn: When I was young, I used to go to the park every day.\n",
      "mar_Deva: मी लहान होतो तेव्हा मी दररोज उद्यानात जायचो.\n",
      "eng_Latn: He has many old books, which he inherited from his ancestors.\n",
      "mar_Deva: त्यांच्याकडे अनेक जुनी पुस्तके आहेत, जी त्यांना त्यांच्या पूर्वजांकडून वारशाने मिळाली आहेत.\n",
      "eng_Latn: I can't figure out how to solve my problem.\n",
      "mar_Deva: माझी समस्या कशी सोडवायची हे मला समजत नाही.\n",
      "eng_Latn: She is very hardworking and intelligent, which is why she got all the good marks.\n",
      "mar_Deva: ती खूप मेहनती आणि हुशार आहे, म्हणूनच तिला सर्व चांगले गुण मिळाले.\n",
      "eng_Latn: We watched a new movie last week, which was very inspiring.\n",
      "mar_Deva: आम्ही गेल्या आठवड्यात एक नवीन चित्रपट पाहिला, जो खूप प्रेरणादायी होता.\n",
      "eng_Latn: If you had met me at that time, we would have gone out to eat.\n",
      "mar_Deva: त्यावेळी तुम्ही मला भेटलात तर आम्ही जेवायला बाहेर गेलो असतो.\n",
      "eng_Latn: She went to the market with her sister to buy a new sari.\n",
      "mar_Deva: ती तिच्या बहिणीसोबत नवीन साडी विकत घेण्यासाठी बाजारात गेली होती.\n",
      "eng_Latn: Raj told me that he is going to his grandmother's house next month.\n",
      "mar_Deva: राजने मला सांगितले की तो पुढच्या महिन्यात त्याच्या आजीच्या घरी जाणार आहे.\n",
      "eng_Latn: All the kids were having fun at the party and were eating lots of sweets.\n",
      "mar_Deva: सर्व मुले पार्टीमध्ये मजा करत होती आणि भरपूर मिठाई खात होती.\n",
      "eng_Latn: My friend has invited me to his birthday party, and I will give him a gift.\n",
      "mar_Deva: माझ्या मित्राने मला त्याच्या वाढदिवसाच्या पार्टीला आमंत्रित केले आहे आणि मी त्याला एक भेटवस्तू देईन.\n",
      "eng_Latn: Let’s eat, Grandma.\n",
      "mar_Deva: चला, आजी, जेवूया.\n"
     ]
    }
   ],
   "source": [
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "en_indic_model = initialize_model(en_indic_ckpt_dir, quantization)\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "en_indic_ckpt_dir = \"thenlpresearcher/iitb-en-indic-without-punct\"\n",
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "en_sents = [\n",
    "    \"When I was young, I used to go to the park every day.\",\n",
    "    \"He has many old books, which he inherited from his ancestors.\",\n",
    "    \"I can't figure out how to solve my problem.\",\n",
    "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
    "    \"We watched a new movie last week, which was very inspiring.\",\n",
    "    \"If you had met me at that time, we would have gone out to eat.\",\n",
    "    \"She went to the market with her sister to buy a new sari.\",\n",
    "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
    "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
    "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
    "    \"Let’s eat, Grandma.\"\n",
    "]\n",
    "\n",
    "# en_sents = sentences_meant\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
    "for input_sentence, translation in zip(en_sents, hi_translations):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del en_indic_tokenizer, en_indic_model\n",
    "\n",
    "# df['gt -- marathi'] = hi_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from IndicTransToolkit.processor import IndicProcessor\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cpu\"\n",
    "quantization = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_empty_weights\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m en_indic_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_indic_ckpt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# import torch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai4bharat/indictrans2-en-indic-dist-200M\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36minitialize_model\u001b[0;34m(ckpt_dir, quantization)\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     18\u001b[0m     ckpt_dir,\n\u001b[1;32m     19\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m     low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mqconfig,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qconfig \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEVICE \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     27\u001b[0m         model\u001b[38;5;241m.\u001b[39mhalf()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3712\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3708\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3709\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3710\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3711\u001b[0m         )\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1166\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1166\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1167\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1168\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen moving module from meta to a different device.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1169\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
    "en_indic_ckpt_dir = \"thenlpresearcher/iitb-en-indic-without-punct\"\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "en_indic_model = initialize_model(en_indic_ckpt_dir, quantization)\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "model_checkpoint = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
    "\n",
    "# \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "quantization = None\n",
    "en_indic_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "en_sents = [\n",
    "    \"When I was young, I used to go to the park every day.\",\n",
    "    \"He has many old books, which he inherited from his ancestors.\",\n",
    "    \"I can't figure out how to solve my problem.\",\n",
    "    \"She is very hardworking and intelligent, which is why she got all the good marks.\",\n",
    "    \"We watched a new movie last week, which was very inspiring.\",\n",
    "    \"If you had met me at that time, we would have gone out to eat.\",\n",
    "    \"She went to the market with her sister to buy a new sari.\",\n",
    "    \"Raj told me that he is going to his grandmother's house next month.\",\n",
    "    \"All the kids were having fun at the party and were eating lots of sweets.\",\n",
    "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
    "    \"Let’s eat, Grandma.\"\n",
    "]\n",
    "\n",
    "# en_sents = sentences_meant\n",
    "\n",
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "hi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n",
    "\n",
    "print(f\"\\n{src_lang} - {tgt_lang}\")\n",
    "for input_sentence, translation in zip(en_sents, hi_translations):\n",
    "    print(f\"{src_lang}: {input_sentence}\")\n",
    "    print(f\"{tgt_lang}: {translation}\")\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del en_indic_tokenizer, en_indic_model\n",
    "\n",
    "# df['gt -- marathi'] = hi_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_ids(\"eng_Latn\"))\n",
    "print(tokenizer.convert_tokens_to_ids(\"mar_Deva\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, device, batch_size=8, max_length=256):\n",
    "    \"\"\"\n",
    "    Translate a batch of sentences using a seq2seq model like IndicTrans with safety checks.\n",
    "\n",
    "    Args:\n",
    "        input_sentences (list of str): Source sentences to translate.\n",
    "        src_lang (str): Source language code, e.g., \"eng_Latn\".\n",
    "        tgt_lang (str): Target language code, e.g., \"mar_Deva\".\n",
    "        model: Hugging Face seq2seq model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        ip: Preprocessing object (IndicProcessor).\n",
    "        device: torch device (\"cuda\" or \"cpu\").\n",
    "        batch_size (int): Batch size for generation.\n",
    "        max_length (int): Maximum length of generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        translations (list of str): Translated sentences.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    translations = []\n",
    "\n",
    "    # Safe access for decoder_start_token_id\n",
    "    decoder_start_token_id = getattr(model.config, \"decoder_start_token_id\", None)\n",
    "    pad_token_id = getattr(tokenizer, \"pad_token_id\", None)\n",
    "    eos_token_id = getattr(tokenizer, \"eos_token_id\", None)\n",
    "\n",
    "    if decoder_start_token_id is None:\n",
    "        print(\"[Warning] decoder_start_token_id is None. Using default generation behavior.\")\n",
    "\n",
    "    for i in range(0, len(input_sentences), batch_size):\n",
    "        batch = input_sentences[i : i + batch_size]\n",
    "        print('here')\n",
    "\n",
    "        # Preprocess the batch\n",
    "        batch_preprocessed = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        if not isinstance(batch_preprocessed, list) or len(batch_preprocessed) == 0:\n",
    "            print(f\"[Warning] Preprocessed batch is empty at index {i}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "#         Debug: print first 2 sentences after preprocessing\n",
    "        print(f\"[Debug] Preprocessed batch sample: {batch_preprocessed[:2]}\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            batch_preprocessed,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # Move tensors to the correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate translations with safety parameters\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                generated_tokens = model.generate(\n",
    "                    **inputs,\n",
    "                    use_cache=True,\n",
    "                    min_length=5,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=5,\n",
    "                    num_return_sequences=1,\n",
    "                    early_stopping=True,\n",
    "                    decoder_start_token_id=decoder_start_token_id,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    eos_token_id=eos_token_id\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] Generation failed for batch starting at index {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Decode generated tokens\n",
    "        decoded_texts = tokenizer.batch_decode(\n",
    "            generated_tokens,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "#         Debug: print first 2 decoded outputs\n",
    "        print(f\"[Debug] Decoded sample: {decoded_texts[:2]}\")\n",
    "\n",
    "        # Postprocess translations\n",
    "        try:\n",
    "            postprocessed = ip.postprocess_batch(decoded_texts, lang=tgt_lang)\n",
    "            translations += postprocessed\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] Postprocessing failed for batch starting at index {i}: {e}\")\n",
    "            translations += decoded_texts  # fallback\n",
    "\n",
    "        # Free GPU memory\n",
    "        del inputs, generated_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang, tgt_lang = \"eng_Latn\", \"mar_Deva\"\n",
    "\n",
    "prefix = f\"{tgt_lang} {src_lang}\"\n",
    "\n",
    "def remove_prefix(text):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):].strip()\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▋                                  | 2/14 [00:00<00:01,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva Chanting the choir raised the volume as the celebrant intoned the prayer .', 'eng_Latn mar_Deva A six-month-old calf was submitted for examination , showing lameness in all four legs which had been present since soon after birth .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva As mentioned , first impressions can be misleading .', 'eng_Latn mar_Deva To get a clean assembly load the assembled equals table before the assembly is run .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▌                               | 3/14 [00:00<00:01,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva X is an effective acute , oral treatment for migraine with a rapid onset of action', 'eng_Latn mar_Deva No newspaper is completely unbiased in my expert opinion .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva The only problem with the new project established in the desert at high cost is the lack of good access roads .', 'eng_Latn mar_Deva Having failed in all previous attempts he evolved a new plan which surprised everybody .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva In his daily traffickings a Cairene resident is often made conscious of the suffocating pollution .', 'eng_Latn mar_Deva If drama implies conflict and poetry metaphor then poetic drama must imply the dramatization of metaphor !']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████                    | 7/14 [00:00<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva The calculator executed the functions that had been selected and displayed the results on the top half of the screen .', 'eng_Latn mar_Deva There are many disturbing factors fatigue , poor eye-sight , poor reading ability , anxiety or undue caution , distractibility , and inadequate motivation .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva The crucial words in the specification are the signals must be routed internally .', 'eng_Latn mar_Deva Would you please supply a list of the correct settings for the ABC']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva However oxidative stress occurs when the balance is disturbed , through an increase in reactive oxygen species', \"eng_Latn mar_Deva Should any burner fail to ignite its respective section will revert to ' purge ' and in this way the system ensures safe removal of unburned fuel\"]\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva Unfortunately though incorrect predictions were made about both negative and positive experiments , leading to doubts about the reliability of the underlying model .', 'eng_Latn mar_Deva Reject the applicant using procedure XYZ to ensure compliance with standard evaluation protocols .']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|██████████████████████████████▋        | 11/14 [00:00<00:00, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: [\"eng_Latn mar_Deva Insert the new disk into the disk drive with the notch at the bottom facing the drive 's entry slot .\", 'eng_Latn mar_Deva Replace the fuel lines and electrical conduits , which have cracks or damaged B-nut fittings to prevent leaks and ensure safe operation .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva Take no action as the camera operates automatically under all lighting conditions .', 'eng_Latn mar_Deva The species of fish supported by the reef are varied and abundant food supplies are available to sustain their populations .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva Connection to PTT-supplied packet-switch networks will be a prime requirement of the workstation and gateways into these networks are planned by the PTTs to ensure seamless data communication .', 'eng_Latn mar_Deva The system consistently achieved 50 gallons per hour optimum output .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva Morphological studies of paraquat or oxygen toxicity in rats have shown significant cellular damage in lung tissue .', \"eng_Latn mar_Deva One should read G. V. Carey 's chapter Proof Correction in Mind the Stop .\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 14/14 [00:00<00:00, 15.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "here\n",
      "[Debug] Preprocessed batch sample: ['eng_Latn mar_Deva a water : glycol ratio of < ID1 > is preferred .', 'eng_Latn mar_Deva It has thickness of only < ID1 > inch .']\n",
      "[Debug] Decoded sample: ['\"', '\"']\n",
      "\n",
      "Successful translations: 54 / 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- TRANSLATION --------------------\n",
    "valid_src = []\n",
    "valid_pred = []\n",
    "valid_gt = []\n",
    "valid_gem = []\n",
    "valid_cfilt = []\n",
    "\n",
    "# Translate in batches\n",
    "all_translations = []\n",
    "for i in tqdm(range(0, len(src_sentences), BATCH_SIZE)):\n",
    "    batch = src_sentences[i:i+BATCH_SIZE]\n",
    "#     print(batch)\n",
    "    translations = batch_translate(\n",
    "        batch,\n",
    "        src_lang,\n",
    "        tgt_lang,\n",
    "        en_indic_model,\n",
    "        en_indic_tokenizer,\n",
    "        ip,\n",
    "        device=device\n",
    "    )\n",
    "    all_translations.extend(translations)\n",
    "\n",
    "    if translations is None:\n",
    "        print(f\"[SKIPPED] Batch {i}: Returned None\")\n",
    "        continue\n",
    "\n",
    "    cleaned = [remove_prefix(t) for t in translations]\n",
    "\n",
    "    valid_src.extend(batch)\n",
    "    valid_pred.extend(cleaned)\n",
    "    valid_gt.extend(ref_gt[i:i + len(batch)])\n",
    "    valid_gem.extend(ref_gem[i:i + len(batch)])\n",
    "    valid_cfilt.extend(ref_cfilt[i:i + len(batch)])\n",
    "\n",
    "print(f\"\\nSuccessful translations: {len(valid_pred)} / {len(src_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Saved predictions to combined_outputs.csv\n",
      "\n",
      "===== FINAL METRICS =====\n",
      "All references combined → BLEU: 0.03, chrF++: 0.01\n",
      "GT Marathi → BLEU: 0.02\n",
      "Gemini    → BLEU: 0.02\n",
      "CFILT     → BLEU: 0.02\n",
      "\n",
      "🎯 BEST REFERENCE = Gemini (by highest BLEU)\n",
      "Metrics written to punct_combined_baseline_outputs_eval_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# -------------------- SAVE OUTPUTS --------------------\n",
    "results_df = pd.DataFrame({\n",
    "    \"src\": valid_src,\n",
    "    \"prediction\": valid_pred,\n",
    "    \"gt\": valid_gt,\n",
    "    \"gemini\": valid_gem,\n",
    "    \"cfilt\": valid_cfilt\n",
    "})\n",
    "\n",
    "results_df.to_csv(f\"{mode}_outputs.csv\", index=False)\n",
    "print(f\"✔ Saved predictions to {mode}_outputs.csv\")\n",
    "\n",
    "# -------------------- METRICS --------------------\n",
    "bleu = load(\"sacrebleu\")\n",
    "chrf = load(\"chrf\")\n",
    "\n",
    "def compute_scores(preds, ref1, ref2, ref3):\n",
    "    \"\"\"\n",
    "    Compute BLEU and chrF++ scores using all three references for each sentence.\n",
    "    \"\"\"\n",
    "    references = [[r1, r2, r3] for r1, r2, r3 in zip(ref1, ref2, ref3)]  # sacrebleu format\n",
    "    bleu_score = bleu.compute(predictions=preds, references=references)[\"score\"]\n",
    "    chrf_score = chrf.compute(predictions=preds, references=references)[\"score\"]\n",
    "    return bleu_score, chrf_score\n",
    "\n",
    "bleu_score, chrf_score = compute_scores(valid_pred, valid_gt, valid_gem, valid_cfilt)\n",
    "\n",
    "# Determine best reference per metric (based on BLEU)\n",
    "all_scores = {\n",
    "    \"GT\":    bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gt])[\"score\"],\n",
    "    \"Gemini\": bleu.compute(predictions=valid_pred, references=[[r] for r in valid_gem])[\"score\"],\n",
    "    \"CFILT\":  bleu.compute(predictions=valid_pred, references=[[r] for r in valid_cfilt])[\"score\"]\n",
    "}\n",
    "\n",
    "best_ref = max(all_scores, key=all_scores.get)\n",
    "\n",
    "print(\"\\n===== FINAL METRICS =====\")\n",
    "print(f\"All references combined → BLEU: {bleu_score:.2f}, chrF++: {chrf_score:.2f}\")\n",
    "print(f\"GT Marathi → BLEU: {all_scores['GT']:.2f}\")\n",
    "print(f\"Gemini    → BLEU: {all_scores['Gemini']:.2f}\")\n",
    "print(f\"CFILT     → BLEU: {all_scores['CFILT']:.2f}\")\n",
    "print(f\"\\n🎯 BEST REFERENCE = {best_ref} (by highest BLEU)\")\n",
    "\n",
    "# -------------------- SAVE METRICS --------------------\n",
    "with open(f\"punct_{mode}_indictrans2_eval_metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"All references combined → BLEU {bleu_score:.2f}, chrF++ {chrf_score:.2f}\\n\")\n",
    "    f.write(f\"GT    BLEU {all_scores['GT']:.2f}\\n\")\n",
    "    f.write(f\"Gem   BLEU {all_scores['Gemini']:.2f}\\n\")\n",
    "    f.write(f\"CFILT BLEU {all_scores['CFILT']:.2f}\\n\")\n",
    "    f.write(f\"\\nBEST REFERENCE = {best_ref}\\n\")\n",
    "\n",
    "print(f\"Metrics written to punct_{mode}_baseline_outputs_eval_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'उत्सवी गायकांनी प्रार्थनेचा उच्चार केल्याने गायकवृंदाचा जप केल्याने आवाज वाढला.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['prediction'][0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "001c2cf8d21d4703bc190149a82d06ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c6c16b336c404fa84cba2ee42debde",
       "IPY_MODEL_31bbbf7e34a14335b628add663c71642",
       "IPY_MODEL_a23d83e9f0ea4ac8b57ec5f1614414dc"
      ],
      "layout": "IPY_MODEL_c8c29e7a13804b6c9e97c3ff425bd81d"
     }
    },
    "04672fbdc8754babbb7bb3bfb7bcfd2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "064aa5db0924410bb17646220304faea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c2955e8cff44b5ba998ca584e485e54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12e50a27f9f24575bd86305f3b609095": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_330c9a9e97c9436f9d29bb12c586253a",
      "placeholder": "​",
      "style": "IPY_MODEL_382f2bc5fc944482861293a496fb03ed",
      "value": "100%"
     }
    },
    "13c6c16b336c404fa84cba2ee42debde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7616cd08bb494b8f988b9e893bb8bb25",
      "placeholder": "​",
      "style": "IPY_MODEL_ae117dc6e1fb41eca964e34601c6aa6c",
      "value": "Downloading: 100%"
     }
    },
    "17ecb55f59dd411084d8692b80cc8cad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1962a04347894250993d2eb3d2946e5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23e6dd2b833d44aabc5b78a83d7ef136": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24cc9a4f2e4a411296dd6822329e5815": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b4edc877e524610bb9ecc8103763250",
      "max": 611,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af955c59afc54adc8f1072d371b688bc",
      "value": 611
     }
    },
    "31bbbf7e34a14335b628add663c71642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c0e574d69ffb426b87d3445d26e02926",
      "max": 300887193,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_064aa5db0924410bb17646220304faea",
      "value": 300887193
     }
    },
    "330c9a9e97c9436f9d29bb12c586253a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34fd15427c7d4f4189c9ce38924107b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53ec358cc4784c25a50e17bd8764fd56",
       "IPY_MODEL_c916612f156b457f828410fdc85c8def",
       "IPY_MODEL_de1e4b0f94f74b4380de2cb4447352f1"
      ],
      "layout": "IPY_MODEL_04672fbdc8754babbb7bb3bfb7bcfd2e"
     }
    },
    "382f2bc5fc944482861293a496fb03ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4295f3dc0a494198b776f682b327924d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad65c758c1114adab13e6dbf29f660aa",
      "placeholder": "​",
      "style": "IPY_MODEL_eb10554a79a04ecf92517ba96ca956d9",
      "value": " 3.19k/? [00:00&lt;00:00, 53.1kB/s]"
     }
    },
    "44db6a1a111148ceaa804f7302b0629b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48eb8b6feb8a47b79fbd924a7a5c6df1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ec358cc4784c25a50e17bd8764fd56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed5983a37cba4faea852cf2e13b355b4",
      "placeholder": "​",
      "style": "IPY_MODEL_c04bdf05ef684c8fb12ea45833394e77",
      "value": "Downloading: "
     }
    },
    "5600aa1b802c46b183417910f0e18cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b4edc877e524610bb9ecc8103763250": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7616cd08bb494b8f988b9e893bb8bb25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b69f1226cf5457aa164146a890a2c5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44db6a1a111148ceaa804f7302b0629b",
      "placeholder": "​",
      "style": "IPY_MODEL_8c32a5f5cb724610a4ea8a5b42be5b28",
      "value": " 42.0/42.0 [00:00&lt;00:00, 792B/s]"
     }
    },
    "7dac8e5e55ce4e638942c4299b377e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aff67fa78c640bf83598dd7fccc5b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1962a04347894250993d2eb3d2946e5c",
      "placeholder": "​",
      "style": "IPY_MODEL_d5ce787580584a40883fdac45df7917a",
      "value": "Downloading: 100%"
     }
    },
    "8c32a5f5cb724610a4ea8a5b42be5b28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97ed312717014144af97d250fcd74d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b689909adc244204af714b9a19e158c0",
      "max": 42,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5e1f379a1084c6e978a633698b4b02a",
      "value": 42
     }
    },
    "98b513fb020744bfbbe0678daa33f2aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c2955e8cff44b5ba998ca584e485e54",
      "max": 1520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1d75f47a76d4bf8b2941fe543e46242",
      "value": 1520
     }
    },
    "a23d83e9f0ea4ac8b57ec5f1614414dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48eb8b6feb8a47b79fbd924a7a5c6df1",
      "placeholder": "​",
      "style": "IPY_MODEL_c2c7fbedceba45aa822c96d7f1cb6134",
      "value": " 287M/287M [00:10&lt;00:00, 29.0MB/s]"
     }
    },
    "a434f279213545da8a92da73bc320504": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9aa1538562840f2b969340f1cbf8bbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad65c758c1114adab13e6dbf29f660aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adbad0b85f78492583b2c5b291341030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae117dc6e1fb41eca964e34601c6aa6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af955c59afc54adc8f1072d371b688bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b689909adc244204af714b9a19e158c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b982f93475414c0c8a92601717138dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a434f279213545da8a92da73bc320504",
      "placeholder": "​",
      "style": "IPY_MODEL_17ecb55f59dd411084d8692b80cc8cad",
      "value": "Downloading: "
     }
    },
    "c04bdf05ef684c8fb12ea45833394e77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0e574d69ffb426b87d3445d26e02926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2c7fbedceba45aa822c96d7f1cb6134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8c29e7a13804b6c9e97c3ff425bd81d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c916612f156b457f828410fdc85c8def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adbad0b85f78492583b2c5b291341030",
      "max": 1405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f12dcd6d164513bae3d4d7835cc45e",
      "value": 1405
     }
    },
    "d3190da3507f4ab4be03027179566508": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5ce787580584a40883fdac45df7917a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5e1f379a1084c6e978a633698b4b02a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8499a1a46144034b56489025aaee86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db4d60783a2b4cc9ac647f53524e9201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8aff67fa78c640bf83598dd7fccc5b97",
       "IPY_MODEL_97ed312717014144af97d250fcd74d9d",
       "IPY_MODEL_7b69f1226cf5457aa164146a890a2c5e"
      ],
      "layout": "IPY_MODEL_23e6dd2b833d44aabc5b78a83d7ef136"
     }
    },
    "de1e4b0f94f74b4380de2cb4447352f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5600aa1b802c46b183417910f0e18cc4",
      "placeholder": "​",
      "style": "IPY_MODEL_a9aa1538562840f2b969340f1cbf8bbb",
      "value": " 2.81k/? [00:00&lt;00:00, 57.6kB/s]"
     }
    },
    "e1d75f47a76d4bf8b2941fe543e46242": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb10554a79a04ecf92517ba96ca956d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed5983a37cba4faea852cf2e13b355b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef09f1189d75431591bebf33d288dacc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f56b5b8abcfc415fbf46ca0f2fe619d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_12e50a27f9f24575bd86305f3b609095",
       "IPY_MODEL_24cc9a4f2e4a411296dd6822329e5815",
       "IPY_MODEL_f83d5ec459fe4dfd9ebf46fcca9895ee"
      ],
      "layout": "IPY_MODEL_ef09f1189d75431591bebf33d288dacc"
     }
    },
    "f83d5ec459fe4dfd9ebf46fcca9895ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7dac8e5e55ce4e638942c4299b377e3a",
      "placeholder": "​",
      "style": "IPY_MODEL_d3190da3507f4ab4be03027179566508",
      "value": " 611/611 [04:36&lt;00:00,  2.78ba/s]"
     }
    },
    "f8f12dcd6d164513bae3d4d7835cc45e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fd705f2d192c41f8bbbf5c6270e6ffb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b982f93475414c0c8a92601717138dc4",
       "IPY_MODEL_98b513fb020744bfbbe0678daa33f2aa",
       "IPY_MODEL_4295f3dc0a494198b776f682b327924d"
      ],
      "layout": "IPY_MODEL_d8499a1a46144034b56489025aaee86f"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
